# Overview

## Source
https://docs.literalai.com/get-started/overview

Literal AI is the collaborative platform for building **production-grade LLM apps**.
Literal AI is the collaborative **observability** , **evaluation** and **analytics** platform for building **production-grade LLM apps**. Literal AI offers multimodal logging, including vision, audio, and video.
![Literal AI Features](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/features.svg)
Collaborative Flow on Literal AI
It covers a wide range of LLM-based use cases such as agentic applications, RAG, chatbots and task automation. Literal AI integrates seemlessly with many third parties such as [OpenAI](https://docs.literalai.com/integrations/openai), [LangChain/LangGraph](https://docs.literalai.com/integrations/langchain) or [Llama Index](https://docs.literalai.com/integrations/llama-index).
Literal AI is developed by the builders of [Chainlit](https://github.com/Chainlit/chainlit), the open-source Conversational AI Python framework.
## 
[​](https://docs.literalai.com/get-started/overview#key-features)
Key features
  1. [Logs:](https://docs.literalai.com/guides/logs) Instrument your code with the Literal AI SDK to log your LLM app in production.
  2. [Prompt Management:](https://docs.literalai.com/guides/prompts) Safely create, A/B test, debug, and version prompts directly from Literal AI.
  3. [Dataset:](https://docs.literalai.com/guides/dataset) Create datasets mixing production data and hand written examples to run non regression tests/experiments.
  4. [Evaluation:](https://docs.literalai.com/guides/scorers) Evaluate and monitor the performance of your LLM app in production. View LLM metrics in a dashboard, set automated rules and collect product & user analytics.


![Literal AI Platform](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/literal-ai-overview.png)
Literal AI Platform Overview
## 
[​](https://docs.literalai.com/get-started/overview#next-up)
Next up
## [Instrument your CodeInstall the Literal AI SDK and get your API key.](https://docs.literalai.com/get-started/quick-start)## [Create your First PromptCreate, version and A/B test your prompts in the Prompt Playground.](https://docs.literalai.com/guides/playground)

---

# Quick Start

## Source
https://docs.literalai.com/get-started/quick-start

Install the Literal AI client and get your API key
## [Instrument your CodeInstall the Literal AI SDK and get your API key.](https://docs.literalai.com/get-started/quick-start#logging-llm-generations-agent-runs-or-conversation-threads)## [Create your first Prompt from the PlaygroundCreate, version and A/B test your prompts in the Prompt Playground.](https://docs.literalai.com/guides/playground)
## 
[​](https://docs.literalai.com/get-started/quick-start#logging-llm-generations%2C-agent-runs-or-conversation-threads)
Logging LLM Generations, Agent Runs or Conversation Threads
1
Install the Literal AI package
Python
TypeScript
Copy
```
pip install literalai

```

2
Instantiate the Literal AI client
Python
TypeScript
Copy
```
import os
from literalai import LiteralClient
# Not compatible with gunicorn's --preload flag
literalai_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY")) # This is the default and can be omitted

```

3
Get your Literal AI API Key
Go to your [project page](https://cloud.getliteral.ai) and click on the **Settings** tab. You will find your API key in the **API Key** section.
![Literal AI API key](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/api_key.png)
Copy your API key
## 
[​](https://docs.literalai.com/get-started/quick-start#start-logging-llm-generations-and-agent-runs)
Start logging LLM generations and agent runs
Literal AI provides
  1. low-level SDKs in Python and TypeScript
  2. integrations with LLM providers (OpenAI, etc.) and AI frameworks (LangChain, LlamaIndex, Vercel AI SDK, etc.)


## [LoggingLearn how to instrument your code with the Literal AI SDK.](https://docs.literalai.com/guides/logs)## [IntegrationsBrowse the list of integrations available with Literal AI.](https://docs.literalai.com/integrations)

---

# Annotation Queues

## Source
https://docs.literalai.com/guides/annotation-queue

Annotation Queues help you organise and streamline the human evaluation process. You can easily set-up queues, add data for review and provide an accessible and efficient ui for domain experts to score, tag and add to datasets crucial insights.
## 
[​](https://docs.literalai.com/guides/annotation-queue#create-an-annotation-queue)
Create an annotation queue
You can access your queues on the `Annotation Queues` page, and create one by clicking the `+` button in the top right corner of the table.
![Create annotation queue](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/create-queue.gif)
Annotation Queues Page
This will open a modal form, that you can fill with a **name** to identify the annotation queue and a **description** to convey the queue meaning.
Each new project comes with a “Default” annotation queue.
## 
[​](https://docs.literalai.com/guides/annotation-queue#populate-a-queue-from-logs)
Populate a queue from logs
Once you have created a queue, you can start adding data to it. You can add runs/generations at once from the logs page.
Add Multiple Items to the Annotation Queue
You can also add a single item to the queue from a thread/run/generation detail page.
Add a Single Item to the Annotation Queue
## 
[​](https://docs.literalai.com/guides/annotation-queue#efficiently-review-queue-items)
Efficiently review queue items
On the `Annotation Queues` page, a reviewer can open a specific queue and start reviewing added items.
![The annotation item page](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/annotation-queue.png)
Annotation Queue Item Page
A reviewer can:
  * **Score** the item
  * **Tag** the item
  * **Add to Dataset** the item


Finally, once the review is done, they can mark the item as Reviewed.
## 
[​](https://docs.literalai.com/guides/annotation-queue#check-progress)
Check progress
Admins can monitor the progress of the queues on the `Annotation Queues` page by clicking on the `Admin View` button.
Annotation Queue Items List Page
In this view, the Admin can switch between the `Pending` and `Reviewed` tabs to see how many items are left to review and how many have been reviewed.

---

# Continuous Improvement

## Source
https://docs.literalai.com/guides/continuous-improvement

Improve your LLM applications over time with continuous improvement.
# 
[​](https://docs.literalai.com/guides/continuous-improvement#continuous-improvement-for-llm-applications)
Continuous Improvement for LLM Applications
Continuous improvement is a crucial aspect of developing and maintaining high-quality LLM applications. This guide will walk you through the process of evaluating and improving your LLM-powered systems over time.
![Literal AI Features](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/features.svg)
Collaborative Flow on Literal AI
## 
[​](https://docs.literalai.com/guides/continuous-improvement#evaluation-framework)
Evaluation Framework
Before implementing continuous improvement, it’s essential to establish a robust evaluation framework. Follow these steps to create an effective evaluation process:
### 
[​](https://docs.literalai.com/guides/continuous-improvement#determine-the-evaluation-level)
Determine the Evaluation Level
Choose the appropriate level for evaluation:
  * LLM call level (similar to unit tests)
  * Agent run level (similar to integration tests)
  * Conversation level


### 
[​](https://docs.literalai.com/guides/continuous-improvement#define-evaluation-metrics)
Define Evaluation Metrics
Identify what aspects of your LLM application you want to measure:
  * Hallucination rate
  * Answer relevancy
  * Application-specific behaviors
  * Response quality
  * Task completion rate


### 
[​](https://docs.literalai.com/guides/continuous-improvement#select-evaluation-methods)
Select Evaluation Methods
Choose one or more evaluation methods based on your needs:
  * LLM-as-a-Judge: Use another LLM to evaluate outputs
  * Code-based evaluation: Implement programmatic checks
  * Hybrid approach: Combine LLM and code-based evaluations
  * Embedding similarity: Compare vector representations of responses
  * Human review: Incorporate manual evaluation by experts


You can find more information on how to perform evaluations [here](https://docs.literalai.com/guides/scorers).
## 
[​](https://docs.literalai.com/guides/continuous-improvement#improvement-process)
Improvement Process
Once you have established your evaluation framework, follow these steps to continuously improve your LLM application:
### 
[​](https://docs.literalai.com/guides/continuous-improvement#pre-production-iteration)
Pre-production Iteration
  1. Create a [dataset](https://docs.literalai.com/guides/dataset) with ground truth examples
  2. Implement your [evaluation](https://docs.literalai.com/guides/scorers) procedure
  3. Iterate on your LLM application ([prompts](https://docs.literalai.com/guides/prompts), code, etc.) to improve performance
  4. Build and test the first production-ready version


### 
[​](https://docs.literalai.com/guides/continuous-improvement#production-monitoring-and-evaluation)
Production Monitoring and Evaluation
Now that you have a production system, implement the following strategies to gather data and improve your application in production:
#### 
[​](https://docs.literalai.com/guides/continuous-improvement#product-feedback-loops)
Product Feedback Loops
  * Implicit feedback: Track user actions (e.g., accepting or rejecting suggestions)
  * Explicit feedback: Implement user rating systems (e.g., thumbs up/down)


#### 
[​](https://docs.literalai.com/guides/continuous-improvement#human-review)
Human Review
  * Regularly have human experts review a subset of [logged interactions](https://docs.literalai.com/guides/logs) using [annotation queues](https://docs.literalai.com/guides/annotation-queue)
  * Identify areas for improvement and edge cases


#### 
[​](https://docs.literalai.com/guides/continuous-improvement#automated-ai-evaluations)
Automated AI Evaluations
  * Implement reference-free evaluations to continuously monitor performance
  * Use metrics like perplexity, coherence, or task-specific scores


You can find more information on how to perform evaluations [here](https://docs.literalai.com/guides/online-evals).
### 
[​](https://docs.literalai.com/guides/continuous-improvement#continuous-improvement-cycle)
Continuous Improvement Cycle
  1. Analyze data from production monitoring
  2. Identify edge cases and areas for improvement
  3. Add new examples to your [evaluation dataset](https://docs.literalai.com/guides/dataset)
  4. Update [prompts](https://docs.literalai.com/guides/prompts), agent code, or model fine-tuning
  5. Run tests to ensure improvements don’t introduce regressions
  6. Deploy the new version to production


#### 
[​](https://docs.literalai.com/guides/continuous-improvement#ci%2Fcd-integration)
CI/CD Integration
To ensure that there are no regressions, integrate the following test into your CI/CD pipeline:
  1. Pull the most representative [dataset](https://docs.literalai.com/guides/dataset)
  2. Run the LLM system and the [evaluations](https://docs.literalai.com/guides/experiment)
  3. Pull the baseline performance metrics with regards to that dataset
  4. Compare results to the baseline metrics using a confidence interval


### 
[​](https://docs.literalai.com/guides/continuous-improvement#global-performance-monitoring)
Global Performance Monitoring
Track product metrics such as:
  * Conversion rates
  * User retention
  * Task completion rates
  * User satisfaction scores


Use these metrics to assess the overall impact of your LLM application and guide future improvements.
## 
[​](https://docs.literalai.com/guides/continuous-improvement#conclusion)
Conclusion
By implementing a robust continuous improvement process, you can ensure that your LLM application remains effective, relevant, and valuable to your users over time. Regular [evaluation](https://docs.literalai.com/guides/experiment), monitoring, and iteration are key to maintaining a high-quality LLM-powered system.

---

# Dashboard

## Source
https://docs.literalai.com/guides/dashboard

Monitor your AI application usage, and track performance and reliability.
# 
[​](https://docs.literalai.com/guides/dashboard#monitoring-your-ai-application)
Monitoring Your AI Application
Effective monitoring is crucial for maintaining and optimizing your AI application. This guide will walk you through the key aspects of monitoring, including using the dashboard, understanding important metrics, and accessing logs.
Monitoring is closely related to [evaluation](https://docs.literalai.com/guides/online-evals) and [continuous improvement](https://docs.literalai.com/guides/continuous-improvement).
## 
[​](https://docs.literalai.com/guides/dashboard#dashboard-overview)
Dashboard Overview
The dashboard provides a comprehensive view of your AI application’s performance and usage.
![Dashboard](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/dashboard.png)
Dashboard
Each card can be filtered.
### 
[​](https://docs.literalai.com/guides/dashboard#volume-metrics)
Volume Metrics
Track the usage and activity of your AI application over time:
  * Number of conversation threads
  * Agent runs
  * Text generations
  * Token usage
  * User feedback submissions


### 
[​](https://docs.literalai.com/guides/dashboard#latency-metrics)
Latency Metrics
Monitor the speed and responsiveness of your AI:
  * Time to first token: How quickly your AI starts generating a response
  * Token throughput: The rate at which tokens are generated


### 
[​](https://docs.literalai.com/guides/dashboard#ai-performance-evaluations)
AI Performance Evaluations
![Configure AI Evaluations](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/configure-ai-evals.png)
Configure AI Evaluations
Evaluate the quality and effectiveness of your AI:
  * Set up AI evaluations in production to continuously monitor performance
  * For detailed information on setting up evaluations, refer to our [evaluation guide](https://docs.literalai.com/guides/scorers)


### 
[​](https://docs.literalai.com/guides/dashboard#llm-cost-metrics)
LLM Cost Metrics
Track the cost of your LLM Provider APIs over time.
## [Setup Cost TrackingSetup cost tracking for your LLM Provider APIs.](https://docs.literalai.com/guides/settings/llm#cost-tracking)
## 
[​](https://docs.literalai.com/guides/dashboard#accessing-logs)
Accessing Logs
Detailed [logs](https://docs.literalai.com/guides/logs) provide valuable insights for troubleshooting and optimization:
  1. In the sidebar, navigate to the “Logs” section
  2. Use filters to narrow down logs by date, conversation ID, or AI eval results
  3. Review log entries for specific conversations or errors to identify root causes

---

# Datasets

## Source
https://docs.literalai.com/guides/dataset

Datasets are collections of input/expected output samples for conducting experiments and non regression tests.
A Dataset is made of items. A Dataset item has an `input`, `expected output` and can contain `metadata`. The input, expected output and metadata of the items in a dataset should follow the same schema.
## 
[​](https://docs.literalai.com/guides/dataset#dataset-types)
Dataset Types
There are two types of datasets in Literal AI: **Key-Value** and **Generation**.
### 
[​](https://docs.literalai.com/guides/dataset#key-value-datasets)
Key-Value Datasets
Key-Value datasets can have any key-value pairs for input and expected output. This type of dataset can be used to for example store [Runs](https://docs.literalai.com/guides/logs#semantics) of agents. An example of a Key-Value dataset item:
Copy
```
input = {
 "query": "Can you name a movie about space travel?"
}
expected_output = { 
 "response": "A movie about space travel is \"Interstellar\"." 
}

```

### 
[​](https://docs.literalai.com/guides/dataset#generation-datasets)
Generation Datasets
Generation datasets are a type of dataset that follow the OpenAI message format. An example of a Generation Dataset Item:
Copy
```
input = {
 "messages": [{
  "role": "system"
  "content": "You are a helpful assistant." 
 }, {
  "role": "user"
  "content": "Can you name a movie about space travel?" 
 }] 
}
expected_output = { 
 "role": "assistant",
 "content": "A movie about space travel is \"Interstellar\"." 
}

```

## 
[​](https://docs.literalai.com/guides/dataset#create-a-dataset)
Create a Dataset
### 
[​](https://docs.literalai.com/guides/dataset#from-file)
From File
You can upload a dataset on Literal AI from CSV (**Key-Value** datasets) or JSONL (**Generation** datasets).
To do so, go to the Datasets page and click on the `+` button to create a new dataset. Then drag and drop your file to upload it.
Uploading a Key-Value Dataset
### 
[​](https://docs.literalai.com/guides/dataset#from-code)
From Code
You can also create a `Dataset` and populate it with items programmatically using SDKs.
See [installation](https://docs.literalai.com/get-started/quick-start) to get your API key and instantiate SDKs
Here is an example of how to create a dataset:
Python
TypeScript
Copy
```
# create a new dataset
dataset = literalai_client.api.create_dataset(
 name="Foo", 
 description="A dataset to store samples.", 
 metadata={"isDemo": True},
 type="key_value" # Default type is "key_value", other is "generation"
)

```

Now that we have a `Dataset`, we can create dataset items:
Python
TypeScript
Copy
```
dataset_item = literalai_client.api.create_dataset_item(
 dataset_id = dataset.id
 input = { "content": "What is Literal AI ?" },
 expected_output = { "content": "Literal AI is an observability solution." }
)

```

### 
[​](https://docs.literalai.com/guides/dataset#from-existing-logs)
From existing logs
A benefit of managing your datasets and production logs in Literal AI is that you can continuously improve your app by using the logs to create new dataset items.
![Create a Dataset Item from logs](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/add-to-dataset.gif)
Create a Dataset Item from logs
Whenever you identify a new edge case in your logs, you can create a dataset item to account for it in your experiments. By editing the expected output to match the desired output, you ensure that your future experiments will account for this particular issue.
## 
[​](https://docs.literalai.com/guides/dataset#get-a-dataset)
Get a dataset
Once you have created a dataset, you can retrieve it using the SDK to use it in your experiments or your CI.
Python
TypeScript
Copy
```
dataset = literalai_client.api.get_dataset(id=dataset.id)
for item in dataset.items:
 pass

```

---

# Fine-tuning

## Source
https://docs.literalai.com/guides/distillation

Fine-tune your LLM models to improve performance and reduce costs.
Fine-tuning is the process of training an existing Machine Learning model — LLM here — on a custom dataset of input/output examples.
## 
[​](https://docs.literalai.com/guides/distillation#how-it-works)
How it works
### 
[​](https://docs.literalai.com/guides/distillation#fine-tuning)
Fine-tuning
LLM providers offer fine-tuning as a service, you simply need to provide the trainign data in the right format. Behind the scenes, the LLM provider freezes the original model’s weights and slides in a few layers of trainable weights (LoRA).
Training happens on the LLM provider’s infrastructure and the loss function (cross-entropy) is the metric optimized against. Best practices are to pass in a training and a validation dataset to ensure your fine-tuned model is not overfitting.
Do not forget to test your final model beyond the next-token prediction task: you should set benchmarks to evaluate the performance of the fine-tuned model on the real-world task you plan to use it on: summary, extraction, tone of voice, RAG, etc.
### 
[​](https://docs.literalai.com/guides/distillation#dataset-export)
Dataset export
Literal AI simplifies the creation of training datasets with `Generation` datasets.
You can build `Generation` datasets from your logs, or from your annotation queues: the format is compatible with the OpenAI fine-tuning format.
To download your datasets as JSONL (JSON Lines) files, you can use the `Download` button on the dataset page:
![Download Generation Dataset](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/download-jsonl.gif)
Download Generation Dataset
## 
[​](https://docs.literalai.com/guides/distillation#distillation)
Distillation
Often-times, the hard part is to come up with the right dataset to train on. It should be:
  * high quality
  * representative of the real-world task you want to use the model on
  * large enough to train a good model


From production logs generated with a high-performance LLM, you can automatically generate mulitple quality samples of input/output messages.
Once you have built a stable enough dataset, you can reduce costs by fine-tuning a smaller model on the data you gathered.
This is what is called **distillation** : you are distilling the knowledge of the original model into a smaller model.

---

# Evaluation

## Source
https://docs.literalai.com/guides/evaluation

Learn how to evaluate your LLM applications and agents.
## 
[​](https://docs.literalai.com/guides/evaluation#why-evaluate%3F)
Why evaluate?
Evaluation is key to enable continuous deployment of LLM-based applications and guarantee that newer versions perform better than previous ones. To best capture the **user experience** one must understand the multiple steps which make up the application. As AI applications grow in complexity, they tend to chain multiple steps.
Literal AI lets you log & monitor the various steps of your LLM application. By doing so, you can continuously improve the performance of your LLM system, building the most relevant metrics:
Level| Metrics  
---|---  
LLM Generation| Hallucination, Toxicity, etc.  
Agent Run| Task completion, Number of intermediate steps  
Conversation Thread| User satisfaction  
An example is the vanilla **Retrieval Augmented Generation** (RAG), which augments Large Language Models (LLMs) with domain-specific data. Examples of metrics you can score against are: context relevancy, faithfulness, answer relevancy, etc.
## 
[​](https://docs.literalai.com/guides/evaluation#how-to-think-about-evaluation%3F)
How to think about evaluation?
Scores are a crucial part of developing and improving your LLM application or agent.
Who?| When?| Type of eval metrics| Example  
---|---|---|---  
End-User| In Production| Explicit Feedback (👍👎)| Thumbs-up or down on a chatbot’s answer  
End-User| In Production| Implicit Feedback based on product metric| User conversion to paid offering increases by 15%  
LLM-as-a-Judge| In Production| AI evaluation (without ground truth)| Hallucination, context relevancy, etc.  
LLM-as-a-Judge| During Iteration| AI evaluation against a Dataset (with ground truth or not)| Hallucination, conciseness, helpfulness, context relevancy, answer similarity, etc.  
Domain Expert| During Iteration| Human evaluation against a Dataset (with ground truth or not)| Hallucination, conciseness, helpfulness, context relevancy, answer similarity, etc.  
# 
[​](https://docs.literalai.com/guides/evaluation#leverage-literal-ai)
Leverage Literal AI
## [Setup LLM-as-a-Judge ScorersAutomatically evaluate your LLM logs in production, monitor performance and detect issues.](https://docs.literalai.com/guides/online-evals)## [Add a score using the SDKsAdd scores to your LLM logs using the SDKs.](https://docs.literalai.com/guides/logs#add-a-score)

---

# Experiments

## Source
https://docs.literalai.com/guides/experiment

Experiments enable continous improvement of your Prompt/Agent — i.e. guarantee net improvements.
An experiment evaluates the performance of your **LLM system** (simple prompt or multi-step LLM chain/agent) against a **Dataset** and a set of **Evaluation Metrics**.
![Experiment distribution chart](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/experiment-overview.gif)
Experiment distribution chart
## 
[​](https://docs.literalai.com/guides/experiment#run-experiments-from-literal-ai)
Run Experiments from Literal AI
Run an `Experiment` on a `Prompt` against a `Dataset` and a set of `Scorers` from Literal AI.
Experiments can be run directly from the Prompt Playground. This allows you to run experiments without having to manage an infrastructure.
1
Prompt to iterate on
Go to the Prompt Playground, make modifications to your prompt and vibe-check it.
![Prompt to iterate on](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/playground-start-from-example.gif)
Prompt to iterate on
If you struggle to start, select one of our examples from the top right corner.
2
Pick a Dataset and select Scorers 
In the upper right corner, click “Experiment on Dataset”.
![Experiment on Dataset](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/experiment-dataset.gif)
Experiment on Dataset
You should specify how to resolve prompt variables with your dataset `input`, `expectedOutput` and `metadata` columns. The `Scorer` configuration offers to use the prompt’s completion through the `output` key.
Running the experiment will redirect you to the Experiment details page, where you can track progress!
More Evaluators to come soon!
## 
[​](https://docs.literalai.com/guides/experiment#compare-experiments)
Compare experiments
You can only compare two experiments if they were run on the same dataset.
![Comparing two experiments ran on the same dataset.](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/compare-experiments.gif)
Comparing two experiments ran on the same dataset.
## 
[​](https://docs.literalai.com/guides/experiment#run-an-experiment-from-your-code)
Run an experiment from your code
Complex multi-step LLM systems are heavily dependent on your code and instrastructure. Literal AI enables you to evaluate your LLM systems from your own code and then log the results on Literal AI.
Here is a naive example of how you can run an experiment with Literal AI:
See [installation](https://docs.literalai.com/get-started/quick-start) to get your API key and instantiate the SDK
Python
TypeScript
Copy
```
inputs = [{"question": "question"}]
experiment = literalai_client.api.create_experiment(
  name="Foo", params=[{"foo": "bar"}] # optional
)
@literalai_client.run
def my_agent(input):
  # Faking the agent response
  return {"content": "answer"}
def score_output(output):
  # Faking the scoring
  return [{"name": "context_relevancy", "type": "AI", "value": 0.6}]
@literalai_client.experiment_item_run
def run_and_eval(input):
  output = my_agent(input)
  experiment_item = {
   "scores": score_output(output),
   "input": input,
   "output": output
  }
  experiment.log(experiment_item)
def run_experiment(inputs):
  for input in inputs:
    run_and_eval(input)
run_experiment(inputs)

```

### 
[​](https://docs.literalai.com/guides/experiment#link-to-a-dataset)
Link to a Dataset
The best way to run an experiment is to use a [Dataset](https://docs.literalai.com/guides/dataset) to store your inputs and expected outputs. This way you can track on which data your experiment was run and compare the results of different experiments.
Using a dataset to run an experiment is very similar to the previous example, except that you are iterating over the items of the dataset:
Python
TypeScript
Copy
```
dataset_id = "MY_DATASET_ID"
dataset = literalai_client.api.get_dataset(dataset_id)
experiment = literalai_client.api.create_experiment(
  dataset_id=dataset_id,
  name="Foo",
  params=[{"foo": "bar"}] # optional
)
@literalai_client.experiment_item_run
def run_and_eval(item):
  output = my_agent(item.input)
  experiment_item = {
   # Notice that the experiment item is now linked to the dataset item
   "datasetItemId": item.id,
   "scores": score_output(output),
   "input": item.input,
   "output": output
  }
  experiment.log(experiment_item)
def run_experiment():
  for item in dataset.items:
    run_and_eval(item)

```

### 
[​](https://docs.literalai.com/guides/experiment#link-to-a-prompt)
Link to a Prompt
If you are evaluating a prompt [living on Literal AI](https://docs.literalai.com/guides/prompts), you can bind it to the experiment to track the performance of the prompt.
Python
TypeScript
Copy
```
prompt = literalai_client.api.get_prompt(name="MY_PROMPT", version=0)
experiment = literalai_client.api.create_experiment(
  prompt_id=prompt.id,
  name="Foo",
  params=[{"foo": "bar"}] # optional
)
# Run the experiment the same way as before

```

---

# Logs

## Source
https://docs.literalai.com/guides/logs

Logs are essential to monitor and improve your LLM app in production. Literal AI provides flexible and composable SDKs to log your LLM app at different levels of granularity.
## 
[​](https://docs.literalai.com/guides/logs#semantics)
Semantics
![Literal AI Platform](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/thread-steps.svg)
Log Hierarchy on Literal AI
Literal AI approaches LLM logging at three levels:
  1. **Generation** : Log of a single LLM call. (Generations are Steps.)
  2. **Step** : Log of a regular function execution, which is usually an intermediate step in an LLM system. Possible `type`s are: `tool`, `embedding`, `retrieval`, `rerank`, `undefined`, etc. Steps can be considered as Spans.
  3. **Run** : Trace of an Agent/Chain run, including its intermediate steps. Can contain one or multiple generations.
  4. **Thread** : A collection of Runs that are part of a single conversation.


![Literal AI Platform](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/threads-overview.gif)
A `Thread` with runs & intermediate steps
You can log a generation only (typically for extraction use cases), or log a run only (typically for task automation), or combine them in threads (typically for chatbots).
See [installation](https://docs.literalai.com/get-started/quick-start) to get your API key and instantiate the SDK
## 
[​](https://docs.literalai.com/guides/logs#log-an-llm-generation)
Log an LLM Generation
Generations are logged by [integrations](https://docs.literalai.com/integrations) with LLM providers. They capture the prompt, completion, settings, and token latency.
Here is an example with OpenAI:
Python
TypeScript
Copy
```
literalai_client.instrument_openai()
# Run a regular OpenAI chat completion
from openai import OpenAI
oai = OpenAI()
def call_openai(user_input: str):
  oai.chat.completions.create(
    model="gpt-4o",
    messages=[{ "role": "user", "content": user_input }]
  )
call_openai("Hello world")

```

Check out the [TypeScript client](https://docs.literalai.com/typescript-client/with-wrappers) to learn more about the `wrap` function.
### 
[​](https://docs.literalai.com/guides/logs#multimodal-llm)
Multimodal LLM
You can leverage multimodal capabilities on Literal AI in two ways:
  * Simple logging on API calls to Multimodal LLM APIs, like `gpt-4o`
  * Save multimodal files as [Attachments](https://docs.literalai.com/guides/logs#add-attachments-to-steps). Image, videos, audio and other files are shown as `Attachment` in the Literal AI platform, which can be accessed and downloaded via a `Step`.


![A logged multimodal LLM call](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/multimodal-playground.jpeg)
Example of a logged multimodal LLM call
## 
[​](https://docs.literalai.com/guides/logs#log-a-run)
Log a Run
A **Run** represents a trace of an Agent or Chain execution, capturing all intermediate steps and actions.
Runs can be logged manually using decorators or through framework integrations such as [Llama Index](https://docs.literalai.com/integrations/llama-index) or [LangChain](https://docs.literalai.com/integrations/langchain).
### 
[​](https://docs.literalai.com/guides/logs#log-a-run-with-intermediate-steps)
Log a Run with Intermediate Steps
Here’s how you can log a Run with intermediate steps using Python and TypeScript:
Python
TypeScript
Copy
```
@literalai_client.step(type="tool")
def get_temperature(city: str):
  return "10C"
@literalai_client.run
def my_agent(user_input: str):
  # Reusing the OpenAI call from the previous example
  call_openai(user_input)
  # Naive tool example
  get_temperature("paris")
  return "Success"
  
my_agent("Hello world")
# Wait for all steps to be sent. This is NOT needed in production code.
literalai_client.flush()

```

### 
[​](https://docs.literalai.com/guides/logs#add-metadata-and-tags-to-steps)
Add Metadata and Tags to Steps
**Tags** and **Metadata** can be added to both Runs and Steps to provide additional context and facilitate filtering and categorization.
Python
TypeScript
Copy
```
@literalai_client.run
def my_step(input):
  current_step = literalai_client.get_current_step()
  # some code, llm call, tool call, etc.
  current_step.metadata = {"region": "europe"}
  current_step.tags = ["to_review"]
  return "answer"

```

### 
[​](https://docs.literalai.com/guides/logs#add-attachments-to-steps)
Add Attachments to Steps
You can attach files to a Run or any of its intermediate steps, which is particularly useful for multimodal use cases.
![Attachments on a step](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/attachments.png)
Example of attachments
Python
TypeScript
Copy
```
@literalai_client.step(type="tool")
def load_document():
  with open ("./some.pdf", "rb") as file:
    literalai_client.api.create_attachment(
     name="pdf_document",
     content=file.read()
     )
  return "doc loaded"

```

### 
[​](https://docs.literalai.com/guides/logs#learn-more)
Learn More
The intermediate steps and the agent itself are logged using the `Step` class. You can learn more about the Step API in the following references:
## [Python Step API referenceLearn how to use the Python Step API.](https://docs.literalai.com/python-client/api-reference/step)## [TypeScript Step API referenceLearn how to use the TypeScript Step API.](https://docs.literalai.com/typescript-client/with-wrappers)
## 
[​](https://docs.literalai.com/guides/logs#log-a-thread)
Log a Thread
You can interact with an example Thread in the platform [here](https://cloud.getliteral.ai/thread/b3b61ec8-0d8a-444d-9e70-d6929c1129d1).
It is up to the application to keep track of the thread ID and pass it to the Literal AI client. Every run logged with the same thread ID will be part of the same conversation.
Here is an example:
Python
TypeScript
Copy
```
import uuid
def process_message(thread_id: str, user_input: str):
  with literalai_client.thread(thread_id=thread_id) as thread:
    # Reusing the Agent from the previous example
    my_agent(user_input)
thread_id = str(uuid.uuid4())
# Calling the agent a first time
process_message(thread_id=thread_id, user_input="foo")
# Calling the agent a second time with the same thread ID
process_message(thread_id=thread_id, user_input="bar")
# Wait for all steps to be sent. This is NOT needed in production code.
literalai_client.flush()

```

You can learn more about the Thread API in the following references:
## [Python Thread API referenceLearn how to use the Python Thread API.](https://docs.literalai.com/python-client/api-reference/thread)## [TypeScript Thread API referenceLearn how to use the TypeScript Thread API.](https://docs.literalai.com/typescript-client/with-wrappers)
### 
[​](https://docs.literalai.com/guides/logs#bind-a-thread-to-a-user)
Bind a `Thread` to a `User`
You can bind a `Thread` to a `User` to track their activity: quite handy for chatbots and conversational AIs!
Simply provide a unique `User` identifier, such as an email.
Python
TypeScript
Copy
```
# If the user `john.doe@example.com` does not exist, it's automatically created.
def process_message(thread_id: str, user_input: str):
  with literalai_client.thread(thread_id=thread_id, 
                 participant_id="john.doe@example.com") as thread:
    # Reusing the Agent from the previous example
    my_agent(user_input)

```

You can create a `User` at any time with the [create_user API](https://docs.literalai.com/python-client/api-reference/api#create-user). If your `User` already exists, you may update its `metadata` with the [update_user API](https://docs.literalai.com/python-client/api-reference/api#update-user).
The Literal AI client method `thread()` takes a `participant_id` (`participantId` in TypeScript) argument which accepts any of:
  * `User.id`: the unique ID of your `User` — it’s a UUID
  * `User.identifier`: the unique identifier of your `User` — it can be an email, a username, etc.


Careful with collisions when letting users pick their own `identifier`!
## 
[​](https://docs.literalai.com/guides/logs#log-to-a-specific-environment)
Log to a Specific Environment
Literal AI supports logging to different environments, which allows you to separate your development, staging, and production data: `dev`, `staging`, `prod`. This is particularly useful for managing your LLM application lifecycle. To specify an environment when initializing the LiteralClient, you can use the `environment` parameter:
Python
TypeScript
Copy
```
literalai_client = LiteralClient(environment="dev")

```

## 
[​](https://docs.literalai.com/guides/logs#log-with-a-release)
Log with a Release
Literal AI supports pairing your logs to a release, a release is a version of your deployed code to help you identify new issues and regressions.
This is particularly useful for managing your LLM application once in production. The value can be arbitrary, but we recommend Semantic Versioning, Calendar Versioning, or the Git commit SHA.
To specify a release when initializing the LiteralClient, you can use the `release` parameter:
Python
TypeScript
Copy
```
literalai_client = LiteralClient(release="81bec25")

```

Your logs will have a new key `release` in the metadata.
## 
[​](https://docs.literalai.com/guides/logs#log-a-distributed-trace)
Log a Distributed Trace
## [Distributed Tracing CookbookLearn how to log distributed traces with Literal AI.](https://github.com/Chainlit/literalai-cookbooks/tree/main/python/distributed-tracing)
## 
[​](https://docs.literalai.com/guides/logs#add-a-score)
Add a Score
**Scores** allow you to evaluate the LLM system performance at three levels: LLM generations, Agent Runs and Conversation Threads.
Scores can be human generated (human feedback, like a thump up or down), or AI generated (hallucination evaluation for instance).
They can be visualized on the dashboard charts and used as filters.
### 
[​](https://docs.literalai.com/guides/logs#add-a-user-feedback)
Add a User Feedback
Python
TypeScript
Copy
```
def add_user_feedback(run_id: str, value: int, comment: str):
  literalai_client.api.create_score(
    step_id=run_id,
    name="user-feedback",
    type="HUMAN",
    value=value,
    comment=comment,
  )

```

### 
[​](https://docs.literalai.com/guides/logs#add-a-product-related-metric)
Add a Product-Related Metric
Correlate your LLM system to a product metric, such as conversion, churn, upsell, etc. This can be done by:
  * Adding a specific product-related score on Literal AI.
  * Sending the logged run id to your analytics system, such as PostHog or Amplitude.


### 
[​](https://docs.literalai.com/guides/logs#add-an-ai-evaluation-result)
Add an AI Evaluation Result
Refer to [Online Evals](https://docs.literalai.com/guides/online-evals)
## 
[​](https://docs.literalai.com/guides/logs#fetch-existing-logs)
Fetch Existing Logs
You can fetch existing logs using the SDKs. Here is an example to fetch the last 5 threads where a user participated:
Python
TypeScript
Copy
```
from literalai import LiteralClient
literalai_client = LiteralClient()
user_id = 'uuid'
threads = literalai_client.api.list_threads(
  first=5,
  filters=[{"operator": "eq", "field": "participantId", "value": user_id}]
)
for d in threads.data:
  print(d.to_dict())
literalai_client.flush_and_stop()

```

More generally, you can fetch any Literal AI object. Check out the SDKs and API reference to learn how.
## 
[​](https://docs.literalai.com/guides/logs#on-literal-ai)
On Literal AI
### 
[​](https://docs.literalai.com/guides/logs#filter-logs)
Filter logs
Leverage the powerful filters on Literal AI. Use these same filters to export your data using the SDKs.
![Filter on existing logs](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/filter-logs.gif)
Filter on logs
### 
[​](https://docs.literalai.com/guides/logs#debug-logged-llm-generations)
Debug logged LLM generations
![Debug a logged LLM generation](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/debug-generation.gif)
Replay a logged LLM generation in the Playground
### 
[​](https://docs.literalai.com/guides/logs#add-tags-and-scores-from-the-ui)
Add Tags and Scores from the UI
You can add tags and scores directly from the user interface.
![Add a Tag](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/add-tag-thread.png)
Add a Tag to a Thread
## 
[​](https://docs.literalai.com/guides/logs#conclusion)
Conclusion
Logging with Literal AI is composable and unopinionated. It can be done at different levels depending on your use case.

---

# Online Evals

## Source
https://docs.literalai.com/guides/online-evals

Automatically evaluate your LLM logs in production, monitor performance and detect issues.
## 
[​](https://docs.literalai.com/guides/online-evals#from-the-literal-ai-platform)
From the Literal AI Platform
Automating the evaluation of your [Run outputs or LLM generations](https://docs.literalai.com/guides/logs#semantics) can significantly help detect patterns and areas of improvement for your LLM app in production, especially with large volumes of data.
An **Online Eval** is composed of:
  * **Name** : A name to identify the rule.
  * **Log Type** : Either `Agent Run` or `LLM Generation`, it’s the target to evaluate.
  * **Sample Rate** : The percentage of logs to evaluate.
  * **Filters** : Additional conditions to selectively evaluate certain logs.
  * **Scorer** : The scorer to use for the evaluation.


To create an Online Eval, go to the `Online Evals` page and click on the `+` button in the upper right corner of the table.
![Create Online Eval](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/create-online-eval.gif)
Create Online Eval
Once the Online Eval in place, your Runs or LLM Generations get automatically evaluated.
You can check the distribution of scores on an Online Eval’s page:
![Online Eval Scores Distribution](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/online-eval-distribution.png)
Online Eval Scores Distribution
If an Online Eval failed on a Run or LLM Generation, the `Log` column will show the error message.
## 
[​](https://docs.literalai.com/guides/online-evals#from-the-sdks)
From the SDKs
The SDKs provide `Score` creation APIs with all fields exposed.
If your metrics are code-based or combine LLM calls with arithmetic operations, like Ragas, you can directly use the SDKs to create scores from your application code.
Scores must be tied either to a `Step` or a `Generation` object. The concept of `Score` on a `Thread` is not well-defined at this stage.
Python
TypeScript
Copy
```
import os
from literalai import LiteralClient
literalai_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))
score = literalai_client.api.create_score(
  step_id="<STEP_UUID>",
  name="Sentiment",
  label="Positive",
  type="AI",
  comment="The user tipped $100, that means she is happy with the service.",
  value=1.0
)

```

Automation of actions based on evaluation results is coming soon!

---

# Playground

## Source
https://docs.literalai.com/guides/playground

The playground is a place where you can create, test, and debug prompts.
## [Try the Prompt Playground on Literal AICreate your first Prompt on the Playground and run experiments on multiple LLMs.](https://cloud.getliteral.ai)
## 
[​](https://docs.literalai.com/guides/playground#overview)
Overview
You can access the playground via:
  1. **The main menu** Directly access a fresh playground to create, test and save prompts from scratch.
  2. **Prompts** Start from an existing prompt template: edit and run experiments to validate your template/settings changes.
  3. **LLM Generations** Investigate Generations from a real chat conversation and reproduce production issues.


If you start from scratch, you can open example templates, and iterate from there.
Depicted below is a view of the Playground opened on a `Prompt`.
The selected prompt and version show in the upper left corner. Right of it, three actions:
  * `With Code`: Get a code snippet to [get the prompt programmatically](https://docs.literalai.com/guides/prompts#programmatically)
  * `Experiment on Dataset`: Run [experiments](https://docs.literalai.com/guides/experiment) against a dataset of your choice.
  * `Save`: Save the current prompt as a new version — check out [Prompt Management](https://docs.literalai.com/guides/prompts)


The center pieces labeled 1 and 2 are respectively the **Prompt Template** and the **Interactive Chat**. We describe their contents in the following sections.
![An example of a prompt template in the Playground.](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/playground-overview.png)
Playground Overview
## 
[​](https://docs.literalai.com/guides/playground#prompt-template)
Prompt Template
### 
[​](https://docs.literalai.com/guides/playground#template-and-variables)
Template and Variables
Let’s take a closer look at the Prompt Template section:
The Template and Variables parts (1 & 2 below) really contain that new “programming language” to instruct LLMs. You can add multiple messages to your prompt:
  * `System`: Instructions about the role of the assistant. For example, you can say that the assistant should provide concise answers.
  * `User`: The user’s input, often enriched with context.
  * `Assistant`: An LLM response, to simulate a conversation flow.
  * `Tool`: Response to a tool call.


![An example of a prompt template in the Playground.](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/prompt-template-playground.png)
Prompt Template
Avoid exposing variables on `System` messages and letting user input reach. Prefer to use variables in `User` and `Assistant` messages.
User messages typically contain variables to embed a user query along with context. The example above shows a user message with two variables:
  * `chunks`: The chunks retrieved, say from a vector database.
  * `question`: The user’s query.


You can specify default values for variables but those are usually defined in your application code.
Variables follow the [Mustache templating format](https://scalate.github.io/scalate/documentation/mustache.html#Syntax).
Double curly brackets (mustaches 😉) always surround variable names. A variable is written like `{{variable}}`.
If-statements and for-loops are written as `{{#x}} ... {{/x}}`:
  * if `x` is a boolean value, the section tags act like a conditional `if` statement
  * when `x` is an array, they behave like a `for` loop: access each element with `{{.}}`


### 
[​](https://docs.literalai.com/guides/playground#tools)
Tools
You can also declare tools on your prompt: given a tool description in proper JSON format, LLM models can determine whether a call to that tool is the next best action based on the user’s input.
If you need to specify the **return value** of a tool, use template messages above and select the `Tool` message type.
### 
[​](https://docs.literalai.com/guides/playground#output-types)
Output types
Choose the output format for your prompt:
![List of output formats.](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/playground-output.png)
Output formats
#### 
[​](https://docs.literalai.com/guides/playground#text)
Text
`text`: free-text response
#### 
[​](https://docs.literalai.com/guides/playground#json-mode)
JSON mode
`json_object`: JSON response with no specific schema enforced. Some providers ask to **explicitly** instruct the LLM to output JSON.
#### 
[​](https://docs.literalai.com/guides/playground#json-schema)
JSON schema
`json_schema`: JSON response following the given JSON schema.
#### 
[​](https://docs.literalai.com/guides/playground#score-schema)
Score schema
`score_schema`: specific to Literal AI and particular useful for LLM-as-a-Judge prompts. You can choose a Score Schema which forces your LLM to output a JSON of the form:
Copy
```
{
 "reason": "...", // The reason behind the chosen category
 "category": "..." // Category in score schema picked by LLM
}

```

Be advised that the format of `json_schema` itself varies from provider to provider. Make sure to follow your provider’s specification.
## 
[​](https://docs.literalai.com/guides/playground#llm-interaction)
LLM Interaction
Interaction with LLMs takes place in the central panel:
![LLM interaction in the playground.](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/playground-interaction.png)
LLM Interaction
### 
[​](https://docs.literalai.com/guides/playground#llm-settings)
LLM Settings
Atop, you will find the LLM settings. Select any configured LLM provider and pick a model. The settings icon lets you choose temperature, stop sequences, etc.
![Provider & Model](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/playground-llm-model.png)Provider & Model| ![LLM settings](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/playground-llm-settings.png)LLM Settings  
---|---  
Here’s a recap on classic LLM settings:
**Temperature** : Controls randomness. Lower temperatures result in less random generations (0 = deterministic). The higher the temperature, the closer to uniform token sampling.
**Maximum Length** : Maximum number of tokens to generate. Limits vary from model to model: anywhere between 1024 and 32,192.
**Stop Sequences** : Use up to four sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
**Top P** or Nucleas Sampling: Controls diversity by restricting tokens to consider. Higher `Top P` values consider more possible tokens, even the less likely ones, which makes the generated text more diverse.
**Frequency Penalty** : Penalizes new tokens based on their existing frequency in the text so far. Decreases the model’s likelihood to repeat the same line verbatim.
**Presence Penalty** : Penalizes new tokens based on whether they appear in the text so far. Increases the model’s likelihood to talk about new topics.
### 
[​](https://docs.literalai.com/guides/playground#try-out-%26-in-context-debugging)
Try out & in-context debugging
The bottom box lets you input a multimodal message (text, image, etc.) to send to the LLM. You may also add messages from within the canvas to simulate a conversation flow / test a new idea.
If you accessed the playground via an LLM call, you get the full conversation context. A must to troubleshoot production issues!
Use `Cmd+Enter` to send your message.
### 
[​](https://docs.literalai.com/guides/playground#multiple-llms)
Multiple LLMs
Finally, leverage multiple tabs to simultaneously run your prompt against different LLMs / settings. Selectively run tabs with the play button on the top right of each tab.
![Multiple tabs in the playground.](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/playground-tabs.gif)
Multiple tabs
Once satisfied with your “vibes check”, it’s time to run [experiments](https://docs.literalai.com/guides/experiment) on larger datasets.
Saving, A/B testing and deploying your prompt comes in last. Check out the [Prompts](https://docs.literalai.com/guides/prompts) documentation.

---

# Prompts

## Source
https://docs.literalai.com/guides/prompts

Prompt Management enables collaboration between engineering and product teams to create, version, A/B test, debug, and monitor prompts directly from Literal AI.
![Literal AI Platform](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/prompt-management.svg)
Prompt Management: prompts can be stored in either Literal AI or in your application code.
## 
[​](https://docs.literalai.com/guides/prompts#what-is-a-prompt-template%3F)
What is a Prompt Template?
A **Prompt** or **Prompt Template** in Literal AI consists of:
  * the **prompt** i.e. the messages, and its variables
  * the **LLM provider** i.e. OpenAI, Anthropic, etc.
  * the **LLM settings** i.e. temperature, top P, etc.


Prompts in Literal AI allow you to:
  * Enable product teams and domain experts to draft and iterate on prompts.
  * Deploy new prompt versions without redeploying your code.
  * Track which prompt version was used for a specific generation. Improve prompts by debugging logged LLM generations with context.
  * Compare prompt versions, LLM models, settings and providers to determine which one performs better.
  * Collaborate effectively with your team.


## 
[​](https://docs.literalai.com/guides/prompts#create-a-prompt)
Create a Prompt
Go to the Prompt Playground to create your first prompt template. On the left panel, define the template messages. In each template message, you can define variables following the [Mustache](https://mustache.github.io/mustache.5.html) syntax.
![Literal AI Platform](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/create-prompt.gif)
Create a Prompt Template
You can try out the prompt template by setting values for the variables, picking an LLM provider/model and clicking on `Submit`.
The Prompt Playground supports multiple LLM providers/models and you can switch between them to see how the prompt behaves.
Once you are happy with the prompt, you click on `Save`.
### 
[​](https://docs.literalai.com/guides/prompts#create-a-new-version)
Create a new version
Whether to fix a bug or to add a new feature, you can create a new version of a prompt template.
![Create a New Version](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/create-prompt-version.gif)
Create a New Version
To do so, go to the Prompts page, select the prompt template you want to iterate on. Then select the version of the prompt you want to create a new version from.
Edit the template messages and click on `Save` when you are done. A diff of the changes will be displayed. Optionally provide a changelog to keep track of the changes.
### 
[​](https://docs.literalai.com/guides/prompts#programmatically)
Programmatically
If you prefer to keep your Prompt Template in your code, you can still version it on Literal AI. Note that A/B testing is not available if you manage prompt templates in your code.
See [installation](https://docs.literalai.com/get-started/quick-start) to get your API key and instantiate the SDK
Python
TypeScript
Copy
```
PROMPT_NAME = "RAG prompt"
template_messages = [
  {
    "role": "user",
    "content": """Answer the question based on the context below.
Context:
{{#chunks}}
{{.}}
{{/chunks}}
Query:
{{query}}
Answer:"""
  }
]
prompt = literalai_client.api.get_or_create_prompt(
  name=PROMPT_NAME,
  template_messages=template_messages
  #tools=...
  #settings=...
)

```

Literal AI will check if the prompt changed compared to the last version and create a new version if needed.
## 
[​](https://docs.literalai.com/guides/prompts#pull-a-prompt-template-from-literal-ai)
Pull a Prompt Template from Literal AI
If your prompt templates live on Literal AI, you will have to pull them in your app before using them.
Prompt templates are cached to ensure fast access. Caching happens server-side, we are working to provide client-side caching!
Python
TypeScript
Copy
```
# Not specifying the version will use the A/B testing rollout probabilities
prompt = literalai_client.api.get_prompt(name="RAG prompt")
# Specifying the version will use the exact version
prompt = literalai_client.api.get_prompt(name="RAG prompt", version=0)

```

## 
[​](https://docs.literalai.com/guides/prompts#format-a-prompt-template)
Format a Prompt Template
Once you have your prompt instance, you can format it with the relevant variables.
### 
[​](https://docs.literalai.com/guides/prompts#format-to-openai-format)
Format to OpenAI format
Python
TypeScript
Copy
```
query = "What are the features of Literal AI?"
chunks = ["Literal AI is an LLM Ops platform"]
messages = prompt.format_messages(query=query, chunks=chunks)

```

### 
[​](https://docs.literalai.com/guides/prompts#convert-to-langchain-chat-prompt)
Convert to LangChain Chat Prompt
Python
TypeScript
Copy
```
langchain_prompt = prompt.to_langchain_chat_prompt_template()
# Use langchain_prompt as any other LangChain prompt

```

Coupled with integrations like [OpenAI](https://docs.literalai.com/integrations/openai) or [LangChain](https://docs.literalai.com/integrations/langchain), the Literal AI SDK will not only log the generations but also track which prompt versions were used to generate them.
This is especially useful to track the performance of your prompt versions and debug in context.
## 
[​](https://docs.literalai.com/guides/prompts#a%2Fb-test-a-prompt)
A/B test a Prompt
Progressively roll out new prompt versions or LLMs in production using A/B testing:
  * Increases confidence in deployments by allowing gradual rollout and comparison of different versions.
  * Empowers product teams to implement and test prompt improvements independently, reducing reliance on engineering teams.
  * Enables data-driven decision making by comparing performance metrics between versions.
  * Facilitates rapid iteration and optimization of prompts in a production environment.
  * Minimizes risk by allowing easy rollback if a new version underperforms.


A/B Testing prompt versions in production
### 
[​](https://docs.literalai.com/guides/prompts#from-literal-ai)
From Literal AI
When pulling a prompt template without specifying a version, Literal AI will use the A/B testing rollout probabilities to select the version to use.
![Literal AI Platform](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/prompt-ab-testing.gif)
Setting Prompt Template A/B Testing
By default, version `v0` has a `100 %` rollout. You can change the rollout splits from the Prompt details page page.
### 
[​](https://docs.literalai.com/guides/prompts#from-code)
From Code
You can also update the A/B testing rollout probabilities from code:
Python
TypeScript
Copy
```
literalai_client.api.update_prompt_ab_testing(
  "MY_PROMPT",
  [{ "version": 0, "rollout": 0.5 }, 
   { "version": 1, "rollout": 0.5 }]
)

```

## 
[​](https://docs.literalai.com/guides/prompts#iterate-on-prompts)
Iterate on Prompts
You can test a specific version of a prompt against a dataset and a set of scorers to measure its performance and avoid regressions. You can find more information in the [experiments](https://docs.literalai.com/guides/experiment#run-an-experiment-on-literal-ai) guide.

---

# Scorers

## Source
https://docs.literalai.com/guides/scorers

Scorers are functions (code or LLM-as-a-judge) that take an input, output and expected output and return a score.
Scorers are the basic building blocks of evaluation. They are used to score `Steps` or `Generations` based on a `Score Schema`.
## 
[​](https://docs.literalai.com/guides/scorers#from-the-literal-ai-platform)
From the Literal AI Platform
To create a scorer, go to the `Scorers` page and click on the `+` button in the upper right corner of the table.
### 
[​](https://docs.literalai.com/guides/scorers#llm-as-a-judge)
LLM-as-a-judge
Here’s an example of the flow:
![Create Scorer](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/create-scorer.gif)
Create Scorer
Once you have created a scorer, your next step is to use it.
## [Online EvalsLearn how to evaluate your LLM logs in production automatically.](https://docs.literalai.com/guides/online-evals)## [Prompt ExperimentsLearn how to run experiments from the Playground.](https://docs.literalai.com/guides/experiment)
## 
[​](https://docs.literalai.com/guides/scorers#from-the-sdks)
From the SDKs
For custom scorers, you can leverage the Literal AI SDK to [add scores](https://docs.literalai.com/guides/logs#add-a-score) to your `Steps` or `Generations`.

---

# General

## Source
https://docs.literalai.com/guides/settings/general

## 
[​](https://docs.literalai.com/guides/settings/general#literal-ai-projects)
Literal AI Projects
You can have multiple projects in Literal AI, each with its own set of members, roles, and settings. Projects are the primary way to organize your work and collaborate with your team.
### 
[​](https://docs.literalai.com/guides/settings/general#project-settings)
Project settings
Project-level settings in Literal AI contain:
  * **General** information: 
    * Manage Literal AI API keys to log your application runs.
    * Tags: Customize and manage tags to categorize and label your data
    * Project lifecycle: Manage your project’s lifecycle, including deletion.
  * **LLM** : Manage provider credentials for the LLMs used in your project and define costs per model.
  * **Scoring** : Define and manage Score Schemas to consistently evaluate and measure your AI models’ performance.
  * **Team** : Manage your project’s members, roles and permissions.


### 
[​](https://docs.literalai.com/guides/settings/general#manage-literal-ai-api-keys)
Manage Literal AI API Keys
To copy or create a new API key, go to the `Settings` page and look for the **Literal AI API Keys** section.
Each API key is associated with a specific project, user and role.

---

# LLM

## Source
https://docs.literalai.com/guides/settings/llm

Manage your LLM credentials and costs.
## 
[​](https://docs.literalai.com/guides/settings/llm#credentials)
Credentials
### 
[​](https://docs.literalai.com/guides/settings/llm#supported-providers)
Supported providers
Credentials are stored online, and keys are encrypted.
Literal AI supports all major LLM providers:
  * OpenAI
  * Mistral
  * Anthropic
  * Google
  * Azure OpenAI
  * Amazon Bedrock
  * Groq


Adding a credential is as simple as providing an API key:
![Add a credential](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/add-credentials.gif)
Add a credential
Literal AI comes with a set of pre-configured models for each provider, but you can add your own. _Custom Models_ should be the list of models you fine-tuned on specific providers.
All credentials can be **used** by any Admin or AI Engineer on your team. Once added, credentials are not visible anymore. Admins may edit or delete credentials.
**Azure OpenAI** and **Amazon Bedrock** have additional fields to configure.
For Azure OpenAI, you need to map the Azure OpenAI endpoint to the following format:
`https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2024-06-01`
For instance, `https://my_instance.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-06-01`
would map to the following Literal AI provider:
![An example of an Azure OpenAI credential configuration.](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/azureopenai.png)
Azure OpenAI Credential Configuration
No need to add a base URL to the endpoint!
### 
[​](https://docs.literalai.com/guides/settings/llm#custom-providers)
Custom providers
If your LLM provider does not fall into one of the above, you may define your own, custom.
In addition to the API key, you provide a base URL and the available models.
The chat completions endpoint should follow OpenAI’s API format.
![An example of a custom provider.](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/add-custom-provider.gif)
Custom provider
Make sure your endpoint is reachable from the Internet! Especially if using LM Studio with `http://localhost:1234/v1` 😉
## 
[​](https://docs.literalai.com/guides/settings/llm#cost-tracking)
Cost Tracking
Keep an eye on the financial aspects of your LLM application:
1
Access LLM Settings
Navigate to the “Settings” section and select the “LLM” tab.
2
Configure Model Costs
Configure the cost per token for each model you’re using to enable precise cost tracking:
  * **Pattern** : Specify a regular expression to match the model name (e.g., “gpt-4o*” for all GPT-4o variants)
  * **Input Price** : Set the cost for input tokens in USD per million tokens
  * **Output Price** : Set the cost for output tokens in USD per million tokens
  * **Period** : Optionally define start and end dates for time-specific pricing (useful for handling price changes)


You can add multiple model configurations to accurately track expenses across your LLM providers.
3
Monitor Costs in the Dashboard
Return to the main dashboard to view cost metrics over time, including total cost, cost per conversation, and cost breakdowns by model.
![Cost Metrics in the Dashboard](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/model-costs.png)
Cost Metrics in the Dashboard

---

# Scoring

## Source
https://docs.literalai.com/guides/settings/scoring

## 
[​](https://docs.literalai.com/guides/settings/scoring#score-schemas)
Score Schemas
Literal AI offers an easy way to manage scores: **Score Schemas**.
With **Score Schemas** , admin users can control and expose to all users the various types of evaluations allowed in their application. **Score Schemas** come in two flavors:
  * **Categorical** — Schemas let you create a set of categories, each tied to a numeric value.
  * **Continuous** — Schemas with a **[min; max]** range which users score in.


For **Categorical** Schemas, each category has a description which is used to guide the scorer (be it **Human** or **AI**) in selecting the most relevant category.
Here’s how to create a Score Schema:
![Create a Score Schema](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/create-score-schema.gif)
Create Score Schema
Feel free to browse your steps from the Logs tab and simply add a “Human Review” directly on your `Step` and `Generation` entities:
![Score a Generation](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/score-generation.gif)
Score a Generation
To go through a wide range of logs, leverage [annotation queues](https://docs.literalai.com/guides/annotation-queue) which make it easier to streamline the review process.

---

# Team

## Source
https://docs.literalai.com/guides/settings/team

## 
[​](https://docs.literalai.com/guides/settings/team#create-%26-invite-members-to-a-project)
Create & Invite Members to a Project
To create a new project, click on the Project switcher at the bottom of the sidebar and click on `Create Project`.
Create a New Project
Once a project is created, you can invite others to collaborate:
  1. Go to the project `Settings`.
  2. Go to the `Team` tab.
  3. Click on the `+` button.


### 
[​](https://docs.literalai.com/guides/settings/team#role-based-access-control-rbac)
Role Based Access Control (RBAC)
Project administrators hold the responsibility of assigning and managing permissions for all team members. Permissions in Literal AI are packaged as the following set of roles :
#### 
[​](https://docs.literalai.com/guides/settings/team#supported-roles)
Supported roles
  * **Admin** : Full access to the project, including team management, project settings, and all resources.
  * **AI Engineer** : Manages observability data, API keys, and AI prompts. Cannot alter project settings or team roles.
  * **Domain Expert** : Reads observability data, scores, and tags data. Limited access to project resources and no API key or project settings management.


Feel free to reach out to us to create a custom role: contact@literalai.com
#### 
[​](https://docs.literalai.com/guides/settings/team#permissions-per-role)
Permissions per role
Admin| AI Engineer| Domain Expert  
---|---|---  
Annotations (scores, tags…)| Read & Write| Read & Write| Read & Write  
Observability (threads, steps…)| Read & Write| Read & Write| Read  
Datasets (items, experiments…)| Read & Write| Read & Write| Read  
Prompt (playground, versions…)| Read & Write| Read & Write  
API Keys & Credentials| Read & Write| Read & Write  
Project & Team| Read & Write| Read

---

# Users

## Source
https://docs.literalai.com/guides/users

Understand your end users, and how they interact with your application.
You can review user engagement from the **Users** page.
## 
[​](https://docs.literalai.com/guides/users#access-threads-for-a-specific-user)
Access threads for a specific user
Directly jump to `Threads` for specific users, check when they last engaged with the conversation.
![Users Page](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/application-users.png)
Users Page
## 
[​](https://docs.literalai.com/guides/users#filter-by-engagement)
Filter by engagement
Filter users based on their engagement timeframes:
  * Select specific date ranges to analyze user activity
  * Use preset periods (last 7 days, last 30 days, etc.)
  * Track engagement patterns over time
  * Identify active vs inactive users


Follow proper privacy and consent guidelines when collecting user information.

---

# Chainlit

## Source
https://docs.literalai.com/integrations/chainlit

1
Create a Project and copy your API key
Create a project [here](https://cloud.getliteral.ai) and copy your Literal AI API key.
2
Run your Chainlit application
To start monitoring your Chainlit application, just set the `LITERAL_API_KEY` environment variable and run your application as you normally would.
Literal AI will automatically start monitoring your Chainlit application.
You **DO NOT** need to add any additional code, such as Literal AI decorators or instrumentation to your Chainlit application to start monitoring it with Literal AI.

---

# LangChain/LangGraph

## Source
https://docs.literalai.com/integrations/langchain

The Langchain integration enables to monitor your Langchain agents and chains with a single line of code.
You should create a new instance of the callback handler for each invocation.
The LangChain integration already support LLM tracing. You should not use it in conjunction with other LLM provider integrations such as [OpenAI](https://docs.literalai.com/integrations/openai).
Python
TypeScript
Copy
```
import os
from literalai import LiteralClient
from langchain_openai import ChatOpenAI
from langchain.schema.runnable.config import RunnableConfig
from langchain.schema import StrOutputParser
from langchain.prompts import ChatPromptTemplate
literalai_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))
cb = literalai_client.langchain_callback()
# optional: attach to a prompt template on Literal AI
# prompt = literalai_client.api.get_or_create_prompt(...)
# prompt_template = prompt.to_langchain_chat_prompt_template()
prompt_template = ChatPromptTemplate.from_messages(
  ['human', 'Tell me a short joke about {topic}']
)
model = ChatOpenAI(streaming=True)
runnable = prompt_template | model | StrOutputParser()
res = runnable.invoke(
  {"topic": "ice cream"},
  config=RunnableConfig(callbacks=[cb], run_name="joke")
  )

```

## 
[​](https://docs.literalai.com/integrations/langchain#multiple-langchain-calls-in-a-single-thread)
Multiple langchain calls in a single thread
You can combine the Langchain callback handler with the concept of [Thread](https://docs.literalai.com/guides/logs) to monitor multiple langchain calls in a single thread.
Python
TypeScript
Copy
```
import os
from literalai import LiteralClient
literalai_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))
with literalai_client.thread(name="Langchain example") as thread:
  cb = literalai_client.langchain_callback()
  # Call your Langchain agent here

```

## 
[​](https://docs.literalai.com/integrations/langchain#adding-tags%2C-metadata-or-a-step-id)
Adding tags, metadata or a Step ID
If you use LangChain’s built-in tags and metadata, they will be added to the Literal AI generations. Additionally, you can specify a Step ID to ensure a generation is logged with this Step ID.
Copy
```
import { v4 as uuidv4 } from 'uuid';
import { ChatOpenAI } from '@langchain/openai';
const client = new LiteralClient({ apiKey, apiUrl });
const cb = client.instrumentation.langchain.literalCallback();
const model = new ChatOpenAI({});
const literalaiStepId = uuidv4();

await model.invoke('Hello, how are you?', {
 callbacks: [cb],
 metadata: {
  key: 'value',
  // use literalaiStepId in the metadata to specify a Step ID
  literalaiStepId,
 },
 tags: ['tag1', 'tag2'],
});

```

## 
[​](https://docs.literalai.com/integrations/langchain#langgraph)
LangGraph
LangGraph works similarly to LangChain when it comes to using the Literal AI callback handler.
Python
Typescript
Copy
```
# list imports 
workflow = StateGraph(MessagesState)
# define graph ...
app = workflow.compile(checkpointer=checkpointer)
# run the app with LangChain callback handler
cb = literalai_client.langchain_callback()
final_state = app.invoke(
  {"messages": [HumanMessage(content="what is the weather in sf")]},
  config=RunnableConfig(callbacks=[cb])
)

```

## [PythonCheck out this Python LangGraph example.](https://github.com/Chainlit/literalai-cookbooks)## [TypescriptCheck out this Typescript LangGraph example.](https://github.com/Chainlit/literalai-cookbooks/typescript/langchain-langgraph/README.md)
## 
[​](https://docs.literalai.com/integrations/langchain#link-a-literal-ai-prompt-to-a-langchain%2Flanggraph-run)
Link a Literal AI Prompt to a LangChain/LangGraph Run
Python
TypeScript
Copy
```
prompt = literalai_client.api.get_prompt(name="RAG prompt")
langchain_prompt = prompt.to_langchain_chat_prompt_template()
# Use langchain_prompt as any other LangChain prompt

```

The Literal AI SDK will not only log the generations but also track which prompt versions were used to generate them.
This is especially useful to track the performance of your prompt versions and debug in context.

---

# LiteLLM

## Source
https://docs.literalai.com/integrations/litellm

[LiteLLM](https://litellm.ai) allows you to interact with 100+ LLMs seamlessly using a consistent OpenAI-compatible format, either use their python SDK or their proxy server.
Starting from LiteLLM v1.48.12, you can:
  * **Log** LLM calls to Literal AI and **evaluate** your LLM or prompt performance
  * Create **multi-step traces** with Literal AI decorators
  * Bind **Prompt Templates** directly to LiteLLM calls


## 
[​](https://docs.literalai.com/integrations/litellm#pre-requisites)
Pre-Requisites
Ensure you have the `literalai` package installed:
Copy
```
pip install literalai litellm

```

## 
[​](https://docs.literalai.com/integrations/litellm#quick-start)
Quick Start
Copy
```
import litellm
import os
os.environ["LITERAL_API_KEY"] = ""
os.environ['OPENAI_API_KEY']= ""
os.environ['LITERAL_BATCH_SIZE'] = "1" # You won't see logs appear until the batch is full and sent
litellm.success_callback = ["literalai"] # Log Input/Output to LiteralAI
litellm.failure_callback = ["literalai"] # Log Errors to LiteralAI
# openai call
response = litellm.completion(
 model="gpt-4o-mini",
 messages=[
  {"role": "user", "content": "Hi 👋 - i'm openai"}
 ]
)

```

## 
[​](https://docs.literalai.com/integrations/litellm#multi-step-traces)
Multi Step Traces
This integration is compatible with the Literal AI SDK decorators, enabling conversation and agent tracing
Copy
```
import litellm
from literalai import LiteralClient
import os
os.environ["LITERAL_API_KEY"] = ""
os.environ['OPENAI_API_KEY']= ""
os.environ['LITERAL_BATCH_SIZE'] = "1" # You won't see logs appear until the batch is full and sent
litellm.input_callback = ["literalai"] # Support other Literal AI decorators and prompt templates
litellm.success_callback = ["literalai"] # Log Input/Output to LiteralAI
litellm.failure_callback = ["literalai"] # Log Errors to LiteralAI
literalai_client = LiteralClient()
@literalai_client.run
def my_agent(question: str):
  # agent logic here
  response = litellm.completion(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": question}],
    metadata={"literalai_parent_id": literalai_client.get_current_step().id}
  )
  return response
my_agent("Hello world")
# Waiting to send all logs before exiting, not needed in a production server
literalai_client.flush()

```

Learn more about [Literal AI logging capabilities](https://docs.literalai.com/guides/logs).
## 
[​](https://docs.literalai.com/integrations/litellm#bind-a-generation-to-its-prompt-template)
Bind a Generation to its Prompt Template
This integration works out of the box with prompts managed on Literal AI. This means that a specific LLM generation will be bound to its template.
Learn more about [Prompt Management](https://docs.literalai.com/guides/prompts) on Literal AI.
## 
[​](https://docs.literalai.com/integrations/litellm#openai-proxy-usage)
OpenAI Proxy Usage
If you are using the Lite LLM proxy, you can use the Literal AI OpenAI instrumentation to log your calls.
Copy
```
from literalai import LiteralClient
from openai import OpenAI
client = OpenAI(
  api_key="anything",      # litellm proxy virtual key
  base_url="http://0.0.0.0:4000" # litellm proxy base_url
)
literalai_client = LiteralClient(api_key="")
# Instrument the OpenAI client
literalai_client.instrument_openai()
settings = {
  "model": "gpt-4o-mini", # model you want to send litellm proxy
  "temperature": 0,
  # ... more settings
}
response = client.chat.completions.create(
  messages=[
    {
      "role": "system",
      "content": "You are a helpful bot, you always reply in Spanish"
    },
    {
      "role": "user",
      "content": message.content
    }
  ],
  **settings
)

```

---

# Llama Index

## Source
https://docs.literalai.com/integrations/llama-index

The Llama Index integration enables to monitor your RAG pipelines with a single line of code:
LlamaIndex instrumentation
Copy
```
literalai_client.instrument_llamaindex()

```

The Llama Index integration already support LLM tracing. You should not use it in conjunction with other LLM provider integrations such as [OpenAI](https://docs.literalai.com/integrations/openai).
The Llama Index integration in the Python SDK is compatible with Llama Index starting with version 0.10.58.
LlamaIndex offers a variety of concepts to interact with LLMs:
  * Query Engines
  * LLMs
  * Agents


We explain what each concept leads to in terms of `Thread`, `Run` and `Generation` logs, and show you a visual of what you can expect on Literal AI.
# 
[​](https://docs.literalai.com/integrations/llama-index#query-engines)
Query Engines
Python
Copy
```
from literalai import LiteralClient
from llama_index.core import Document, VectorStoreIndex
client = LiteralClient()
# The magic happens here
client.instrument_llamaindex()
index = VectorStoreIndex.from_documents([Document.example()])
query_engine = index.as_query_engine()
# Here we are outside of a thread so the integration will create one for us
query_engine.query("Tell me about LLMs")
# If we call LlamaIndex from inside a context, the integration will use it
with client.thread(name="Test LlamaIndex Thread") as thread:
  query_engine.query("Tell me about LLMs")
# If we make several calls inside the context, they will all be logged in the same thread
with client.thread(name="LlamaIndex Questions") as thread:
  query_engine.query("Tell me about LLMs")
  query_engine.query("What is RAG ?")
# Optional: make sure to flush the events before stopping the client
client.flush_and_stop()

```

Each `Thread` will result in the following tree on Literal AI :
![A Llamaindex RAG thread on Literal AI](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/llamaindex-thread.png)
A LlamaIndex RAG thread on Literal AI
# 
[​](https://docs.literalai.com/integrations/llama-index#llms)
LLMs
LlamaIndex offers wrappers around LLM providers to interact with their APIs.
### 
[​](https://docs.literalai.com/integrations/llama-index#llm-chat)
`llm.chat`
The methods `llm.chat` and `llm.stream_chat` both generate a standalone `Generation`:
![A LlamaIndex LLM call - Standalone Generation](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/llamaindex-llm-call.png)
A LlamaIndex LLM call - Standalone Generation
Please note that LlamaIndex token usage is not available for streaming methods due to limitations in the event data present on the `LLMChatEndEvent` for chunk completions.
However, the Literal AI platform defaults token counts computation to the `cl100k_base` tokenizer which is a fair approximation of the expected token usage.
### 
[​](https://docs.literalai.com/integrations/llama-index#llm-predict-and-call)
`llm.predict_and_call`
The `llm.predict_and_call` also results in a standalone `Generation` on the Literal AI platform.
Specifically, LlamaIndex does not trigger events related to tool calls and we recommend decorating your tools’ function definitions with `@literalai_client.step(type="tool", name="My Tool")` to view the calls performed.
Note that a `Step` of type `tool` cannot be standalone on the Literal AI platform and we thus recommend you to add a contextual `Step` wrapper around your `llm.predict_and_call` call, as such:
LlamaIndex llm.predict_and_call
Copy
```
with literalai_client.step(type="run", name="Predict & Call")
  llm.predict_and_call(...)

```

# 
[​](https://docs.literalai.com/integrations/llama-index#agents)
Agents
LlamaIndex has the concept of agent as part of its `FunctionCallingAgent` and specifically derived an `OpenAIAgent` with specificities to the OpenAI model offerings.
Function calling agents can be tuned in a variety of ways, but the general idea is that they iteratively perform the configured LLM calls with tool options until the LLM deems it unnecessary to call a tool.
When calling an `agent.chat`, you can expect to obtain a “run” `Step` of the following form:
![A LlamaIndex agent chat - Agent Run with multiple intermediate steps](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/llamaindex-agent-chat.png)
A LlamaIndex agent chat - Agent Run with multiple intermediate steps
The tool calls in the stack above show only because the functions themselves are decorated with an `@literalai_client.step(type="tool", name="My Tool")`.

---

# Other LLM Providers (Anthropic, etc.)

## Source
https://docs.literalai.com/integrations/llm-providers

Monitor all LLM providers via LangChain simple unified interface `init_chat_model`:
Python
TypeScript
Copy
```
from langchain.chat_models import init_chat_model
from literalai import LiteralClient
literalai_client = LiteralClient()
gpt_4o = init_chat_model("gpt-4o", model_provider="openai", temperature=0)
claude_sonnet = init_chat_model("claude-3-5-sonnet-20241022", model_provider="anthropic", temperature=0)
# Literal AI callback
cb = literalai_client.langchain_callback()
# Invoke the model with input and callback configuration
gpt_4o.invoke("what's your name", config=RunnableConfig(callbacks=[cb]))
claude_sonnet.invoke("what's your name", config=RunnableConfig(callbacks=[cb]))
literalai_client.flush()

```

You can thus monitor Anthropic, Mistral AI, Cohere and many other LLM providers. Full list [here](https://python.langchain.com/docs/integrations/platforms/).

---

# LLM Inference servers (vLLM, etc.)

## Source
https://docs.literalai.com/integrations/messages-server

By integrating the OpenAI SDK with Literal AI’s instrumentation, you can also effectively monitor **message-based inference servers such as LMStudio, vLLM or HuggingFace** , ensuring that you have full visibility into the performance and usage of your AI models.
lmstudio.py
example.ts
Copy
```
from literalai import LiteralClient
lc = LiteralClient()
lc.instrument_openai()
# Example: reuse your existing OpenAI setup
from openai import OpenAI
# Point to the local server
client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")
completion = literalai_client.chat.completions.create(
 model="TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf",
 messages=[
  {"role": "system", "content": "Always answer in rhymes."},
  {"role": "user", "content": "Introduce yourself."}
 ],
 temperature=0.7,
)
print(completion.choices[0].message)

```

The same works for HuggingFace [messages API](https://huggingface.co/docs/text-generation-inference/en/messages_api) with
Copy
```
base_url="https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2/v1"

```

---

# Mistral AI

## Source
https://docs.literalai.com/integrations/mistralai

You can use the Literal AI platform to instrument Mistral AI API calls. This allows you to track and monitor the usage of Mistral AI API calls in your application and replay them in the Prompt Playground.
The Mistral AI instrumentation supports sync, async, streamed and regular responses!
## 
[​](https://docs.literalai.com/integrations/mistralai#instrumenting-mistral-ai-api-calls)
Instrumenting Mistral AI API calls
Python
Copy
```
from mistralai import Mistral, UserMessage
import json, os
from literalai import LiteralClient
literalai_client = LiteralClient(api_key="")
literalai_client.instrument_mistralai()
client = Mistral(api_key="")
model = "mistral-large-latest"
messages = [
  {
    "role": "user",
    "content": "What is the best French cheese?",
  }
]
# With streaming
stream_response = client.chat.stream(
  model = model,
  messages = messages,
)
for chunk in stream_response:
  print(chunk.data.choices[0].delta.content)
# Now you can use the OpenAI API as you normally would

```

---

# OpenAI

## Source
https://docs.literalai.com/integrations/openai

You can use the Literal AI platform to instrument OpenAI API calls. This allows you to track and monitor the usage of the OpenAI API in your application and replay them in the Prompt Playground.
The OpenAI instrumentation supports **completions** , **chat completions** , and **image generation**. It handles both regular and streamed responses.
## 
[​](https://docs.literalai.com/integrations/openai#instrumenting-openai-api-calls)
Instrumenting OpenAI API calls
Python
TypeScript
Copy
```
import os
from literalai import LiteralClient
"""
You need to call the `instrument_openai` method from the Literal AI client to
enable the integration. Call it before any OpenAI API call.
"""
literalai_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))
literalai_client.instrument_openai()
# Now you can use the OpenAI API as you normally would

```

## 
[​](https://docs.literalai.com/integrations/openai#with-threads-and-runs)
With Threads and Runs
You can use [Threads and Runs](https://docs.literalai.com/guides/logs) on top of the OpenAI API to create structured and organized logs.
Python
TypeScript
Copy
```
import os
from literalai import LiteralClient
from openai import OpenAI
openai_client = OpenAI()
literalai_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))
literalai_client.instrument_openai()
@literalai_client.step(type="run")
def my_assistant(user_query: str):
  completion = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
          {
            "role": "user",
            "content": user_query,
          }
        ],
      )
  return completion.choices[0].message.content

def main():
  with literalai_client.thread(name="Example") as thread:
    initial_user_query = "Hello, how are you?"
    my_assistant(initial_user_query)
    
    follow_up_query = "Follow up query"
    my_assistant(follow_up_query)
main()
# Network requests by the SDK are performed asynchronously.
# Invoke flush to guarantee the completion of all requests prior to the process termination.
# WARNING: If you run a continuous server, you should not use this method.
literalai_client.flush()

```

## 
[​](https://docs.literalai.com/integrations/openai#adding-tags-and-metadata)
Adding Tags and Metadata
You can add tags and metadata to the Generations created by the instrumentation.
Python
TypeScript
Copy
```
completion = openai_client.chat.completions.create(
  model="gpt-4o-mini",
  messages=[
    # ...
  ],
  # Only tags are supported right now on the Python client
  literalai_tags=["tag1", "tag2"],
  literalai_metadata={"key": "value"},
)

```

## 
[​](https://docs.literalai.com/integrations/openai#openai-assistants)
OpenAI Assistants
You can sync your OpenAI Assistant threads with Literal AI in a few lines of code.
TypeScript
Copy
```
import OpenAI from 'openai';
import { LiteralClient, User } from '@literalai/client';
const openai = new OpenAI();
const literalAiClient = new LiteralClient({apiKey: process.env["LITERAL_API_KEY"]});
const syncer = literalAiClient.openai(openai).assistant.syncer;
async function main() {
 // You can sync a thread at any moment. We recommend to sync it once you get a `completed` run status.
 const threadId = 'THREAD_ID_TO_SYNC';
 // Optional: Add/update a user to the thread. Use any unique identifier you like.
 const user = new User({ identifier: 'willy', metadata: { name: 'Willy' } });
 await syncer.syncThread(threadId, user);
}
main();

```

---

# OpenLLMetry

## Source
https://docs.literalai.com/integrations/openllmetry

Literal AI supports [OpenLLMetry](https://github.com/traceloop/openllmetry) and its integrations.
You can use the OpenLLMetry integration to log your LLM, AI framework, and RAG pipeline calls to Literal AI.
Copy
```
from literalai import AsyncLiteralClient
import os
lai = AsyncLiteralClient(api_key=os.environ["LITERAL_API_KEY"])
lai.initialize()

```

Here is an example of how to log a regular OpenAI chat completion:
Copy
```
from literalai import AsyncLiteralClient
import os
lai = AsyncLiteralClient(api_key=os.environ["LITERAL_API_KEY"])
lai.initialize()
# Run a regular OpenAI chat completion
@lai.thread()
async def run_openai_chat_completion():
  lai.set_properties(
    tags=["hello", "world"],
    metadata={"foo": 1},
  )
  oai = AsyncOpenAI(api_key=os.environ["OPENAI_API_KEY"])
  await oai.chat.completions.create(
    model="gpt-4o",
    messages=[{ "role": "user", "content": "Hello, how are you?" }]
  )
asyncio.run(run_openai_chat_completion())

```

Here are examples: [Example 1](https://github.com/Chainlit/literalai-python/blob/cafe70ddd5838a2cf35193ac922c9b1fd85f79e7/examples/langchain_variable.py#L40) and [Example 2](https://github.com/Chainlit/literalai-python/blob/cafe70ddd5838a2cf35193ac922c9b1fd85f79e7/examples/multimodal.py#L25)

---

# Vercel AI SDK

## Source
https://docs.literalai.com/integrations/vercel-ai-sdk

This integration allows you to very simply add observability and monitoring to your LLM application based on [Vercel’s AI SDK](https://sdk.vercel.ai/docs/introduction). The instrumentation is available for the 4 main methods of the Vercel AI SDK: `streamText`, `generateText`, `streamObject` and `generateObject`.
The Vercel AI SDK integration already support LLM tracing. You should not use it in conjunction with other LLM provider integrations such as [OpenAI](https://docs.literalai.com/integrations/openai).
Copy
```
import { LiteralClient } from '@literalai/client';
import { openai } from '@ai-sdk/openai';
// Here we use the wrapped `streamText` from the Literal AI client. This will allow us to automatically capture
// both the response from the LLM API and the other informations present in the response (latency, token counts, etc...).
const streamText = literalAiClient.instrumentation.vercel.streamText;
export async function POST(req: Request) {
  const { messages } = await req.json();
 
 // The wrapped version of `streamText` accepts the same parameters as the original function.
 const result = streamText({
  model: openai('gpt-4o'),
  messages,
 });
 return result.toDataStreamResponse();
}

```

## 
[​](https://docs.literalai.com/integrations/vercel-ai-sdk#with-threads-and-runs)
With Threads and Runs
In most cases, you will want to keep track of the different generations from your application by grouping them into [Threads or Runs](https://docs.literalai.com/guides/logs). This is especially useful when you want to understand the context in which a generation was made, or when you want to compare different generations.
TypeScript
Copy
```

const generateText = literalAiClient.instrumentation.vercel.generateText;
export async function POST(req: Request) {
const streamText =
 literalAiClient.instrumentation.vercel.streamText;
export async function POST(req: Request) {
 const { messages } = await req.json();
 const result = await literalAiClient
  .thread({ name: "Example" })
  .wrap(async () => {
   return streamText({
    model: openai("gpt-4o"),
    messages,
   });
  });
 return result.toDataStreamResponse();
}
}

```

## 
[​](https://docs.literalai.com/integrations/vercel-ai-sdk#with-metadata%2C-tags-and-step-ids)
With Metadata, Tags and Step IDs
Using our Vercel AI SDK integration, you can pass metadata, tags and a step ID at the generation level. These values will be automatically added to the generation when it is logged on Literal AI.
Copy
```
import { v4 as uuidv4 } from 'uuid';
const literalaiStepId = uuidv4();
const generateText = literalAiClient.instrumentation.vercel.generateText;
const { text } = await generateText({
 model: openai('gpt-4o-mini'),
 prompt: question,
 literalaiStepId,
 literalaiTags: ['tag1', 'tag2'],
 literalaiMetadata: { otherKey: 'otherValue' }
});

```

## 
[​](https://docs.literalai.com/integrations/vercel-ai-sdk#cookbooks)
Cookbooks
You can find more involved examples in our Cookbooks repository :
  * [This chatbot](https://github.com/Chainlit/literalai-cookbooks/blob/main/typescript/nextjs-openai) uses Vercel AI SDK’s `useChat` hook in the frontend
  * [This example](https://github.com/Chainlit/literalai-cookbooks/blob/main/typescript/vercel-ai-sdk) uses the Vercel AI SDK integration in the backend

---

# Export Data

## Source
https://docs.literalai.com/more/export-data

Data in Literal AI can be exported at any time, as the data is always yours. Exporting data can be useful to train or fine-tune LLM models on production threads. You can export the data via the SDKs. There is also an export button on Literal AI, which shows you how to export the data via the SDKs.
Here’s an example of how to export all Threads:
Python
TypeScript
Copy
```
import os
from literalai import LiteralClient
client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"))
has_next_page = True
after = None
filters = [
  {
    "field": "createdAt",
    "operator": "gte",
    "value": "2024-04-30T13:24:38.994Z"
  }
]
order_by = {
  "column": "createdAt",
  "direction": "DESC"
}
threads = []
while has_next_page:
  response = client.api.get_threads(
    filters=filters,
    order_by=order_by,
    after=after
  )
  threads.extend(response.data)
  has_next_page = response.page_info.has_next_page
  after = response.page_info.end_cursor

```

### 
[​](https://docs.literalai.com/more/export-data#exporting-opentelemetry-data)
Exporting OpenTelemetry Data
For paying customers, Literal AI offers the ability to export data in the OpenTelemetry format, providing a standardized way to collect and analyze telemetry data from your LLM applications.

---

# Release Notes

## Source
https://docs.literalai.com/more/release-notes

All changes and improvements to Literal AI are listed here. For changes in the SDKs, go to the [Python SDK](https://docs.literalai.com/python-client/development/changelog) or [TypeScript SDK](https://docs.literalai.com/typescript-client/development/changelog).
Literal AI cloud is currently compatible with:
  * [Chainlit](https://docs.chainlit.io/get-started/overview) version `1.0.504` and above.
  * [Python SDK](https://docs.literalai.com/python-client/development) version `0.0.509` and above.
  * [TypeScript SDK](https://docs.literalai.com/typescript-client/development) version `0.0.503` and above.


# 
[​](https://docs.literalai.com/more/release-notes#0-2-0-beta-march-18th%2C-2025)
0.2.0-beta (March 18th, 2025)
### 
[​](https://docs.literalai.com/more/release-notes#improvements)
Improvements
  * Step ingestion is now more efficient
  * Trace tree view now display the relative duration between steps
  * Added keyboard navigation to the trace tree view
  * Both the trace and waterfall view now “skip” inactive time for better readability


# 
[​](https://docs.literalai.com/more/release-notes#0-1-9-beta-march-09th%2C-2025)
0.1.9-beta (March 09th, 2025)
### 
[​](https://docs.literalai.com/more/release-notes#improvements-2)
Improvements
  * The dashboard has been reorganized to improve readability. Tool call distribution and prompt distribution charts have been added.
  * A LLM powered meta prompt generator has been added in the prompt playground
  * A LLM powered variable value generator has been added in the prompt playground
  * Data ingestion performances have been improved
  * Added support for Grok models
  * Added support for new models across various providers


# 
[​](https://docs.literalai.com/more/release-notes#0-1-8-beta-february-24th%2C-2025)
0.1.8-beta (February 24th, 2025)
### 
[​](https://docs.literalai.com/more/release-notes#improvements-3)
Improvements
  * Waterfall visualization for AI agent run and workflow traces to understand which step took the most time
  * Added support for o3 mini and claude 3.7


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes)
Bug fixes
  * Tool definition do not require `{"type":"function", "function": ...}` anymore


# 
[​](https://docs.literalai.com/more/release-notes#0-1-7-beta-january-26th%2C-2025)
0.1.7-beta (January 26th, 2025)
### 
[​](https://docs.literalai.com/more/release-notes#improvements-4)
Improvements
  * Support for Vertex AI


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-2)
Bug fixes
  * Complex tool calls are now supported in the prompt playground


# 
[​](https://docs.literalai.com/more/release-notes#0-1-6-beta-december-23rd%2C-2024)
0.1.6-beta (December 23rd, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#improvements-5)
Improvements
  * Add support for alt click on tables
  * Show number of logs in settings
  * Allow prompt deletion


# 
[​](https://docs.literalai.com/more/release-notes#0-1-5-beta-december-9th%2C-2024)
0.1.5-beta (December 9th, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#improvements-6)
Improvements
  * Add support for gpt-4o-2024-11-20 and pixtral models
  * Add the participant avatar in the thread detail view
  * Sort by experiment scores put null scores at the bottom
  * Remove the unused topic analysis button


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-3)
Bug Fixes
  * Improve the code editor elements behavior


# 
[​](https://docs.literalai.com/more/release-notes#0-1-4-beta-december-2nd%2C-2024)
0.1.4-beta (December 2nd, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#improvements-7)
Improvements
  * Give access to prompt variables in Scorers via `metadata.variables`
  * Add filter on `Score` value in Logs > Runs table
  * Improve experience when adding a `Step` to a `Dataset`
  * Experiments table allows filtering on params
  * Dashboard: Prevent label overflow in “Online Evals” chart
  * Logs > Scores can be filtered on target log & tags
  * Display choice of `Score` label in Rule logs


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-4)
Bug Fixes
  * In `Thread` details, LLM steps rely on given passed name if: 
    * name is not empty
    * model name doesn’t start with name
  * Prompt Playground: refresh tab messages which would disappear on Prompt creation/save
  * On “Azure OpenAI” credentials, link to example configuration in docs
  * Correct environment filter for “Online Evals” logs
  * Align bar charts radius across application
  * Allow underscores on IDs


# 
[​](https://docs.literalai.com/more/release-notes#0-1-3-beta-november-25th%2C-2024)
0.1.3-beta (November 25th, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features)
New features
  * Add to a dataset directly from within the Prompt Playground


### 
[​](https://docs.literalai.com/more/release-notes#improvements-8)
Improvements
  * Add support of up-to-date models for Mistral AI, Google and Groq
  * Support for images of up to 10mb (was 2mb) in the Prompt Playground
  * Add the field “environment” to rules
  * Fetch more versions when selecting a prompt version
  * Support broader ids shape for thread and steps (was uuid only)
  * Display the score value in the rules’ logs
  * Display tool calls from within the generation table


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-5)
Bug Fixes
  * Fix an issue in the playground where the rollout was not visible on update
  * Fix an issue in the tables where the row height would be fit the content


# 
[​](https://docs.literalai.com/more/release-notes#0-1-2-beta-november-18th%2C-2024)
0.1.2-beta (November 18th, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#improvements-9)
Improvements
  * Added `Toxicity`, `Factuality` and `Sentiment` to out-of-the-box `Scorer`’s
  * Added a “Promote” option when saving a new `Prompt` in the Playground
  * Playground persists local context at tab level
  * `Experiment` details now show Output/Expected Output diff
  * Prompt version selection in Playground now shows each version’s rollout percentage
  * Added table cell overviews on hover for text and JSON-like columns
  * Removed “Open” button on table row navigation
  * On the Playground, version selection now shows each version’s rollout percentage
  * Added OpenAI timestamped models


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-6)
Bug Fixes
  * Fixed tool parsing in Playground
  * Fixed search functionality in compare experiments selection


# 
[​](https://docs.literalai.com/more/release-notes#0-1-1-beta-november-13th%2C-2024)
0.1.1-beta (November 13th, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-7)
Bug fixes
  * Fixed bug `Cannot query field "label" on type "PromptVersion".`
  * Fixed a race condition on the playground prompt messages formatting


# 
[​](https://docs.literalai.com/more/release-notes#0-1-0-beta-november-11th%2C-2024)
0.1.0-beta (November 11th, 2024)
This release includes a complete overhaul on credentials.
Each major LLM provider now offers a single credential. Additional credentials should be created as Custom LLM providers.
See [Credentials](https://docs.literalai.com/guides/settings/llm).
### 
[​](https://docs.literalai.com/more/release-notes#new-features-2)
New features
  * Added ability to run experiments on prompt directly from Playground
  * Added `Scorers` page to manage ways to score on Online Evals and Experiments
  * Added `Online Evals` page to manage automatic monitoring
  * Made score columns sortable on the experiments table for better data analysis
  * Added scores distribution chart and error display in experiments
  * Enabled editing of `Dataset` items
  * On `Datasets` table, we now have links to experiments on each dataset


### 
[​](https://docs.literalai.com/more/release-notes#improvements-10)
Improvements
  * Overhauled credentials management to simplify Experiment/Online Evals setup
  * Keep sidebar state in localStorage and removed playground resize
  * Added close button on hover for error messages
  * Changed display to 8-character UUIDs and added a copy ID button


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-8)
Bug fixes
  * Removed Summary tab and scores displays in tables
  * Defaulted table JSON previews to YAML


# 
[​](https://docs.literalai.com/more/release-notes#0-0-629-beta-october-25th%2C-2024)
0.0.629-beta (October 25th, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-3)
New features
  * Prompt Playground allows for model comparison with multiple tabs
  * “AI Evals” let you manage/configure monitoring rules with custom prompts
  * New “Scoring” tab in Settings to manage `Score` Schemas: categories, descriptions, etc.


### 
[​](https://docs.literalai.com/more/release-notes#improvements-11)
Improvements
  * Experiment Item details view: 
    * Shows scores and input/output diffs
    * Clarifies diffed experiments in comparison mode
  * Prevent deletion of AI Score on `Experiment`
  * `Thread` tree view: allow expand/collapse nested steps on tree nodes
  * Styling: navigation side bar, code editors, spacing, etc.


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-9)
Bug fixes
  * Fix handling of max database connections
  * Gracefully handle streaming errors - e.g. tool calls with Google
  * Invalidate cache on `Prompt` version tags edits
  * Prevent “Human Review” overlap on `Experiment` item details
  * Move “Sentiment classification” `Prompt` example to rely on `json_schema` with `oneOf` descriptions


# 
[​](https://docs.literalai.com/more/release-notes#0-0-628-beta-october-21th%2C-2024)
0.0.628-beta (October 21th, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-4)
New features
  * Add tagging supports on prompts
  * Deprecate unused prompt versions
  * Add experiment item detailed view
  * Access score schemas in settings


### 
[​](https://docs.literalai.com/more/release-notes#improvements-12)
Improvements
  * Existing datasets can import CSV files
  * Improve Redis failure handling
  * Update code theming
  * Support description for score schemas


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-10)
Bug fixes
  * Fix uncaught error in Playground monitoring
  * Facilitate credentials management in Playground
  * Fix credential visibility issue (Amazon Bedrock)


# 
[​](https://docs.literalai.com/more/release-notes#0-0-627-beta-october-14th%2C-2024)
0.0.627-beta (October 14th, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-5)
New features
  * Allow re-run of experiments from Experiment details
  * Added Scores column to Logs > Generations table


### 
[​](https://docs.literalai.com/more/release-notes#improvements-13)
Improvements
  * Show `Score` label instead of value on experiment scores
  * Add manual item creation on empty `Dataset`


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-11)
Bug fixes
  * Fix Thread updates on participant id/identifier
  * Fix Prompt Playground duplication for overlapping default and custom models
  * Improved navigation to Logs > Scores
  * Moved asynchronous step ingestion to main thread
  * Increased default number of steps fetched on a `Thread` to 100


# 
[​](https://docs.literalai.com/more/release-notes#0-0-626-beta-october-7th%2C-2024)
0.0.626-beta (October 7th, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-6)
New features
  * Added new end-to-end cookbook with LangGraph
  * Owners can now transfer project ownership to another Team member from Settings.


### 
[​](https://docs.literalai.com/more/release-notes#improvements-14)
Improvements
  * Settings > General allows for `Tag` management actions: add, filter and delete.
  * `Thread` details display children `Step`’s incrementally — improves page load times for long threads.
  * Improved UX when adding a `Run` / `Generation` to a `Dataset`: layout, JSON/YAML editing and format validation.


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-12)
Bug fixes
  * Fixed data ingestion for Users/Participants and `Last engaged at` information.
  * Fixed the `Thread` table filter on `Step` name: filter speed improved by a 15x factor.


# 
[​](https://docs.literalai.com/more/release-notes#0-0-625-beta-september-30th%2C-2024)
0.0.625-beta (September 30th, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#improvements-15)
Improvements
  * Workers stability increased.
  * Colors for both light and dark theme are more contrasted.
  * Hint text for experiments has been improved.


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-13)
Bug fixes
  * Prevent creation of multiple demo projects.
  * Prevent the UI from breaking when encountering cyclic steps.
  * When using tools in prompt playground avoid skipping on tools.


# 
[​](https://docs.literalai.com/more/release-notes#0-0-624-beta-september-24th%2C-2024)
0.0.624-beta (September 24th, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-7)
New features
  * Added Amazon Bedrock support to Prompt Playground.
  * The Prompt Playground now lets you structure LLM outputs by specifying any JSON schema.
  * Users can now leave projects.


### 
[​](https://docs.literalai.com/more/release-notes#improvements-16)
Improvements
  * API keys are now hidden in Settings.
  * Human/AI reviews display category labels instead of values.
  * LLM Generations created without token usage have a default estimate with tiktoken cl100k_base.
  * The Thread details page has an improved user experience.


Many more UI/UX & performance improvements across the platform !
### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-14)
Bug fixes
  * Dataset imports from CSV now allows JSON objects as column values.
  * Experiment which fail to launch - e.g. wrong API key - now show as failed.


# 
[​](https://docs.literalai.com/more/release-notes#0-0-623-beta-september-16th%2C-2024)
0.0.623-beta (September 16th, 2024)
AI Evaluation rules should be re-configured to account for the switch to custom Evaluator prompts. Check out [Score-based Rules](https://docs.literalai.com/guides/online-evals) to get started !
### 
[​](https://docs.literalai.com/more/release-notes#new-features-8)
New features
  * Introduced “Run Experiment” feature on datasets
  * Enabled custom prompt for LLM-as-a-Judge evaluators
  * Enabled structured output in Playground and in prompts
  * Added Total Cost chart to dashboard (input + output tokens)


### 
[​](https://docs.literalai.com/more/release-notes#improvements-17)
Improvements
  * Dataset table was enhanced with a side-panel Item View
  * Prompt Playground - Added keyboard shortcuts
  * Added ability to navigate from Generation to its root Run
  * Improved Step query efficiency for token count and environment filters
  * Improved queries for retrieving steps and threads
  * Optimized score management for faster edits and fewer queries
  * Enhanced worker performance with multi-threaded asynchronous step ingestion


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-15)
Bug fixes
  * Prompt Playground: 
    * Fixed template messages editing issues
    * Improved scroll behavior for streamed LLM generation
  * Enabled Google as a Provider in Playground
  * Updated prompt cache invalidation strategy


# 
[​](https://docs.literalai.com/more/release-notes#0-0-621-beta-september-2nd%2C-2024)
0.0.621-beta (September 2nd, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-9)
New features
  * We are introducing the concept of “Model Costs”, which will allow you to monitor the actual costs associated with your logged generations. You can now setup the costs for the various LLM models you use in production, including negociated prices.
  * When you exceed your monthly allowance on the free tier, you will now be notified by a pop-up and a persistent message in the sidebar.


### 
[​](https://docs.literalai.com/more/release-notes#improvements-18)
Improvements
  * Add a settings column to the prompt versions table.
  * In the Settings page, you can now see details about a Score Schema by hovering above it.
  * Various performance improvements across the platform.


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-16)
Bug fixes
  * Fixes an ingestion error when a step’s data includes Unicode NULL characters.
  * Fixes a race condition which could slow down step ingestion.


# 
[​](https://docs.literalai.com/more/release-notes#0-0-620-beta-august-27th%2C-2024)
0.0.620-beta (August 27th, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-10)
New features
  * Logs now include Scores so that you can browse Human / AI evaluations of your application
  * The Dashboard includes two new tiles for recently ingested Runs & Scores, and you can jump to that data in one click.
  * You can now add descriptions to individual values in score schemas, to help both with human annotation and AI scoring
  * When setting up prompt A/B testing, you can now search prompt versions by their number


### 
[​](https://docs.literalai.com/more/release-notes#improvements-19)
Improvements
  * API keys are now hidden by default with *** in Settings / LLM. Enjoy screen-sharing with your friends!
  * Improved speed ingestion for Step means faster ways to troubleshoot your application.
  * Identify the origin of Generations at a glance by checking out the Prompt name & version column in the Logs / Generations table. We have also added the much needed Output column!
  * Domain Experts can now browse Settings to better intuit their restricted permissions.
  * Thread details now show a more discoverable Scores section, right underneath Tags.


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-17)
Bug fixes
  * Fixed a bug where you couldn’t select a tag when searching for it in the Filters UI
  * Fixed a bug with query refreshing
  * Solved an issue with the registration of clients and products on Stripe


# 
[​](https://docs.literalai.com/more/release-notes#0-0-617-beta-august-12th%2C-2024)
0.0.617-beta (August 12th, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-11)
New Features
  * Prompt A/B testing (replaces the champion system)


### 
[​](https://docs.literalai.com/more/release-notes#improvements-20)
Improvements
  * The UI now supports any number of tags (previously capped at 100)
  * Annotation queues, Datasets and Logs are linked more naturally traversable directly from the UI.
  * You can now edit a step before adding it to a Dataset from an Annotation Queue


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-18)
Bug fixes
  * Fixed multiple bugs in the Annotation Queues
  * Domain Experts are now able to add items to a dataset
  * Redis connection should no longer hang


# 
[​](https://docs.literalai.com/more/release-notes#0-0-615-beta-july-31th%2C-2024)
0.0.615-beta (July 31th, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-12)
New features
  * Annotation Queues: you can now collaborate as a team and assign steps for review
  * Environments allow you to silo experiment, development, staging and production logs
  * Experiments: 
    * Faster bootstrap to launch experiments without the need to link to a Dataset
    * Experiment items are now sortable by score in a leaderboard fashion for easier comparison
    * You can now troubleshoot your experiment items by visualizing the experiment runs as full traces
  * Generations: you can now enrich your logged LLM calls with metadata


### 
[​](https://docs.literalai.com/more/release-notes#improvements-21)
Improvements
  * Work with your own LLM endpoint by configuring a Custom LLM provider in your settings - based on OpenAI’s messages API
  * Quickly identify your champion version in the Prompt versions table with a star icon
  * Swiftly re-use your Playground prompts in code by using the new Copy button on Template messages
  * If created via a Score Template, a Score gets linked to its template for traceability
  * Improved Generations table readability by displaying Input as the last message in OpenAI’s messages API
  * Unified & enhanced the user experience on score, tag and credentials creation
  * Added latest Anthropic model Claude Sonnet 3.5


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-19)
Bug fixes
  * Project deletion completes successfully when experiments exist in project
  * Rule invocation now works with Azure OpenAI credentials
  * Improved Markdown rendering of Thread Chat view


# 
[​](https://docs.literalai.com/more/release-notes#0-0-613-beta-july-23rd%2C-2024)
0.0.613-beta (July 23rd, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-13)
New features
  * Datasets : you can now create a dataset from a CSV file
  * Onboarding : empty pages on a new project will now include code snippets and instructions to start sending data on Literal AI
  * Navigation : the sidebar has been revamped for flatter navigation between platform modules


### 
[​](https://docs.literalai.com/more/release-notes#improvements-22)
Improvements
  * A new, tighter tree view that better displays Chain of Thought reasoning
  * Add new Run/Generation filters when creating or updating an evaluation rule : model, duration, prompt lineage, prompt version
  * Improve edition of Azure OpenAI credentials
  * Various improvements on platform deployment, both for cloud and self-hosting


### 
[​](https://docs.literalai.com/more/release-notes#bug-fixes-20)
Bug fixes
  * Fix a bug with Azure OpenAI in the prompt playground and other LLM calls
  * The credentials table will now refresh correctly after creting or updating an item
  * Local LLMs are now correctly handled by the prompt playground and other LLM calls
  * Fix a bug on the prompt playground where changing settings could reset the message list


# 
[​](https://docs.literalai.com/more/release-notes#0-0-612-beta-july-10th%2C-2024)
0.0.612-beta (July 10th, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-14)
New features
  * When scoring a step through the platform, we now track the user who created the score
  * We are preparing the platform for the upcoming release of the Annotation Queue
  * A new chart on the dashboard shows the number of runs per day per run name
  * Upon signup, a new account will now contain a default project populated with Threads, Steps, Datasets etc…


### 
[​](https://docs.literalai.com/more/release-notes#improvements-23)
Improvements
  * We are rolling out a new Role system. Possible roles are now as follows : Admin, AI Engineer, Domain expert
  * We have revamped the creation, edition and deletion of Rules for Online Evaluation
  * We have improved screen space management in tables, notably when displaying code previews
  * Some design tweaks on the dashboard and dark mode


### 
[​](https://docs.literalai.com/more/release-notes#fixes)
Fixes
  * Fixed a bug where the generation was not correctly displayed in a run
  * Fixed a bug where some logos would not display correctly in dark mode
  * Fixed a bug where the scores API could break if no generationId was provided


# 
[​](https://docs.literalai.com/more/release-notes#0-0-611-beta-july-1st%2C-2024)
0.0.611-beta (July 1st, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-15)
New features
  * Easily navigate the Runs view with arrow keys
  * You can now filter Runs/Generations by score presence
  * You can now bulk add Generations to Datasets from the UI
  * Dark theme for the diff editor, box plots and toasters
  * Added a new “Run” chart to the dashboard


### 
[​](https://docs.literalai.com/more/release-notes#improvements-24)
Improvements
  * This version embarks the first iteration of our UI revamp
  * Images are now zoomable in the Prompt Playground


# 
[​](https://docs.literalai.com/more/release-notes#0-0-610-beta-june-24%2C-2024)
0.0.610-beta (June 24, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#improvements-25)
Improvements
  * Update the feedback button
  * Extend “Rules” table with filters and pagination
  * Update “is null” and “is not null” filters with a more explicit behavior
  * Improve the score element UI


### 
[​](https://docs.literalai.com/more/release-notes#fixes-2)
Fixes
  * Fixed an issue where annotators could not access content
  * Fixed an issue when double-clicking on a date-picker
  * Fixed an issue related to “Generation” links


# 
[​](https://docs.literalai.com/more/release-notes#0-0-609-beta-june-17%2C-2024)
0.0.609-beta (June 17, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-16)
New features
  * Added a “maintainer” role for the project, that allows for write while preventing the user to manage the project


### 
[​](https://docs.literalai.com/more/release-notes#improvements-26)
Improvements
  * Simplify the Generation and Step data handling
  * Rules can now be updated directly
  * In “Experiment” you can now see the diff between inputs and outputs columns
  * The navigation is improved
  * Score templates can now be accessed in the “Evaluate” section
  * When scoring with a “categorical” score, the score value now uses its name rather than the value itself
  * In the dashboards we no longer display nullish values as 0
  * Rules have now their own detail page


### 
[​](https://docs.literalai.com/more/release-notes#fixes-3)
Fixes
  * Fixed an issue where prompt playground settings were not correctly persisted
  * Fixed an issue where some step rows were duplicated
  * Fixed an issue with the “dataset link”
  * Fixed an issue where it was not possible to select custom models in the playground
  * Fixed an issue with the “Generations” page pagination


# 
[​](https://docs.literalai.com/more/release-notes#0-0-608-beta-june-4%2C-2024)
0.0.608-beta (June 4, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-17)
New features
  * **Compare** feature is now available! This compare feature allows you to compare between generations.
  * We’ll be debuting in the coming week with **self-service distribution** of the Literal AI platform for self-hosting
  * A new **user role** has been added: “Annotator”, a user that can add tags and scores to the observability entities (e.g. thread, step…) and has no access outside of those.
  * Project administrator can now pick the role of a user when inviting.


### 
[​](https://docs.literalai.com/more/release-notes#improvements-27)
Improvements
  * Literal AI API keys are now shared in the project. Before, admins could create “personal” API keys. The behavior of restricting access to admins is kept.
  * We’re continuing our push towards a more consistent - and prettier - User Experience: 
    * We’ve switched to a more vibrant color scheme
    * Made some visual tweaks on the Dashboard page
    * Observability items such as Threads, Runs, and others will now display as full pages rather than side-panes
    * And lots of other improvements across the platform
  * Some changes on the way the platform is deployed, both on our end and for our on-premise users : 
    * Improved and centralized environment management
    * The Portkey AI Gateway is now handled directly inside the Node process
    * The `BUCKET_NAME` environment variable is no longer mandatory. Trying to store objects will log errors but not disrupt the rest of the operations


### 
[​](https://docs.literalai.com/more/release-notes#fixes-4)
Fixes
  * This week’s release sees a big focus on performance, especially on the `Users` and individual `Thread` pages
  * We’ve also chased and squashed a few bugs related to : 
    * Signing Attachment URLs (for object storage like S3)
    * Conflicts on unique `userId`
    * A visual bug on initialization of “continuous” score templates
  * Audio attachments are now resolved.
  * Links on prompt versions are now directed to the correct prompt.
  * Show the correct projects when accessing the prompt playground.


# 
[​](https://docs.literalai.com/more/release-notes#0-0-607-beta-may-27%2C-2024)
0.0.607-beta (May 27, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-18)
New features
  * Added online evaluations to score LLM generations on the fly.
  * Created the endpoint /api/my-project to quickly access a project ID with an API key
  * Brushed up the Dashboard page with: 
    * Browser-level customizable layouts of charts
    * Filters on each chart to select relevant data - also saved at the browser level
  * Specific to token usage, we offer multiple visualizations


### 
[​](https://docs.literalai.com/more/release-notes#improvements-28)
Improvements
  * Improved the look of Step badges, specifically colors.
  * Sharing threads requires additional privilege
  * Improved UX on text, audio, image and video attachments in Step details
  * Prompt versions show a visual “Open” button to jump to the Prompt Playground
  * Revamped the UI look of the side navigation
  * Stop sequences on Prompt Playground now show visual cues
  * Removed UUID columns across tables to improve readability
  * JSON & Text previews come with full screen & copy/paste options


### 
[​](https://docs.literalai.com/more/release-notes#fixes-5)
Fixes
  * Newly created API keys do not contain special characters


# 
[​](https://docs.literalai.com/more/release-notes#0-0-606-beta-may-20%2C-2024)
0.0.606-beta (May 20, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#breaking-changes)
Breaking Changes
  * Dataset: Renamed intermediary steps `expectedOutput` to `output`. In a Dataset, in the field `Indermediary Steps`, the field `expectedOutput` is renamed to just `output`, because this is the actual output of the LLM. This breaks backward compatibility for users relying on `DatasetItem.intermediarySteps.expectedOutput`, `DatasetItem.expectedOutput` remains unchanged.


### 
[​](https://docs.literalai.com/more/release-notes#new-features-19)
New Features
  * Attachments now come with preview widgets (multi-modality)


### 
[​](https://docs.literalai.com/more/release-notes#improvements-29)
Improvements
  * A page change on tables now scrolls back to top of table
  * We removed name fallback to ID for threads, now shown as N/A
  * We reduced the indent of navigation sub-menus
  * The prompt playground now persists the credentials for your session
  * Improved user feedback options from the UI
  * JSONs in tables now display on multiple lines with syntax highlighting
  * Improved dashboard performance with data fetch in separate requests


### 
[​](https://docs.literalai.com/more/release-notes#fixes-6)
Fixes
  * Fixed creation of attachments and scores when step doesn’t exist
  * Fixed thread duplication when filtering on errors
  * Fixed the upserts of step input/output to prevent going past size limit


# 
[​](https://docs.literalai.com/more/release-notes#0-0-605-beta-may-13%2C-2024)
0.0.605-beta (May 13, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-20)
New Features
  * Support GPT-4o as LLM model provider
  * We now display a diff of the prompt settings when saving a prompt version
  * Steps are now supporting tags


### 
[​](https://docs.literalai.com/more/release-notes#improvements-30)
Improvements
  * We now populate the dataset item `intermediarySteps` when adding a step with children steps
  * The API credentials in the prompt playground are moved
  * Generation details view now has a link to prompt
  * Support higher than 1 temperature setting for compatible LLMs


### 
[​](https://docs.literalai.com/more/release-notes#fixes-7)
Fixes
  * Fix display bugs in prompt playground
  * Fix a bug where we allow for very large json inputs 
    * Now metadataare limited to 1mb
    * Step input and expectedOutput limited to 3mb
  * Fix a bug where full-text searching threads would lead to a spike in cpu usage
  * Fix a rare bug that could occur when ingesting multiple steps with a new tag


# 
[​](https://docs.literalai.com/more/release-notes#0-0-604-beta-may-6%2C-2024)
0.0.604-beta (May 6, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-21)
New Features
  * **UI/UX** : New button on the headers on the Literal AI platform of a page that links to the documentation. This improves the UX of the Literal AI platform.
  * **Release status page**. Literal AI now has a release status page: <https://literalai.betteruptime.com/>. Here, you can see the uptime of the services.
  * **Experiments/Score** : You are now able create Scores directly in Experiments.
  * **Threads** : There is now a search bar in the Threads table.


### 
[​](https://docs.literalai.com/more/release-notes#improvements-31)
Improvements
  * Minor UI updates to: 
    * Sidebar navigation
    * Scores table
    * Table filters
  * Tags: Pressing enter will now create a Tag
  * Warn on dataset deletion
  * Persist playground settings
  * Warn when creating a prompt


### 
[​](https://docs.literalai.com/more/release-notes#fixes-8)
Fixes
  * Fix dashboard evolution badge tooltip period being wrong


# 
[​](https://docs.literalai.com/more/release-notes#0-0-603-beta-april-29%2C-2024)
0.0.603-beta (April 29, 2024)
### 
[​](https://docs.literalai.com/more/release-notes#new-features-22)
New Features
  * **Credentials** : You are now able to share your llm credentials to better collaborate through the prompt playground.


### 
[​](https://docs.literalai.com/more/release-notes#improvements-32)
Improvements
  * **Dashboard** : New comparison badge in dashboard to display the data evolution. ![Dashboard evolution badge](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/changelog-evolution-04-29-2024.png)
  * **UI of Thread and Dataset** : On the top of a `Thread` or `Dataset` page there, the location is now shown as breadcrumb. This prevents getting lost in sheets and improves navigation.
  * **Settings UI** : Split `Settings` menu in the UI into sub-menus for `General`, `LLM` and `Team`.
  * **Prompts** : New `Created by` column on Prompt Version table, which improves the table display.


### 
[​](https://docs.literalai.com/more/release-notes#fixes-9)
Fixes
  * **Prompt Playground** : Fix model select overflow (This is minimal change in the prompt playground. The long model name in the select is now ellipsed when size is reduced)
  * **Experiments** : In `comparison` mode, parameters are made more explicit. In addition, the charts inversed, which is now fixed.
  * **Filters** : `is null` and `not in` filters on tags edge cases are added. This fixes the tags filters in the table, as they were not working as intended before.
  * **Tags** : Newly created `Tags` are now visible in the UI when a `Thread`, `Step` or `Generation` page is refreshed. Tags are now refetched on page refresh.
  * **Tags** : `Tags` can now be added on generations being created.

---

# Roadmap

## Source
https://docs.literalai.com/more/roadmap

We are continuously developing to Literal AI. Our focus is on rolling out new features as well as improving the Developer Experience. Check out our [release notes](https://docs.literalai.com/more/release-notes) to see what has been released recently.
We release a new [guide](https://docs.literalai.com/guides) or [cookbook](https://github.com/Chainlit/literalai-cookbooks) for every new feature in Literal AI.
Upcoming new features:
### 
[​](https://docs.literalai.com/more/roadmap#roles-and-orgs)
Roles and Orgs
Literal AI currently supports [`Roles`](https://docs.literalai.com/guides/settings/team) and [`Projects`](https://docs.literalai.com/guides/settings/general) for collaboration. We plan to enhance project management with:
  * Customizable roles and permissions for diverse organizational needs
  * Improved security through granular access controls


### 
[​](https://docs.literalai.com/more/roadmap#evaluation)
Evaluation
Literal AI currently supports evaluation from your code and LLM-as-a-Judge on Literal AI. We will soon support custom evaluations, directly in Literal AI. This feature will allow you to register code on Literal AI and use it to monitor your LLM system in production.
### 
[​](https://docs.literalai.com/more/roadmap#playground-and-prompts)
Playground and Prompts
We are continuously improving our Playground and Prompt management features:
  * Compare view: We’re adding a new feature to compare different models side-by-side in the Playground. This will allow developers to easily evaluate and contrast the outputs of various models for the same input.
  * Enhanced versioning: We’re expanding our versioning capabilities to include not just prompt templates and tool definitions, but also the tool code itself. This will provide more comprehensive version control and traceability for your LLM applications. These enhancements will enable more robust testing, easier debugging, and better management of your LLM workflows.


### 
[​](https://docs.literalai.com/more/roadmap#dashboard)
Dashboard
The Dashboard in Literal AI will be enhanced for better data visualization and analysis. Key improvements include:
  * Interactive data exploration
  * Minimal changes to existing codebase and database structure


### 
[​](https://docs.literalai.com/more/roadmap#streams)
Streams
This feature is about supporting continuous data streams in your LLM applications. For example, live voice conversation and video streams.
[Contact us](https://literalai.com/contact-us) if you are interested in the functionality.

---

# Privacy & Security

## Source
https://docs.literalai.com/more/security

The Literal AI platform has been built with security in mind, and we continuously work to ensure that our platform is secure and compliant with industry standards. We are committed to providing a secure environment for your data and ensuring that your privacy is protected.
## [Trust CenterClick and check out our Trust center. Contact us for any questions at contact@literalai.com.](https://app.bastion.tech/trust-center/de645942-3d51-4acc-9e62-71b8c7ad5b56)
### 
[​](https://docs.literalai.com/more/security#literal-ai-cloud)
Literal AI Cloud
Literal AI Cloud is hosted on Google Cloud in the europe-west1 region (belgium).
We ensure that our infrastructure is secure and compliant with industry standards:
  * All data is encrypted both at rest and in transit using TLS.
  * The database is backed up daily and stored in a secure location.
  * Security scans are performed regularly to identify vulnerabilities.


You can find our Privacy Policy [here](https://app.termly.io/policy-viewer/policy.html?policyUUID=117a0b91-e2ae-48cd-a66b-fe5a9a7d82e3).
#### 
[​](https://docs.literalai.com/more/security#advantages-of-literal-ai-cloud)
Advantages of Literal AI Cloud
  * **Ease of Use** : Get started quickly without the need to set up your own infrastructure.
  * **Scaling** : You don’t have to worry about scaling your infrastructure as your usage grows.
  * **Security** : We take care of security so you can focus on building your applications.
  * **High Availability** :


### 
[​](https://docs.literalai.com/more/security#self-hosting)
Self Hosting
Literal AI is fully self hostable. Refer to the [Self Hosting Guide](https://docs.literalai.com/self-hosting/get-started) for more information.

---

# Api

## Source
https://docs.literalai.com/python-client/api-reference/api

## 
[​](https://docs.literalai.com/python-client/api-reference/api#literalapi)
LiteralAPI
Copy
```
class LiteralAPI(BaseLiteralAPI)

```

Copy
```
from literalai import LiteralClient
# Initialize the client
literalai_client = LiteralClient(api_key="your_api_key_here")
# Access the API's methods
print(literalai_client.api)

```

#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-users)
get_users
Copy
```
def get_users(first: Optional[int] = None,
       after: Optional[str] = None,
       before: Optional[str] = None,
       filters: Optional[users_filters] = None)

```

Retrieves a list of users based on pagination and optional filters.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-first)
first
Optional[int]
The number of users to retrieve.
[​](https://docs.literalai.com/python-client/api-reference/api#param-after)
after
Optional[str]
A cursor for use in pagination, fetching records after this cursor.
[​](https://docs.literalai.com/python-client/api-reference/api#param-before)
before
Optional[str]
A cursor for use in pagination, fetching records before this cursor.
  * `filters` _Optional[users_filters]_ - Filters to apply to the user query.


**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dict)
Dict
A dictionary containing the queried user data.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-user)
get_user
Copy
```
def get_user(id: Optional[str] = None, identifier: Optional[str] = None)

```

Retrieves a user based on the provided ID or identifier.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id)
id
Optional[str]
The unique ID of the user.
[​](https://docs.literalai.com/python-client/api-reference/api#param-identifier)
identifier
Optional[str]
A unique identifier for the user, such as a username or email.
**Returns** :
The user data as returned by the GraphQL helper function.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-user)
create_user
Copy
```
def create_user(identifier: str, metadata: Optional[Dict] = None)

```

Creates a new user with the specified identifier and optional metadata.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-identifier-1)
identifier
str
A unique identifier for the user, such as a username or email.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata)
metadata
Optional[Dict]
Additional data associated with the user.
**Returns** :
The result of the GraphQL call to create a user.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#update-user)
update_user
Copy
```
def update_user(id: str,
        identifier: Optional[str] = None,
        metadata: Optional[Dict] = None)

```

Updates an existing user identified by the given ID, with optional new identifier and metadata.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-1)
id
str
The unique ID of the user to update.
[​](https://docs.literalai.com/python-client/api-reference/api#param-identifier-2)
identifier
Optional[str]
A new identifier for the user, such as a username or email.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-1)
metadata
Optional[Dict]
New or updated metadata for the user.
**Returns** :
The result of the GraphQL call to update the user.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#delete-user)
delete_user
Copy
```
def delete_user(id: str)

```

Deletes a user identified by the given ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-2)
id
str
The unique ID of the user to delete.
**Returns** :
The result of the GraphQL call to delete the user.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-or-create-user)
get_or_create_user
Copy
```
def get_or_create_user(identifier: str, metadata: Optional[Dict] = None)

```

Retrieves a user by their identifier, or creates a new user if they do not exist.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-identifier-3)
identifier
str
The identifier of the user to retrieve or create.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-2)
metadata
Optional[Dict]
Metadata to associate with the user if they are created.
**Returns** :
The existing or newly created user data.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-threads)
get_threads
Copy
```
def get_threads(first: Optional[int] = None,
        after: Optional[str] = None,
        before: Optional[str] = None,
        filters: Optional[threads_filters] = None,
        order_by: Optional[threads_order_by] = None,
        step_types_to_keep: Optional[List[StepType]] = None)

```

Fetches a list of threads based on pagination and optional filters.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-first-1)
first
Optional[int]
Number of threads to fetch.
[​](https://docs.literalai.com/python-client/api-reference/api#param-after-1)
after
Optional[str]
Cursor for pagination, fetch threads after this cursor.
[​](https://docs.literalai.com/python-client/api-reference/api#param-before-1)
before
Optional[str]
Cursor for pagination, fetch threads before this cursor.
  * `filters` _Optional[threads_filters]_ - Filters to apply on the threads query.
  * `order_by` _Optional[threads_order_by]_ - Order by clause for threads. step_types_to_keep (Optional[List[StepType]]) : If set, only steps of the corresponding types will be returned


**Returns** :
A list of threads that match the criteria.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#list-threads)
list_threads
Copy
```
def list_threads(first: Optional[int] = None,
         after: Optional[str] = None,
         before: Optional[str] = None,
         filters: Optional[threads_filters] = None,
         order_by: Optional[threads_order_by] = None)

```

Lists threads based on pagination and optional filters, similar to get_threads but may include additional processing.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-first-2)
first
Optional[int]
Number of threads to list.
[​](https://docs.literalai.com/python-client/api-reference/api#param-after-2)
after
Optional[str]
Cursor for pagination, list threads after this cursor.
[​](https://docs.literalai.com/python-client/api-reference/api#param-before-2)
before
Optional[str]
Cursor for pagination, list threads before this cursor.
  * `filters` _Optional[threads_filters]_ - Filters to apply on the threads listing.
  * `order_by` _Optional[threads_order_by]_ - Order by clause for threads.


**Returns** :
A list of threads that match the criteria.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-thread)
get_thread
Copy
```
def get_thread(id: str)

```

Retrieves a single thread by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-3)
id
str
The unique identifier of the thread.
**Returns** :
The thread corresponding to the provided ID.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-thread)
create_thread
Copy
```
def create_thread(name: Optional[str] = None,
         metadata: Optional[Dict] = None,
         participant_id: Optional[str] = None,
         tags: Optional[List[str]] = None)

```

Creates a new thread with the specified details.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-name)
name
Optional[str]
Name of the thread.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-3)
metadata
Optional[Dict]
Metadata associated with the thread.
[​](https://docs.literalai.com/python-client/api-reference/api#param-participant-id)
participant_id
Optional[str]
Identifier for the participant.
[​](https://docs.literalai.com/python-client/api-reference/api#param-tags)
tags
Optional[List[str]]
List of tags associated with the thread.
**Returns** :
The newly created thread.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#upsert-thread)
upsert_thread
Copy
```
def upsert_thread(id: str,
         name: Optional[str] = None,
         metadata: Optional[Dict] = None,
         participant_id: Optional[str] = None,
         tags: Optional[List[str]] = None)

```

Updates an existing thread or creates a new one if it does not exist.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-4)
id
str
The unique identifier of the thread.
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-1)
name
Optional[str]
Name of the thread.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-4)
metadata
Optional[Dict]
Metadata associated with the thread.
[​](https://docs.literalai.com/python-client/api-reference/api#param-participant-id-1)
participant_id
Optional[str]
Identifier for the participant.
[​](https://docs.literalai.com/python-client/api-reference/api#param-tags-1)
tags
Optional[List[str]]
List of tags associated with the thread.
**Returns** :
The updated or newly created thread.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#update-thread)
update_thread
Copy
```
def update_thread(id: str,
         name: Optional[str] = None,
         metadata: Optional[Dict] = None,
         participant_id: Optional[str] = None,
         tags: Optional[List[str]] = None)

```

Updates the specified details of an existing thread.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-5)
id
str
The unique identifier of the thread to update.
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-2)
name
Optional[str]
New name of the thread.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-5)
metadata
Optional[Dict]
New metadata for the thread.
[​](https://docs.literalai.com/python-client/api-reference/api#param-participant-id-2)
participant_id
Optional[str]
New identifier for the participant.
[​](https://docs.literalai.com/python-client/api-reference/api#param-tags-2)
tags
Optional[List[str]]
New list of tags for the thread.
**Returns** :
The updated thread.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#delete-thread)
delete_thread
Copy
```
def delete_thread(id: str)

```

Deletes a thread identified by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-6)
id
str
The unique identifier of the thread to delete.
**Returns** :
The result of the deletion operation.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-score)
create_score
Copy
```
def create_score(name: str,
         value: float,
         type: ScoreType,
         step_id: Optional[str] = None,
         generation_id: Optional[str] = None,
         dataset_experiment_item_id: Optional[str] = None,
         comment: Optional[str] = None,
         tags: Optional[List[str]] = None)

```

Creates a single score in the database.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-3)
name
str
The name of the score.
[​](https://docs.literalai.com/python-client/api-reference/api#param-value)
value
float
The numerical value of the score.
[​](https://docs.literalai.com/python-client/api-reference/api#param-type)
type
ScoreType
The type of the score.
[​](https://docs.literalai.com/python-client/api-reference/api#param-step-id)
step_id
Optional[str]
The ID of the step associated with the score.
[​](https://docs.literalai.com/python-client/api-reference/api#param-generation-id)
generation_id
Optional[str]
The ID of the generation associated with the score.
[​](https://docs.literalai.com/python-client/api-reference/api#param-dataset-experiment-item-id)
dataset_experiment_item_id
Optional[str]
The ID of the dataset experiment item associated with the score.
[​](https://docs.literalai.com/python-client/api-reference/api#param-comment)
comment
Optional[str]
An optional comment about the score.
[​](https://docs.literalai.com/python-client/api-reference/api#param-tags-3)
tags
Optional[List[str]]
Optional tags associated with the score.
**Returns** :
The created Score object.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#update-score)
update_score
Copy
```
def update_score(id: str, update_params: ScoreUpdate)

```

Updates a score identified by its ID with new parameters.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-7)
id
str
The unique identifier of the score to update.
[​](https://docs.literalai.com/python-client/api-reference/api#param-update-params)
update_params
ScoreUpdate
A dictionary of parameters to update in the score.
**Returns** :
The result of the update operation.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#delete-score)
delete_score
Copy
```
def delete_score(id: str)

```

Deletes a score identified by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-8)
id
str
The unique identifier of the score to delete.
**Returns** :
The result of the deletion operation.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#upload-file)
upload_file
Copy
```
def upload_file(content: Union[bytes, str],
        thread_id: Optional[str] = None,
        mime: Optional[str] = "application/octet-stream") -> Dict

```

Uploads a file to the server.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-content)
content
Union[bytes, str]
The content of the file to upload.
[​](https://docs.literalai.com/python-client/api-reference/api#param-thread-id)
thread_id
Optional[str]
The ID of the thread associated with the file.
[​](https://docs.literalai.com/python-client/api-reference/api#param-mime)
mime
Optional[str]
The MIME type of the file. Defaults to ‘application/octet-stream’.
**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dict-1)
Dict
A dictionary containing the object key and URL of the uploaded file, or None values if the upload fails.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-attachment)
create_attachment
Copy
```
def create_attachment(thread_id: Optional[str] = None,
           step_id: Optional[str] = None,
           id: Optional[str] = None,
           metadata: Optional[Dict] = None,
           mime: Optional[str] = None,
           name: Optional[str] = None,
           object_key: Optional[str] = None,
           url: Optional[str] = None,
           content: Optional[Union[bytes, str]] = None,
           path: Optional[str] = None) -> "Attachment"

```

Creates an attachment associated with a thread and step, potentially uploading file content.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-thread-id-1)
thread_id
str
The ID of the thread to which the attachment is linked.
[​](https://docs.literalai.com/python-client/api-reference/api#param-step-id-1)
step_id
str
The ID of the step to which the attachment is linked.
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-9)
id
Optional[str]
The ID of the attachment, if updating an existing one.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-6)
metadata
Optional[Dict]
Metadata associated with the attachment.
[​](https://docs.literalai.com/python-client/api-reference/api#param-mime-1)
mime
Optional[str]
MIME type of the file, if content is provided.
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-4)
name
Optional[str]
Name of the attachment.
[​](https://docs.literalai.com/python-client/api-reference/api#param-object-key)
object_key
Optional[str]
Object key of the uploaded file, if already known.
[​](https://docs.literalai.com/python-client/api-reference/api#param-url)
url
Optional[str]
URL of the uploaded file, if already known.
[​](https://docs.literalai.com/python-client/api-reference/api#param-content-1)
content
Optional[Union[bytes, str]]
File content to upload.
[​](https://docs.literalai.com/python-client/api-reference/api#param-path)
path
Optional[str]
Path where the file should be stored.
**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-literalai-observability-step-attachment)
literalai.observability.step.Attachment
The created or updated attachment object.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#update-attachment)
update_attachment
Copy
```
def update_attachment(id: str, update_params: AttachmentUpload)

```

Updates an existing attachment with new parameters.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-10)
id
str
The unique identifier of the attachment to update.
[​](https://docs.literalai.com/python-client/api-reference/api#param-update-params-1)
update_params
AttachmentUpload
The parameters to update in the attachment.
**Returns** :
The result of the update operation.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-attachment)
get_attachment
Copy
```
def get_attachment(id: str)

```

Retrieves an attachment by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-11)
id
str
The unique identifier of the attachment to retrieve.
**Returns** :
The attachment data as returned by the GraphQL helper function.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#delete-attachment)
delete_attachment
Copy
```
def delete_attachment(id: str)

```

Deletes an attachment identified by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-12)
id
str
The unique identifier of the attachment to delete.
**Returns** :
The result of the deletion operation.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-step)
create_step
Copy
```
def create_step(thread_id: Optional[str] = None,
        type: Optional[StepType] = "undefined",
        start_time: Optional[str] = None,
        end_time: Optional[str] = None,
        input: Optional[Dict] = None,
        output: Optional[Dict] = None,
        metadata: Optional[Dict] = None,
        parent_id: Optional[str] = None,
        name: Optional[str] = None,
        tags: Optional[List[str]] = None,
        root_run_id: Optional[str] = None)

```

Creates a new step with the specified parameters.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-thread-id-2)
thread_id
Optional[str]
The ID of the thread this step is associated with.
[​](https://docs.literalai.com/python-client/api-reference/api#param-type-1)
type
Optional[StepType]
The type of the step, defaults to “undefined”.
[​](https://docs.literalai.com/python-client/api-reference/api#param-start-time)
start_time
Optional[str]
The start time of the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-end-time)
end_time
Optional[str]
The end time of the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-input)
input
Optional[Dict]
Input data for the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-output)
output
Optional[Dict]
Output data from the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-7)
metadata
Optional[Dict]
Metadata associated with the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-parent-id)
parent_id
Optional[str]
The ID of the parent step, if any.
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-5)
name
Optional[str]
The name of the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-tags-4)
tags
Optional[List[str]]
Tags associated with the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-root-run-id)
root_run_id
Optional[str]
The ID of the root run, if any.
**Returns** :
The result of the GraphQL helper function for creating a step.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#update-step)
update_step
Copy
```
def update_step(id: str,
        type: Optional[StepType] = None,
        input: Optional[str] = None,
        output: Optional[str] = None,
        metadata: Optional[Dict] = None,
        name: Optional[str] = None,
        tags: Optional[List[str]] = None,
        start_time: Optional[str] = None,
        end_time: Optional[str] = None,
        parent_id: Optional[str] = None)

```

Updates an existing step identified by its ID with new parameters.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-13)
id
str
The unique identifier of the step to update.
[​](https://docs.literalai.com/python-client/api-reference/api#param-type-2)
type
Optional[StepType]
The type of the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-input-1)
input
Optional[str]
Input data for the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-output-1)
output
Optional[str]
Output data from the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-8)
metadata
Optional[Dict]
Metadata associated with the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-6)
name
Optional[str]
The name of the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-tags-5)
tags
Optional[List[str]]
Tags associated with the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-start-time-1)
start_time
Optional[str]
The start time of the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-end-time-1)
end_time
Optional[str]
The end time of the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-parent-id-1)
parent_id
Optional[str]
The ID of the parent step, if any.
**Returns** :
The result of the GraphQL helper function for updating a step.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-steps)
get_steps
Copy
```
def get_steps(
    first: Optional[int] = None,
    after: Optional[str] = None,
    before: Optional[str] = None,
    filters: Optional[steps_filters] = None,
    order_by: Optional[steps_order_by] = None) -> PaginatedResponse[Step]

```

Fetches a list of steps based on pagination and optional filters.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-first-3)
first
Optional[int]
Number of steps to fetch.
[​](https://docs.literalai.com/python-client/api-reference/api#param-after-3)
after
Optional[str]
Cursor for pagination, fetch steps after this cursor.
[​](https://docs.literalai.com/python-client/api-reference/api#param-before-3)
before
Optional[str]
Cursor for pagination, fetch steps before this cursor.
  * `filters` _Optional[steps_filters]_ - Filters to apply on the steps query.
  * `order_by` _Optional[steps_order_by]_ - Order by clause for steps.


**Returns** :
A list of steps that match the criteria.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-step)
get_step
Copy
```
def get_step(id: str)

```

Retrieves a step by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-14)
id
str
The unique identifier of the step to retrieve.
**Returns** :
The step data as returned by the GraphQL helper function.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#delete-step)
delete_step
Copy
```
def delete_step(id: str)

```

Deletes a step identified by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-15)
id
str
The unique identifier of the step to delete.
**Returns** :
The result of the deletion operation.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#send-steps)
send_steps
Copy
```
def send_steps(steps: List[Union[StepDict, "Step"]])

```

Sends a list of steps to be processed.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-steps)
steps
List[Union[StepDict, Step]]
A list of steps or step dictionaries to send.
**Returns** :
The result of the GraphQL helper function for sending steps.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-generations)
get_generations
Copy
```
def get_generations(first: Optional[int] = None,
          after: Optional[str] = None,
          before: Optional[str] = None,
          filters: Optional[generations_filters] = None,
          order_by: Optional[generations_order_by] = None)

```

Fetches a list of generations based on pagination and optional filters.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-first-4)
first
Optional[int]
The number of generations to retrieve.
[​](https://docs.literalai.com/python-client/api-reference/api#param-after-4)
after
Optional[str]
A cursor for use in pagination, fetching records after this cursor.
[​](https://docs.literalai.com/python-client/api-reference/api#param-before-4)
before
Optional[str]
A cursor for use in pagination, fetching records before this cursor.
  * `filters` _Optional[generations_filters]_ - Filters to apply to the generations query.
  * `order_by` _Optional[generations_order_by]_ - Order by clause for generations.


**Returns** :
A list of generations that match the criteria.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-generation)
create_generation
Copy
```
def create_generation(generation: Union[ChatGeneration, CompletionGeneration])

```

Creates a new generation, either a chat or completion type.
Copy
```
from literalai.observability.generation import ChatGeneration
from literalai import LiteralClient
literalai_client = LiteralClient(api_key=)
example_generation = ChatGeneration(
  messages=[
    {
      "role": "user",
      "content": "Hello, how can I help you today?"
    },
  ],
  message_completion={
    "role": "assistant",
    "content": "Sure, I can help with that. What do you need to know?"
  },
  model="gpt-4o-mini",
  provider="OpenAI"
)
literalai_client.api.create_generation(example_generation)

```

**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-generation)
generation
Union[ChatGeneration, CompletionGeneration]
The generation data to create.
**Returns** :
The result of the creation operation.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-dataset)
create_dataset
Copy
```
def create_dataset(name: str,
          description: Optional[str] = None,
          metadata: Optional[Dict] = None,
          type: DatasetType = "key_value")

```

Creates a new dataset with the specified properties.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-7)
name
str
The name of the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-description)
description
Optional[str]
A description of the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-9)
metadata
Optional[Dict]
Additional metadata for the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-type-3)
type
DatasetType
The type of the dataset, defaults to “key_value”.
**Returns** :
The result of the dataset creation operation.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-dataset)
get_dataset
Copy
```
def get_dataset(id: Optional[str] = None,
        name: Optional[str] = None) -> Optional[Dataset]

```

Retrieves a dataset by its ID or name.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-16)
id
Optional[str]
The unique identifier of the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-8)
name
Optional[str]
The name of the dataset.
**Returns** :
The dataset data as returned by the REST helper function.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#update-dataset)
update_dataset
Copy
```
def update_dataset(id: str,
          name: Optional[str] = None,
          description: Optional[str] = None,
          metadata: Optional[Dict] = None)

```

Updates an existing dataset identified by its ID with new properties.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-17)
id
str
The unique identifier of the dataset to update.
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-9)
name
Optional[str]
A new name for the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-description-1)
description
Optional[str]
A new description for the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-10)
metadata
Optional[Dict]
New or updated metadata for the dataset.
**Returns** :
The result of the dataset update operation.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#delete-dataset)
delete_dataset
Copy
```
def delete_dataset(id: str)

```

Deletes a dataset identified by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-18)
id
str
The unique identifier of the dataset to delete.
**Returns** :
The result of the deletion operation.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-experiment)
create_experiment
Copy
```
def create_experiment(name: str,
           dataset_id: Optional[str] = None,
           prompt_id: Optional[str] = None,
           params: Optional[Dict] = None) -> "DatasetExperiment"

```

Creates a new experiment associated with a specific dataset.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-10)
name
str
The name of the experiment.
[​](https://docs.literalai.com/python-client/api-reference/api#param-dataset-id)
dataset_id
Optional[str]
The unique identifier of the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-prompt-id)
prompt_id
Optional[str]
The identifier of the prompt associated with the experiment.
[​](https://docs.literalai.com/python-client/api-reference/api#param-params)
params
Optional[Dict]
Additional parameters for the experiment.
**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dataset-experiment)
DatasetExperiment
The newly created experiment object.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-experiment-item)
create_experiment_item
Copy
```
def create_experiment_item(
    experiment_item: DatasetExperimentItem) -> DatasetExperimentItem

```

Creates an experiment item within an existing experiment.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-experiment-item)
experiment_item
DatasetExperimentItem
The experiment item to be created, containing all necessary data.
**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dataset-experiment-item)
DatasetExperimentItem
The newly created experiment item with scores attached.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-dataset-item)
create_dataset_item
Copy
```
def create_dataset_item(dataset_id: str,
            input: Dict,
            expected_output: Optional[Dict] = None,
            metadata: Optional[Dict] = None)

```

Creates a new dataset item with the specified properties.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dataset-id-1)
dataset_id
str
The unique identifier of the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-input-2)
input
Dict
The input data for the dataset item.
[​](https://docs.literalai.com/python-client/api-reference/api#param-expected-output)
expected_output
Optional[Dict]
The expected output data for the dataset item.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-11)
metadata
Optional[Dict]
Additional metadata for the dataset item.
**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dict-2)
Dict
The result of the dataset item creation operation.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-dataset-item)
get_dataset_item
Copy
```
def get_dataset_item(id: str)

```

Retrieves a dataset item by its unique identifier.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-19)
id
str
The unique identifier of the dataset item to retrieve.
**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dict-3)
Dict
The dataset item data.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#delete-dataset-item)
delete_dataset_item
Copy
```
def delete_dataset_item(id: str)

```

Deletes a dataset item by its unique identifier.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-20)
id
str
The unique identifier of the dataset item to delete.
**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dict-4)
Dict
The result of the dataset item deletion operation.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#add-step-to-dataset)
add_step_to_dataset
Copy
```
def add_step_to_dataset(dataset_id: str,
            step_id: str,
            metadata: Optional[Dict] = None)

```

Adds a step to a dataset.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dataset-id-2)
dataset_id
str
The unique identifier of the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-step-id-2)
step_id
str
The unique identifier of the step to add.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-12)
metadata
Optional[Dict]
Additional metadata for the step being added.
**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dict-5)
Dict
The result of adding the step to the dataset.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#add-generation-to-dataset)
add_generation_to_dataset
Copy
```
def add_generation_to_dataset(dataset_id: str,
               generation_id: str,
               metadata: Optional[Dict] = None)

```

Adds a generation to a dataset.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dataset-id-3)
dataset_id
str
The unique identifier of the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-generation-id-1)
generation_id
str
The unique identifier of the generation to add.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-13)
metadata
Optional[Dict]
Additional metadata for the generation being added.
**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dict-6)
Dict
The result of adding the generation to the dataset.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-or-create-prompt-lineage)
get_or_create_prompt_lineage
Copy
```
def get_or_create_prompt_lineage(name: str, description: Optional[str] = None)

```

Creates a prompt lineage with the specified name and optional description. If the prompt lineage with that name already exists, it is returned.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-11)
name
str
The name of the prompt lineage.
[​](https://docs.literalai.com/python-client/api-reference/api#param-description-2)
description
Optional[str]
An optional description of the prompt lineage.
**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dict-7)
Dict
The result of the prompt lineage creation operation.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-or-create-prompt)
get_or_create_prompt
Copy
```
def get_or_create_prompt(name: str,
             template_messages: List[GenerationMessage],
             settings: Optional[ProviderSettings] = None,
             tools: Optional[List[Dict]] = None) -> Prompt

```

A `Prompt` is fully defined by its `name`, `template_messages`, `settings` and tools. If a prompt already exists for the given arguments, it is returned. Otherwise, a new prompt is created.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-12)
name
str
The name of the prompt to retrieve or create.
[​](https://docs.literalai.com/python-client/api-reference/api#param-template-messages)
template_messages
List[GenerationMessage]
A list of template messages for the prompt.
[​](https://docs.literalai.com/python-client/api-reference/api#param-settings)
settings
Optional[Dict]
Optional settings for the prompt.
[​](https://docs.literalai.com/python-client/api-reference/api#param-tools)
tools
Optional[List[Dict]]
Optional tool options for the model
**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-prompt)
Prompt
The prompt that was retrieved or created.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-prompt)
get_prompt
Copy
```
def get_prompt(id: Optional[str] = None,
        name: Optional[str] = None,
        version: Optional[int] = None) -> Prompt

```

Gets a prompt either by:
  * `id`
  * or `name` and (optional) `version`


Either the `id` or the `name` must be provided. If both are provided, the `id` is used.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-21)
id
str
The unique identifier of the prompt to retrieve.
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-13)
name
str
The name of the prompt to retrieve.
[​](https://docs.literalai.com/python-client/api-reference/api#param-version)
version
Optional[int]
The version number of the prompt to retrieve.
**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-prompt-1)
Prompt
The prompt with the given identifier or name.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-prompt-ab-testing)
get_prompt_ab_testing
Copy
```
def get_prompt_ab_testing(name: str) -> List[PromptRollout]

```

Get the A/B testing configuration for a prompt lineage.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-14)
name
str
The name of the prompt lineage.
**Returns** :
List[PromptRollout]
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#update-prompt-ab-testing)
update_prompt_ab_testing
Copy
```
def update_prompt_ab_testing(name: str, rollouts: List[PromptRollout]) -> Dict

```

Update the A/B testing configuration for a prompt lineage.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-15)
name
str
The name of the prompt lineage.
[​](https://docs.literalai.com/python-client/api-reference/api#param-rollouts)
rollouts
List[PromptRollout]
The percentage rollout for each prompt version.
**Returns** :
Dict
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-my-project-id)
get_my_project_id
Copy
```
def get_my_project_id()

```

Retrieves the projectId associated to the API key.
**Returns** :
The projectId associated to the API key.
## 
[​](https://docs.literalai.com/python-client/api-reference/api#asyncliteralapi)
AsyncLiteralAPI
Copy
```
class AsyncLiteralAPI(BaseLiteralAPI)

```

Copy
```
from literalai import AsyncLiteralClient
# Initialize the client
async_literalai_client = AsyncLiteralClient(api_key="your_api_key_here")
# Access the API's methods
print(async_literalai_client.api)

```

#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-users-2)
get_users
Copy
```
async def get_users(first: Optional[int] = None,
          after: Optional[str] = None,
          before: Optional[str] = None,
          filters: Optional[users_filters] = None)

```

Asynchronously fetches a list of users based on pagination and optional filters.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-first-5)
first
Optional[int]
The number of users to retrieve.
[​](https://docs.literalai.com/python-client/api-reference/api#param-after-5)
after
Optional[str]
A cursor for use in pagination, fetching records after this cursor.
[​](https://docs.literalai.com/python-client/api-reference/api#param-before-5)
before
Optional[str]
A cursor for use in pagination, fetching records before this cursor.
  * `filters` _Optional[users_filters]_ - Filters to apply to the user query.


**Returns** :
The result of the GraphQL helper function for fetching users.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-user-2)
get_user
Copy
```
async def get_user(id: Optional[str] = None, identifier: Optional[str] = None)

```

Asynchronously retrieves a user by ID or identifier.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-22)
id
Optional[str]
The unique identifier of the user to retrieve.
[​](https://docs.literalai.com/python-client/api-reference/api#param-identifier-4)
identifier
Optional[str]
An alternative identifier for the user.
**Returns** :
The result of the GraphQL helper function for fetching a user.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-user-2)
create_user
Copy
```
async def create_user(identifier: str, metadata: Optional[Dict] = None)

```

Asynchronously creates a new user with the specified identifier and optional metadata.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-identifier-5)
identifier
str
The identifier for the new user.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-14)
metadata
Optional[Dict]
Additional metadata for the user.
**Returns** :
The result of the GraphQL helper function for creating a user.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#update-user-2)
update_user
Copy
```
async def update_user(id: str,
           identifier: Optional[str] = None,
           metadata: Optional[Dict] = None)

```

Asynchronously updates an existing user identified by ID with new identifier and/or metadata.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-23)
id
str
The unique identifier of the user to update.
[​](https://docs.literalai.com/python-client/api-reference/api#param-identifier-6)
identifier
Optional[str]
New identifier for the user.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-15)
metadata
Optional[Dict]
New metadata for the user.
**Returns** :
The result of the GraphQL helper function for updating a user.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#delete-user-2)
delete_user
Copy
```
async def delete_user(id: str)

```

Asynchronously deletes a user identified by ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-24)
id
str
The unique identifier of the user to delete.
**Returns** :
The result of the GraphQL helper function for deleting a user.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-or-create-user-2)
get_or_create_user
Copy
```
async def get_or_create_user(identifier: str, metadata: Optional[Dict] = None)

```

Asynchronously retrieves a user by identifier or creates a new one if it does not exist.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-identifier-7)
identifier
str
The identifier of the user to retrieve or create.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-16)
metadata
Optional[Dict]
Metadata for the user if creation is necessary.
**Returns** :
The existing or newly created user.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-threads-2)
get_threads
Copy
```
async def get_threads(first: Optional[int] = None,
           after: Optional[str] = None,
           before: Optional[str] = None,
           filters: Optional[threads_filters] = None,
           order_by: Optional[threads_order_by] = None,
           step_types_to_keep: Optional[List[StepType]] = None)

```

Asynchronously fetches a list of threads based on pagination and optional filters and ordering.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-first-6)
first
Optional[int]
The number of threads to retrieve.
[​](https://docs.literalai.com/python-client/api-reference/api#param-after-6)
after
Optional[str]
A cursor for use in pagination, fetching records after this cursor.
[​](https://docs.literalai.com/python-client/api-reference/api#param-before-6)
before
Optional[str]
A cursor for use in pagination, fetching records before this cursor.
  * `filters` _Optional[threads_filters]_ - Filters to apply to the thread query.
  * `order_by` _Optional[threads_order_by]_ - Ordering criteria for the threads. step_types_to_keep (Optional[List[StepType]]) : If set, only steps of the corresponding types will be returned


**Returns** :
The result of the GraphQL helper function for fetching threads.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#list-threads-2)
list_threads
Copy
```
async def list_threads(first: Optional[int] = None,
            after: Optional[str] = None,
            before: Optional[str] = None,
            filters: Optional[threads_filters] = None,
            order_by: Optional[threads_order_by] = None)

```

Asynchronously lists threads based on pagination and optional filters and ordering, similar to `get_threads`.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-first-7)
first
Optional[int]
The number of threads to list.
[​](https://docs.literalai.com/python-client/api-reference/api#param-after-7)
after
Optional[str]
A cursor for use in pagination, fetching records after this cursor.
[​](https://docs.literalai.com/python-client/api-reference/api#param-before-7)
before
Optional[str]
A cursor for use in pagination, fetching records before this cursor.
  * `filters` _Optional[threads_filters]_ - Filters to apply to the thread query.
  * `order_by` _Optional[threads_order_by]_ - Ordering criteria for the threads.


**Returns** :
The result of the GraphQL helper function for listing threads.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-thread-2)
get_thread
Copy
```
async def get_thread(id: str)

```

Asynchronously retrieves a thread by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-25)
id
str
The unique identifier of the thread to retrieve.
**Returns** :
The result of the GraphQL helper function for fetching a thread.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-thread-2)
create_thread
Copy
```
async def create_thread(name: Optional[str] = None,
            metadata: Optional[Dict] = None,
            participant_id: Optional[str] = None,
            tags: Optional[List[str]] = None)

```

Asynchronously creates a new thread with specified details.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-16)
name
Optional[str]
The name of the thread.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-17)
metadata
Optional[Dict]
Metadata associated with the thread.
[​](https://docs.literalai.com/python-client/api-reference/api#param-participant-id-3)
participant_id
Optional[str]
Identifier for the participant associated with the thread.
[​](https://docs.literalai.com/python-client/api-reference/api#param-tags-6)
tags
Optional[List[str]]
Tags associated with the thread.
**Returns** :
The result of the GraphQL helper function for creating a thread.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#upsert-thread-2)
upsert_thread
Copy
```
async def upsert_thread(id: str,
            name: Optional[str] = None,
            metadata: Optional[Dict] = None,
            participant_id: Optional[str] = None,
            tags: Optional[List[str]] = None)

```

Asynchronously updates or inserts a thread based on the provided ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-26)
id
str
The unique identifier of the thread to upsert.
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-17)
name
Optional[str]
The name of the thread.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-18)
metadata
Optional[Dict]
Metadata associated with the thread.
[​](https://docs.literalai.com/python-client/api-reference/api#param-participant-id-4)
participant_id
Optional[str]
Identifier for the participant associated with the thread.
[​](https://docs.literalai.com/python-client/api-reference/api#param-tags-7)
tags
Optional[List[str]]
Tags associated with the thread.
**Returns** :
The result of the GraphQL helper function for upserting a thread.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#update-thread-2)
update_thread
Copy
```
async def update_thread(id: str,
            name: Optional[str] = None,
            metadata: Optional[Dict] = None,
            participant_id: Optional[str] = None,
            tags: Optional[List[str]] = None)

```

Asynchronously updates an existing thread identified by ID with new details.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-27)
id
str
The unique identifier of the thread to update.
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-18)
name
Optional[str]
New name of the thread.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-19)
metadata
Optional[Dict]
New metadata for the thread.
[​](https://docs.literalai.com/python-client/api-reference/api#param-participant-id-5)
participant_id
Optional[str]
New identifier for the participant.
[​](https://docs.literalai.com/python-client/api-reference/api#param-tags-8)
tags
Optional[List[str]]
New list of tags for the thread.
**Returns** :
The result of the GraphQL helper function for updating a thread.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#delete-thread-2)
delete_thread
Copy
```
async def delete_thread(id: str)

```

Asynchronously deletes a thread identified by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-28)
id
str
The unique identifier of the thread to delete.
**Returns** :
The result of the GraphQL helper function for deleting a thread.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-scores)
get_scores
Copy
```
async def get_scores(first: Optional[int] = None,
           after: Optional[str] = None,
           before: Optional[str] = None,
           filters: Optional[scores_filters] = None,
           order_by: Optional[scores_order_by] = None)

```

Asynchronously fetches scores based on pagination and optional filters.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-first-8)
first
Optional[int]
The number of scores to retrieve.
[​](https://docs.literalai.com/python-client/api-reference/api#param-after-8)
after
Optional[str]
A cursor for use in pagination, fetching records after this cursor.
[​](https://docs.literalai.com/python-client/api-reference/api#param-before-8)
before
Optional[str]
A cursor for use in pagination, fetching records before this cursor.
  * `filters` _Optional[scores_filters]_ - Filters to apply to the scores query.
  * `order_by` _Optional[scores_order_by]_ - Ordering options for the scores.


**Returns** :
The result of the GraphQL helper function for fetching scores.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-scores)
create_scores
Copy
```
async def create_scores(scores: List[ScoreDict])

```

Asynchronously creates multiple scores.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-scores)
scores
List[literalai.observability.step.ScoreDict]
A list of dictionaries representing the scores to be created.
**Returns** :
The result of the GraphQL helper function for creating scores.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-score-2)
create_score
Copy
```
async def create_score(name: str,
            value: float,
            type: ScoreType,
            step_id: Optional[str] = None,
            generation_id: Optional[str] = None,
            dataset_experiment_item_id: Optional[str] = None,
            comment: Optional[str] = None,
            tags: Optional[List[str]] = None)

```

Asynchronously creates a single score.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-19)
name
str
The name of the score.
[​](https://docs.literalai.com/python-client/api-reference/api#param-value-1)
value
float
The numerical value of the score.
[​](https://docs.literalai.com/python-client/api-reference/api#param-type-4)
type
ScoreType
The type of the score.
[​](https://docs.literalai.com/python-client/api-reference/api#param-step-id-3)
step_id
Optional[str]
The ID of the step associated with the score.
[​](https://docs.literalai.com/python-client/api-reference/api#param-generation-id-2)
generation_id
Deprecated - use step_id
[​](https://docs.literalai.com/python-client/api-reference/api#param-dataset-experiment-item-id-1)
dataset_experiment_item_id
Optional[str]
The ID of the dataset experiment item associated with the score.
[​](https://docs.literalai.com/python-client/api-reference/api#param-comment-1)
comment
Optional[str]
A comment associated with the score.
[​](https://docs.literalai.com/python-client/api-reference/api#param-tags-9)
tags
Optional[List[str]]
A list of tags associated with the score.
**Returns** :
The result of the GraphQL helper function for creating a score.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#update-score-2)
update_score
Copy
```
async def update_score(id: str, update_params: ScoreUpdate)

```

Asynchronously updates a score identified by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-29)
id
str
The unique identifier of the score to update.
[​](https://docs.literalai.com/python-client/api-reference/api#param-update-params-2)
update_params
ScoreUpdate
A dictionary of parameters to update.
**Returns** :
The result of the GraphQL helper function for updating a score.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#delete-score-2)
delete_score
Copy
```
async def delete_score(id: str)

```

Asynchronously deletes a score identified by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-30)
id
str
The unique identifier of the score to delete.
**Returns** :
The result of the GraphQL helper function for deleting a score.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#upload-file-2)
upload_file
Copy
```
async def upload_file(
    content: Union[bytes, str],
    thread_id: Optional[str] = None,
    mime: Optional[str] = "application/octet-stream") -> Dict

```

Asynchronously uploads a file to the server.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-content-2)
content
Union[bytes, str]
The content of the file to upload.
[​](https://docs.literalai.com/python-client/api-reference/api#param-thread-id-3)
thread_id
str
The ID of the thread associated with the file.
[​](https://docs.literalai.com/python-client/api-reference/api#param-mime-2)
mime
Optional[str]
The MIME type of the file.
**Returns** :
A dictionary containing the object key and URL of the uploaded file.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-attachment-2)
create_attachment
Copy
```
async def create_attachment(thread_id: str,
              step_id: str,
              id: Optional[str] = None,
              metadata: Optional[Dict] = None,
              mime: Optional[str] = None,
              name: Optional[str] = None,
              object_key: Optional[str] = None,
              url: Optional[str] = None,
              content: Optional[Union[bytes, str]] = None,
              path: Optional[str] = None) -> "Attachment"

```

Asynchronously creates an attachment and uploads it if content is provided.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-thread-id-4)
thread_id
str
The ID of the thread associated with the attachment.
[​](https://docs.literalai.com/python-client/api-reference/api#param-step-id-4)
step_id
str
The ID of the step associated with the attachment.
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-31)
id
Optional[str]
An optional unique identifier for the attachment.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-20)
metadata
Optional[Dict]
Optional metadata for the attachment.
[​](https://docs.literalai.com/python-client/api-reference/api#param-mime-3)
mime
Optional[str]
The MIME type of the attachment.
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-20)
name
Optional[str]
The name of the attachment.
[​](https://docs.literalai.com/python-client/api-reference/api#param-object-key-1)
object_key
Optional[str]
The object key for the attachment if already uploaded.
[​](https://docs.literalai.com/python-client/api-reference/api#param-url-1)
url
Optional[str]
The URL of the attachment if already uploaded.
[​](https://docs.literalai.com/python-client/api-reference/api#param-content-3)
content
Optional[Union[bytes, str]]
The content of the attachment to upload.
[​](https://docs.literalai.com/python-client/api-reference/api#param-path-1)
path
Optional[str]
The file path of the attachment if it is to be uploaded from a local file.
**Returns** :
The attachment object created after the upload and creation process.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#update-attachment-2)
update_attachment
Copy
```
async def update_attachment(id: str, update_params: AttachmentUpload)

```

Asynchronously updates an attachment identified by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-32)
id
str
The unique identifier of the attachment to update.
[​](https://docs.literalai.com/python-client/api-reference/api#param-update-params-3)
update_params
AttachmentUpload
A dictionary of parameters to update the attachment.
**Returns** :
The result of the GraphQL helper function for updating an attachment.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-attachment-2)
get_attachment
Copy
```
async def get_attachment(id: str)

```

Asynchronously retrieves an attachment by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-33)
id
str
The unique identifier of the attachment to retrieve.
**Returns** :
The result of the GraphQL helper function for fetching an attachment.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#delete-attachment-2)
delete_attachment
Copy
```
async def delete_attachment(id: str)

```

Asynchronously deletes an attachment identified by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-34)
id
str
The unique identifier of the attachment to delete.
**Returns** :
The result of the GraphQL helper function for deleting an attachment.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-step-2)
create_step
Copy
```
async def create_step(thread_id: Optional[str] = None,
           type: Optional[StepType] = "undefined",
           start_time: Optional[str] = None,
           end_time: Optional[str] = None,
           input: Optional[Dict] = None,
           output: Optional[Dict] = None,
           metadata: Optional[Dict] = None,
           parent_id: Optional[str] = None,
           name: Optional[str] = None,
           tags: Optional[List[str]] = None,
           root_run_id: Optional[str] = None)

```

Asynchronously creates a new step with the specified parameters.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-thread-id-5)
thread_id
Optional[str]
The ID of the thread associated with the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-type-5)
type
Optional[StepType]
The type of the step, defaults to “undefined”.
[​](https://docs.literalai.com/python-client/api-reference/api#param-start-time-2)
start_time
Optional[str]
The start time of the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-end-time-2)
end_time
Optional[str]
The end time of the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-input-3)
input
Optional[Dict]
Input data for the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-output-2)
output
Optional[Dict]
Output data from the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-21)
metadata
Optional[Dict]
Metadata associated with the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-parent-id-2)
parent_id
Optional[str]
The ID of the parent step, if any.
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-21)
name
Optional[str]
The name of the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-tags-10)
tags
Optional[List[str]]
Tags associated with the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-root-run-id-1)
root_run_id
Optional[str]
The ID of the root run, if any.
**Returns** :
The result of the GraphQL helper function for creating a step.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#update-step-2)
update_step
Copy
```
async def update_step(id: str,
           type: Optional[StepType] = None,
           input: Optional[str] = None,
           output: Optional[str] = None,
           metadata: Optional[Dict] = None,
           name: Optional[str] = None,
           tags: Optional[List[str]] = None,
           start_time: Optional[str] = None,
           end_time: Optional[str] = None,
           parent_id: Optional[str] = None)

```

Asynchronously updates an existing step identified by its ID with new parameters.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-35)
id
str
The unique identifier of the step to update.
[​](https://docs.literalai.com/python-client/api-reference/api#param-type-6)
type
Optional[StepType]
The type of the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-input-4)
input
Optional[str]
Input data for the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-output-3)
output
Optional[str]
Output data from the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-22)
metadata
Optional[Dict]
Metadata associated with the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-22)
name
Optional[str]
The name of the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-tags-11)
tags
Optional[List[str]]
Tags associated with the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-start-time-3)
start_time
Optional[str]
The start time of the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-end-time-3)
end_time
Optional[str]
The end time of the step.
[​](https://docs.literalai.com/python-client/api-reference/api#param-parent-id-3)
parent_id
Optional[str]
The ID of the parent step, if any.
**Returns** :
The result of the GraphQL helper function for updating a step.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-step-2)
get_step
Copy
```
async def get_step(id: str)

```

Asynchronously retrieves a step by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-36)
id
str
The unique identifier of the step to retrieve.
**Returns** :
The result of the GraphQL helper function for fetching a step.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#delete-step-2)
delete_step
Copy
```
async def delete_step(id: str)

```

Asynchronously deletes a step identified by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-37)
id
str
The unique identifier of the step to delete.
**Returns** :
The result of the GraphQL helper function for deleting a step.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#send-steps-2)
send_steps
Copy
```
async def send_steps(steps: List[Union[StepDict, "Step"]])

```

Asynchronously sends a list of steps to be processed.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-steps-1)
steps
List[Union[StepDict, Step]]
A list of steps or step dictionaries to send.
**Returns** :
The result of the GraphQL helper function for sending steps.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-generations-2)
get_generations
Copy
```
async def get_generations(first: Optional[int] = None,
             after: Optional[str] = None,
             before: Optional[str] = None,
             filters: Optional[generations_filters] = None,
             order_by: Optional[generations_order_by] = None)

```

Asynchronously fetches a list of generations based on pagination and optional filters.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-first-9)
first
Optional[int]
The number of generations to retrieve.
[​](https://docs.literalai.com/python-client/api-reference/api#param-after-9)
after
Optional[str]
A cursor for use in pagination, fetching records after this cursor.
[​](https://docs.literalai.com/python-client/api-reference/api#param-before-9)
before
Optional[str]
A cursor for use in pagination, fetching records before this cursor.
  * `filters` _Optional[generations_filters]_ - Filters to apply to the generations query.
  * `order_by` _Optional[generations_order_by]_ - Ordering options for the generations.


**Returns** :
The result of the GraphQL helper function for fetching generations.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-generation-2)
create_generation
Copy
```
async def create_generation(generation: Union[ChatGeneration,
                       CompletionGeneration])

```

Asynchronously creates a new generation with the specified details.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-generation-1)
generation
Union[ChatGeneration, CompletionGeneration]
The generation data to create.
**Returns** :
The result of the GraphQL helper function for creating a generation.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-dataset-2)
create_dataset
Copy
```
async def create_dataset(name: str,
             description: Optional[str] = None,
             metadata: Optional[Dict] = None,
             type: DatasetType = "key_value")

```

Asynchronously creates a new dataset with the specified details.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-23)
name
str
The name of the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-description-3)
description
Optional[str]
A description of the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-23)
metadata
Optional[Dict]
Metadata associated with the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-type-7)
type
DatasetType
The type of the dataset, defaults to “key_value”.
**Returns** :
The result of the GraphQL helper function for creating a dataset.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-dataset-2)
get_dataset
Copy
```
async def get_dataset(id: Optional[str] = None, name: Optional[str] = None)

```

Asynchronously retrieves a dataset by its ID or name.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-38)
id
Optional[str]
The unique identifier of the dataset to retrieve.
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-24)
name
Optional[str]
The name of the dataset to retrieve.
**Returns** :
The processed response from the REST API call.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#update-dataset-2)
update_dataset
Copy
```
async def update_dataset(id: str,
             name: Optional[str] = None,
             description: Optional[str] = None,
             metadata: Optional[Dict] = None)

```

Asynchronously updates an existing dataset identified by its ID with new details.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-39)
id
str
The unique identifier of the dataset to update.
[​](https://docs.literalai.com/python-client/api-reference/api#param-name-25)
name
Optional[str]
The new name of the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-description-4)
description
Optional[str]
A new description for the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-24)
metadata
Optional[Dict]
New metadata for the dataset.
**Returns** :
The result of the GraphQL helper function for updating a dataset.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#delete-dataset-2)
delete_dataset
Copy
```
async def delete_dataset(id: str)

```

Asynchronously deletes a dataset identified by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-40)
id
str
The unique identifier of the dataset to delete.
**Returns** :
The result of the GraphQL helper function for deleting a dataset.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-experiment-item-2)
create_experiment_item
Copy
```
async def create_experiment_item(
    experiment_item: DatasetExperimentItem) -> DatasetExperimentItem

```

Asynchronously creates an item within an experiment.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-experiment-item-1)
experiment_item
DatasetExperimentItem
The experiment item to be created.
**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dataset-experiment-item-1)
DatasetExperimentItem
The created experiment item with updated scores.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#create-dataset-item-2)
create_dataset_item
Copy
```
async def create_dataset_item(dataset_id: str,
               input: Dict,
               expected_output: Optional[Dict] = None,
               metadata: Optional[Dict] = None)

```

Asynchronously creates a dataset item.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dataset-id-4)
dataset_id
str
The unique identifier of the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-input-5)
input
Dict
The input data for the dataset item.
[​](https://docs.literalai.com/python-client/api-reference/api#param-expected-output-1)
expected_output
Optional[Dict]
The expected output data for the dataset item.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-25)
metadata
Optional[Dict]
Additional metadata for the dataset item.
**Returns** :
The result of the GraphQL helper function for creating a dataset item.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#get-dataset-item-2)
get_dataset_item
Copy
```
async def get_dataset_item(id: str)

```

Asynchronously retrieves a dataset item by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-41)
id
str
The unique identifier of the dataset item.
**Returns** :
The result of the GraphQL helper function for fetching a dataset item.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#delete-dataset-item-2)
delete_dataset_item
Copy
```
async def delete_dataset_item(id: str)

```

Asynchronously deletes a dataset item by its ID.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-id-42)
id
str
The unique identifier of the dataset item to delete.
**Returns** :
The result of the GraphQL helper function for deleting a dataset item.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#add-step-to-dataset-2)
add_step_to_dataset
Copy
```
async def add_step_to_dataset(dataset_id: str,
               step_id: str,
               metadata: Optional[Dict] = None)

```

Asynchronously adds a step to a dataset.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dataset-id-5)
dataset_id
str
The unique identifier of the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-step-id-5)
step_id
str
The unique identifier of the step to add.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-26)
metadata
Optional[Dict]
Additional metadata for the step being added.
**Returns** :
The result of the GraphQL helper function for adding a step to a dataset.
#### 
[​](https://docs.literalai.com/python-client/api-reference/api#add-generation-to-dataset-2)
add_generation_to_dataset
Copy
```
async def add_generation_to_dataset(dataset_id: str,
                  generation_id: str,
                  metadata: Optional[Dict] = None)

```

Asynchronously adds a generation to a dataset.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/api#param-dataset-id-6)
dataset_id
str
The unique identifier of the dataset.
[​](https://docs.literalai.com/python-client/api-reference/api#param-generation-id-3)
generation_id
str
The unique identifier of the generation to add.
[​](https://docs.literalai.com/python-client/api-reference/api#param-metadata-27)
metadata
Optional[Dict]
Additional metadata for the generation being added.
**Returns** :
The result of the GraphQL helper function for adding a generation to a dataset.

---

# Client

## Source
https://docs.literalai.com/python-client/api-reference/client

## 
[​](https://docs.literalai.com/python-client/api-reference/client#baseliteralclient)
BaseLiteralClient
Copy
```
class BaseLiteralClient()

```

Base class for LiteralClient and AsyncLiteralClient.
**Example** :
Copy
```
from literalai import LiteralClient, AsyncLiteralClient
# Initialize the client
client = LiteralClient(api_key="your_api_key_here")
async_client = AsyncLiteralClient(api_key="your_api_key_here")

```

**Attributes** :
[​](https://docs.literalai.com/python-client/api-reference/client#param-api)
api
Union[LiteralAPI, AsyncLiteralAPI]
The API client used for communication with Literal AI.
[​](https://docs.literalai.com/python-client/api-reference/client#param-disabled)
disabled
bool
Flag indicating whether the client is disabled.
[​](https://docs.literalai.com/python-client/api-reference/client#param-event-processor)
event_processor
EventProcessor
Processor for handling events.
#### 
[​](https://docs.literalai.com/python-client/api-reference/client#instrument-openai)
instrument_openai
Copy
```
def instrument_openai()

```

Instruments the OpenAI SDK so that all LLM calls are logged to Literal AI.
#### 
[​](https://docs.literalai.com/python-client/api-reference/client#instrument-mistralai)
instrument_mistralai
Copy
```
def instrument_mistralai()

```

Instruments the Mistral AI SDK so that all LLM calls are logged to Literal AI.
#### 
[​](https://docs.literalai.com/python-client/api-reference/client#instrument-llamaindex)
instrument_llamaindex
Copy
```
def instrument_llamaindex()

```

Instruments the Llama Index framework so that all RAG & LLM calls are logged to Literal AI.
#### 
[​](https://docs.literalai.com/python-client/api-reference/client#langchain-callback)
langchain_callback
Copy
```
def langchain_callback(to_ignore: Optional[List[str]] = None,
            to_keep: Optional[List[str]] = None,
            **kwargs: Any)

```

Creates a Callback for Langchain that logs all LLM calls to Literal AI.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/client#param-to-ignore)
to_ignore
Optional[List[str]]
Runs to ignore to declutter logging.
[​](https://docs.literalai.com/python-client/api-reference/client#param-to-keep)
to_keep
Optional[List[str]]
Runs to keep within ignored runs.
**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/client#param-langchain-tracer)
LangchainTracer
The callback to use in Langchain’s invoke methods.
#### 
[​](https://docs.literalai.com/python-client/api-reference/client#start-step)
start_step
Copy
```
def start_step(name: str = "",
        type: Optional[TrueStepType] = None,
        id: Optional[str] = None,
        parent_id: Optional[str] = None,
        thread_id: Optional[str] = None,
        root_run_id: Optional[str] = None,
        **kwargs)

```

Creates a step and starts it in the current context. To log it on Literal AI use `.end()`. This is used to create Agent steps. For conversational messages use `message` instead.
**Arguments** :
[​](https://docs.literalai.com/python-client/api-reference/client#name)
name
Optional[str]
The name of the step to create.
[​](https://docs.literalai.com/python-client/api-reference/client#type)
type
TrueStepType
The type of the step. Must be one of the following :
“run”, “tool”, “llm”, “embedding”, “retrieval”,“rerank”, “undefined”. 
[​](https://docs.literalai.com/python-client/api-reference/client#id)
id
Optional[str]
The id of the step to create.
[​](https://docs.literalai.com/python-client/api-reference/client#parent-id)
parent_id
Optional[str]
The id of the parent step.
[​](https://docs.literalai.com/python-client/api-reference/client#thread-id)
thread_id
Optional[str]
The id of the parent thread.
[​](https://docs.literalai.com/python-client/api-reference/client#root-run-id)
root_run_id
Optional[str]
The id of the root run.
**Returns** :
[​](https://docs.literalai.com/python-client/api-reference/client#param-step)
Step
the created step.
#### 
[​](https://docs.literalai.com/python-client/api-reference/client#get-current-step)
get_current_step
Copy
```
def get_current_step()

```

Gets the current step from the context.
#### 
[​](https://docs.literalai.com/python-client/api-reference/client#get-current-thread)
get_current_thread
Copy
```
def get_current_thread()

```

Gets the current thread from the context.
#### 
[​](https://docs.literalai.com/python-client/api-reference/client#get-current-root-run)
get_current_root_run
Copy
```
def get_current_root_run()

```

Gets the current root run from the context.
#### 
[​](https://docs.literalai.com/python-client/api-reference/client#reset-context)
reset_context
Copy
```
def reset_context()

```

Resets the context, forgetting active steps & setting current thread to None.
#### 
[​](https://docs.literalai.com/python-client/api-reference/client#flush-and-stop)
flush_and_stop
Copy
```
def flush_and_stop()

```

Sends all threads and steps to the Literal AI API. Waits synchronously for all API calls to be done.
## 
[​](https://docs.literalai.com/python-client/api-reference/client#literalclient)
LiteralClient
Copy
```
class LiteralClient(BaseLiteralClient)

```

Synchronous client for interacting with the Literal AI API.
**Example** :
Copy
```
from literalai import LiteralClient
# Initialize the client
client = LiteralClient(api_key="your_api_key_here")

```

## 
[​](https://docs.literalai.com/python-client/api-reference/client#asyncliteralclient)
AsyncLiteralClient
Copy
```
class AsyncLiteralClient(BaseLiteralClient)

```

Asynchronous client for interacting with the Literal AI API.
**Example** :
Copy
```
from literalai import AsyncLiteralClient
# Initialize the client
async_client = AsyncLiteralClient(api_key="your_api_key_here")

```

---

# Dataset

## Source
https://docs.literalai.com/python-client/api-reference/dataset

## 
[​](https://docs.literalai.com/python-client/api-reference/dataset#dataset)
Dataset
Copy
```
@dataclass(repr=False)
class Dataset(Utils)

```

#### 
[​](https://docs.literalai.com/python-client/api-reference/dataset#create-item)
create_item
Copy
```
def create_item(input: Dict,
        expected_output: Optional[Dict] = None,
        metadata: Optional[Dict] = None) -> DatasetItem

```

Create a new dataset item and add it to this dataset.
**Arguments** :
  * `input`: The input data for the dataset item.
  * `expected_output`: The output data for the dataset item (optional).
  * `metadata`: Metadata for the dataset item (optional).


**Returns** :
The created DatasetItem instance.
#### 
[​](https://docs.literalai.com/python-client/api-reference/dataset#create-experiment)
create_experiment
Copy
```
def create_experiment(name: str,
           prompt_id: Optional[str] = None,
           params: Optional[Dict] = None) -> DatasetExperiment

```

Creates a new dataset experiment based on this dataset.
**Arguments** :
  * `name`: The name of the experiment .
  * `prompt_id`: The Prompt ID used on LLM calls (optional).
  * `params`: The params used on the experiment.


**Returns** :
The created DatasetExperiment instance.
#### 
[​](https://docs.literalai.com/python-client/api-reference/dataset#delete-item)
delete_item
Copy
```
def delete_item(item_id: str)

```

Delete a dataset item from this dataset.
**Arguments** :
  * `api`: An instance of the DatasetAPI to make the call.
  * `item_id`: The ID of the dataset item to delete.


#### 
[​](https://docs.literalai.com/python-client/api-reference/dataset#add-step)
add_step
Copy
```
def add_step(step_id: str, metadata: Optional[Dict] = None) -> DatasetItem

```

Create a new dataset item based on a step and add it to this dataset.
**Arguments** :
  * `step_id`: The id of the step to add to the dataset.
  * `metadata`: Metadata for the dataset item (optional).


**Returns** :
The created DatasetItem instance.
#### 
[​](https://docs.literalai.com/python-client/api-reference/dataset#add-generation)
add_generation
Copy
```
def add_generation(generation_id: str,
          metadata: Optional[Dict] = None) -> DatasetItem

```

Create a new dataset item based on a generation and add it to this dataset.
**Arguments** :
  * `generation_id`: The id of the generation to add to the dataset.
  * `metadata`: Metadata for the dataset item (optional).


**Returns** :
The created DatasetItem instance.

---

# Generation

## Source
https://docs.literalai.com/python-client/api-reference/generation

## 
[​](https://docs.literalai.com/python-client/api-reference/generation#basegeneration)
BaseGeneration
Copy
```
@dataclass(repr=False)
class BaseGeneration(Utils)

```

Base class for generation objects, containing common attributes and methods.
**Attributes** :
[​](https://docs.literalai.com/python-client/api-reference/generation#param-id)
id
Optional[str]
The unique identifier of the generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-prompt-id)
prompt_id
Optional[str]
The unique identifier of the prompt associated with the generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-provider)
provider
Optional[str]
The provider of the generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-model)
model
Optional[str]
The model used for the generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-error)
error
Optional[str]
Any error message associated with the generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-settings)
settings
Optional[Dict]
Settings used for the generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-variables)
variables
Optional[Dict]
Variables used in the generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-tags)
tags
Optional[List[str]]
Tags associated with the generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-metadata)
metadata
Optional[Dict]
Metadata associated with the generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-tools)
tools
Optional[List[Dict]]
Tools used in the generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-token-count)
token_count
Optional[int]
The total number of tokens in the generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-input-token-count)
input_token_count
Optional[int]
The number of input tokens in the generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-output-token-count)
output_token_count
Optional[int]
The number of output tokens in the generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-tt-first-token)
tt_first_token
Optional[float]
Time to first token in the generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-token-throughput-ins)
token_throughput_in_s
Optional[float]
Token throughput in seconds.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-duration)
duration
Optional[float]
Duration of the generation.
**Methods** :
from_dict(cls, generation_dict: Dict) -> Union[“ChatGeneration”, “CompletionGeneration”]: Creates a generation object from a dictionary. to_dict(self) -> Dict: Converts the generation object to a dictionary.
## 
[​](https://docs.literalai.com/python-client/api-reference/generation#completiongeneration)
CompletionGeneration
Copy
```
@dataclass(repr=False)
class CompletionGeneration(BaseGeneration, Utils)

```

Represents a completion generation with a prompt and its corresponding completion.
**Attributes** :
[​](https://docs.literalai.com/python-client/api-reference/generation#param-prompt)
prompt
Optional[str]
The prompt text for the generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-completion)
completion
Optional[str]
The generated completion text.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-type)
type
GenerationType
The type of generation, which is set to GenerationType.COMPLETION.
## 
[​](https://docs.literalai.com/python-client/api-reference/generation#chatgeneration)
ChatGeneration
Copy
```
@dataclass(repr=False)
class ChatGeneration(BaseGeneration, Utils)

```

Represents a chat generation with a list of messages and a message completion.
**Attributes** :
[​](https://docs.literalai.com/python-client/api-reference/generation#param-messages)
messages
Optional[List[GenerationMessage]]
The list of messages in the chat generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-message-completion)
message_completion
Optional[GenerationMessage]
The completion message of the chat generation.
[​](https://docs.literalai.com/python-client/api-reference/generation#param-type-1)
type
GenerationType
The type of generation, which is set to GenerationType.CHAT.

---

# Step

## Source
https://docs.literalai.com/python-client/api-reference/step

## 
[​](https://docs.literalai.com/python-client/api-reference/step#step)
Step
Copy
```
class Step(Utils)

```

## 
[​](https://docs.literalai.com/python-client/api-reference/step#using-the-decorator-on-a-function)
Using the decorator on a function
If you want to create a step from a function, you should use the `@literalai_client.step` decorator.
The step is automatically ended and sent to the platform when the function returns.
Another advantage of using the decorator is that you get several variables automatically set for you:
  * **name** : The name of the step. This is automatically set to the function name.
  * **input** : The input of the step. This is automatically set to the arguments of the function.
  * **output** : The output of the step. This is automatically set to the return value of the function.


Here is how to use the decorator:
Copy
```
@literalai_client.step
def my_step():
  # do something

```

If you want to override the default step parameters, you can pass them to the decorator:
Copy
```
@literalai_client.step(id="my-step-id", name="My step", type="run")
def my_step():
  # do something

```

You can access the step object with the `get_current_step` method:
Copy
```
@literalai_client.step
def my_step():
  step = literalai_client.get_current_step()
  # do something

```

## 
[​](https://docs.literalai.com/python-client/api-reference/step#using-the-with-statement)
Using the `with` statement
If you want to create a step from a block of code, you should use the `with` statement.
The step is automatically ended and sent to the platform when the code exits the `with` block.
Copy
```
with literalai_client.step() as step:
  # do something

```

If you want to override the default step parameters, you can pass them to the `step` method:
Copy
```
with literalai_client.step(id="my-step-id", name="My step", type="run") as step:
  # do something

```

## 
[​](https://docs.literalai.com/python-client/api-reference/step#using-the-start-step-method)
Using the `start_step` method
This method should be used as a last resort because it doesn’t automatically end the step.
You must call the `end` method on the step object to end the step and send it to the platform.
Copy
```
step = literalai_client.start_step()
# do something
step.end()

```

You can either pass the step parameters to the `start_step` method, or set them directly on the step object:
Copy
```
step = literalai_client.start_step(id="my-step-id", name="My step", type="run")
step.input = "test input"
# do something
step.output = "Hello world"
step.end()

```

## 
[​](https://docs.literalai.com/python-client/api-reference/step#step-parameters)
Step parameters
[​](https://docs.literalai.com/python-client/api-reference/step#param-thread-id)
thread_id
uuid
The id of the thread
[​](https://docs.literalai.com/python-client/api-reference/step#param-id)
id
uuid
The id of the step. If not provided, a random uuid will be generated. Use custom ones to match your own system. Step ids must be unique across your project.
[​](https://docs.literalai.com/python-client/api-reference/step#param-name)
name
string
default:""
The name of the step (automatically set to the function name if using the decorator)
[​](https://docs.literalai.com/python-client/api-reference/step#param-type)
type
StepType
default:"undefined"
The type of the step. A Step can be one of the following types:
  * `run`: A generic step
  * `tool`: A step that runs a tool
  * `llm`: A step that runs a language model
  * `embedding`: A step that runs an embedding model
  * `retrieval`: A step that retrieves documents
  * `rerank`: A step that reranks documents
  * `undefined`: An undefined step


[​](https://docs.literalai.com/python-client/api-reference/step#param-metadata)
metadata
dict
default:"{}"
Metadata associated with the step. This enables you to add custom fields to your steps.
[​](https://docs.literalai.com/python-client/api-reference/step#param-parent-id)
parent_id
uuid
The id of the parent step. This enables you to create nested steps.
[​](https://docs.literalai.com/python-client/api-reference/step#param-start-time)
start_time
string
The start time of the step.
[​](https://docs.literalai.com/python-client/api-reference/step#param-end-time)
end_time
string
The end time of the step.
[​](https://docs.literalai.com/python-client/api-reference/step#param-created-at)
created_at
string
The server-side creation time of the step.
[​](https://docs.literalai.com/python-client/api-reference/step#param-input)
input
dict
A dictionary symbolizing an input. Prefer using `content` key to store a message.
[​](https://docs.literalai.com/python-client/api-reference/step#param-output)
output
dict
A dictionary symbolizing an output. Prefer using `content` key to store a message.
[​](https://docs.literalai.com/python-client/api-reference/step#param-tags)
tags
List[str]
default:"[]"
The tags of the step. This is a complimentary field to the metadata field. It enables you to add custom tags to your steps.
[​](https://docs.literalai.com/python-client/api-reference/step#param-generation)
generation
BaseGeneration
The generation object associated with the step.
[​](https://docs.literalai.com/python-client/api-reference/step#param-attachments)
attachments
List[Attachment]
default:"[]"
The attachments associated with the step.

---

# Thread

## Source
https://docs.literalai.com/python-client/api-reference/thread

## 
[​](https://docs.literalai.com/python-client/api-reference/thread#thread)
Thread
Copy
```
class Thread(Utils)

```

## 
[​](https://docs.literalai.com/python-client/api-reference/thread#using-the-with-statement)
Using the `with` statement
If you prefer to have more flexibility in logging Threads, you can use the `with` statement. You can create a thread and execute code within it using the `with` statement:
Copy
```
with literalai_client.thread() as thread:
  # do something

```

You can also continue a thread by passing the thread id to the `thread` method:
Copy
```

previous_thread_id = "UUID"
with literalai_client.thread(thread_id=previous_thread_id) as thread:
  # do something

```

## 
[​](https://docs.literalai.com/python-client/api-reference/thread#using-the-literal-ai-api-client)
Using the Literal AI API client
You can also create Threads using the `literalai_client.api.create_thread()` method.
Copy
```
thread = literalai_client.api.create_thread(
  participant_id="<PARTICIPANT_UUID>",
  environment="production",
  tags=["tag1", "tag2"],
  metadata={"key": "value"},
)

```

## 
[​](https://docs.literalai.com/python-client/api-reference/thread#using-chainlit)
Using Chainlit
If you built your LLM application with Chainlit, you don’t need to specify Threads in your code. Chainlit logs Threads for you by default.

---

# Changelog

## Source
https://docs.literalai.com/python-client/development/changelog

# 
[​](https://docs.literalai.com/python-client/development/changelog#0-1-1-november-13th%2C-2024)
`0.1.1` (November 13th, 2024)
  * Handle serialization of lists in JSON objects


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-1-0-november-12th%2C-2024)
`0.1.0` (November 12th, 2024)
  * Added `create_prompt_variant` API to create `Prompt` variants
  * Modified `create_experiment` to take as input a `Prompt` variant ID instead of a `Prompt` ID


Starting with `0.1.0`, you need to have Literal AI version 0.1.0-beta or above to create experiments.
# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-627-october-28th%2C-2024)
`0.0.627` (October 28th, 2024)
  * Added fields `label`, `status` and `tags` to `Prompt` query retrieval
  * Fixed serialization/deserialization in LangChain integration


Starting with `0.0.627`, you need to have Literal AI version 0.0.629-beta or above.
# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-626-october-21st%2C-2024)
`0.0.626` (October 21st, 2024)
  * Exposed `label` to `create_scores` API for non-solely numerical scores
  * Added `release` parameter to `LiteralClient` constructor.
  * Fixed `Prompt` object string representation


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-625-october-1st%2C-2024)
`0.0.625` (October 1st, 2024)
  * LlamaIndex instrumentation support extended to (in addition to `engine.query`): 
    * `llm.chat` and `llm.predict_and_call`
    * `agent.chat` for custom `FunctionCallingAgent` and LlamaIndex’s `OpenAIAgent`
    * stream versions of the above methods
  * Prevent duplicate LlamaIndex instrumentation


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-624-september-27th%2C-2024)
`0.0.624` (September 27th, 2024)
  * Make sure each experiment run has its own ID
  * LangChain integration: 
    * Allow additional variables in prompt template
    * Make callback handler serializable


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-623-september-24th%2C-2024)
`0.0.623` (September 24th, 2024)
  * Add token usage to LangChain integration
  * Improved LangChain integration to support all types of LangChain messages
  * Fixed LangChain missing tool call ID


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-612)
`0.0.612`
  * Removed runtime dependency on `llama-index`, only required during instrumentation.


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-611)
`0.0.611`
  * Adapt the Mistral AI integration to account for Mistral AI Python client changes
  * Added option to send a root run ID upon creation of a `Step`
  * The API `update_prompt_ab_testing` replaces the old `promote_prompt` and extends on it to allow for A/B testing
  * Tags and metadata arguments passed to LangChain will now show in logs
  * Fixed the event processor when batch not filled every X seconds


Starting with `0.0.611`, you need to have Literal AI version 0.0.617-beta or above.
# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-610)
`0.0.610`
  * Adapt the LlamaIndex integration to the new [instrumentation API](https://docs.llamaindex.ai/en/stable/module_guides/observability/instrumentation/#instrumentation)
  * Add option to send metadata on Generation creation


Starting with `0.0.610`, you need to have Literal AI version 0.0.615-beta or above.
The LLamaIndex integration is compatible with `llama-index` versions 0.10.58 and above.
# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-608)
`0.0.608`
  * LangChain integration improvements: 
    * runs logs have a properly nested structure
    * variables serialization for complex objects


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-607)
`0.0.607`
  * Added Mistral AI Instrumentation


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-606)
`0.0.606`
**Fixes**
  * JSON parse errors are now logged


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-605)
`0.0.605`
**Fixes**
  * The HTTP calls are now following redirections


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-604)
`0.0.604`
**Improvements**
  * Enhance error handling


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-603)
`0.0.603`
**Fixes**
  * Default prompt version to `None`


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-602)
`0.0.602`
**Fixes**
  * Flush doesn’t wait for the internal batch to be empty
  * Add `repr` to classes
  * Error with participant identifier


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-601)
`0.0.601`
**Improvements**
  * Strip bytes from steps


**New Features**
  * Add `get_steps` api


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-600)
`0.0.600`
**Improvements**
  * Change default batch size from 1 to 5


**New Features**
  * Rename `literal_` to `literalai_`


**Fixes**
  * Make params optional


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-509)
`0.0.509`
**Deprecations**
  * `format()` is deprecated. `format_messages()` should now be used.


**New Features**
  * Add support for tags with OpenAI Instrumentation


# 
[​](https://docs.literalai.com/python-client/development/changelog#0-0-508)
`0.0.508`
**Deprecations**
  * `create_prompt()` is deprecated. `get_or_create_prompt()` should now be used.


**New Features**
  * `get_or_create_prompt()`. For creating a new Prompt, use `get_or_create_prompt()`.
Copy
```
A `Prompt` is fully defined by its `name`, `template_messages`, `settings` and tools.
If a prompt already exists for the given arguments, it is returned.
Otherwise, a new prompt is created.
Args:
  name (str): The name of the prompt to retrieve or create.
  template_messages (List[GenerationMessage]): A list of template messages for the prompt.
  settings (Optional[Dict]): Optional settings for the prompt.
Returns:
  Prompt: The prompt that was retrieved or created.

```

---

# Introduction & Installation

## Source
https://docs.literalai.com/python-client/get-started/introduction

## [Open Source SDKCheck out the open-source Github Repo](https://github.com/Chainlit/literalai-python)
### 
[​](https://docs.literalai.com/python-client/get-started/introduction#introduction)
Introduction
In addition to the Python SDK documentation, all [Guides](https://docs.literalai.com/guides) showcase code examples in Python.
### 
[​](https://docs.literalai.com/python-client/get-started/introduction#installation)
Installation
To install the Python client, run the following command:
Copy
```
pip install literalai

```

You can use either the **synchronous** or **asynchronous** Literal AI Python client.
All APIs can be used both sync or async, except for functions on `Dataset`, which are always sync. All examples in the documentation are, for simplicity, using the synchronous client.
If you want to use the async functions, make sure to `await` the functions.
The Python client is not compatible with gunicorn’s `--preload` flag.
To instantiate the synchronous client:
Copy
```
import os
from literalai import LiteralClient
literalai_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY")) 

```

or, async:
Copy
```
import os
from literalai import AsyncLiteralClient
async_literalai_client = AsyncLiteralClient(api_key=os.getenv("LITERAL_API_KEY")) 

```

Additionally, you can pass the optional `batch_size` argument to batch multiple events - handled simultaneously by Literal. Events are batched up to `batch_size`. Defaults to 5.
#### 
[​](https://docs.literalai.com/python-client/get-started/introduction#disable-logging)
Disable Logging
When instantiating the client, you have the option to disable logging. This can be useful when you want to disable the logs without changing the code.
Copy
```
import os
from literalai import LiteralClient
literalai_client = LiteralClient(api_key=os.getenv("LITERAL_API_KEY"), disabled=True) # Disable decorators

```

---

# Manual Deployment

## Source
https://docs.literalai.com/self-hosting/deployment

You can deploy the Literal AI platform anywhere by provisioning the infrastructure, setting up the environment variables and running the Docker image.
# 
[​](https://docs.literalai.com/self-hosting/deployment#get-the-image)
Get the Image
Copy
```
docker login --username literalai
# When prompted for a password, use the token you have been provided with
docker pull literalai/platform:latest

```

# 
[​](https://docs.literalai.com/self-hosting/deployment#provision-the-infrastructure)
Provision the Infrastructure
To run the Literal AI platform, you need to set up the following services:
  1. PostgreSQL database >= 15
  2. File storage (AWS S3, Google Cloud Storage, Azure Blob Storage)
  3. [Optional] Redis cache
  4. [Optional] SMTP server


## 
[​](https://docs.literalai.com/self-hosting/deployment#disclaimer)
Disclaimer
It is your responsibility to ensure that the infrastructure is secure and compliant with your company standards. Here are some recommendations:
  * Run the database and redis cache in a private network so that only the container running the Literal AI platform can access them.
  * Disallow public access to the file storage.
  * Disable credential authentication and use OAuth providers for authentication.


# 
[​](https://docs.literalai.com/self-hosting/deployment#configure-environment-variables)
Configure Environment Variables
Before running the container, you need to set up the environment variables.
## 
[​](https://docs.literalai.com/self-hosting/deployment#1-database-configuration)
1. Database Configuration
The [pgcrypto](https://www.postgresql.org/docs/current/pgcrypto.html) module is required for PostgreSQL. When using Azure, you will need to manually [add the extension](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/concepts-extensions).
### 
[​](https://docs.literalai.com/self-hosting/deployment#required)
Required
You can either pass the full connection string or the individual components.
Variable| Description| Example  
---|---|---  
`DATABASE_URL`| URL for database communication (PostgreSQL supported).| `postgresql://username:password@host:port/db_name`  
OR
Variable| Description  
---|---  
`DATABASE_HOST`| Host of the database.  
`DATABASE_USERNAME`| Database username.  
`DATABASE_PASSWORD`| Password for the DB user.  
`DATABASE_NAME`| Database name.  
### 
[​](https://docs.literalai.com/self-hosting/deployment#optional)
Optional
Variable| Description| Example  
---|---|---  
`DATABASE_DIRECT_URL`| Direct URL for database migrations (cannot use pg-bouncer).| `postgresql://username:password@host:port/db_name`  
`DATABASE_SSL`| Set to ‘true’ if using a secure network.| `false`  
## 
[​](https://docs.literalai.com/self-hosting/deployment#2-authentication-configuration)
2. Authentication Configuration
### 
[​](https://docs.literalai.com/self-hosting/deployment#provider-agnostic)
Provider Agnostic
Variable| Description| Example  
---|---|---  
`NEXTAUTH_SECRET`| Secret for encrypting JWT and hashing tokens.| `your secret`  
`NEXTAUTH_URL`| URL for your domain.| `https://yourdomain.com`  
`ENABLE_CREDENTIALS_AUTH`| Enable credentials (email password) authentication.| `false`  
### 
[​](https://docs.literalai.com/self-hosting/deployment#provider-specific)
Provider Specific
Remember you will have to allow the OAuth redirect URL in your provider’s settings.
Variable| Description| OAuth Redirect URL  
---|---|---  
[Azure](https://next-auth.js.org/providers/azure-ad)| `AZURE_AD_CLIENT_ID` `AZURE_AD_CLIENT_SECRET` `AZURE_AD_TENANT_ID`| `/api/auth/callback/azure-ad`  
[Google](https://next-auth.js.org/providers/google)| `GOOGLE_CLIENT_ID` `GOOGLE_CLIENT_SECRET`| `/api/auth/callback/google`  
[Okta](https://next-auth.js.org/providers/okta)| `OKTA_CLIENT_ID` `OKTA_CLIENT_SECRET` `OKTA_ISSUER`| `/api/auth/callback/okta`  
[GitHub](https://next-auth.js.org/providers/github)| `GITHUB_ID` `GITHUB_SECRET`| `/api/auth/callback/github`  
## 
[​](https://docs.literalai.com/self-hosting/deployment#3-file-storage-configuration)
3. File Storage Configuration
### 
[​](https://docs.literalai.com/self-hosting/deployment#provider-agnostic-2)
Provider Agnostic
Variable| Description| Example  
---|---|---  
`BUCKET_NAME`| Name of the bucket| `my_bucket`  
### 
[​](https://docs.literalai.com/self-hosting/deployment#provider-specific-2)
Provider Specific
Provider| Variables| Comment  
---|---|---  
AWS S3| `APP_AWS_ACCESS_KEY` `APP_AWS_SECRET_KEY` `APP_AWS_REGION`  
Google Cloud Storage| `APP_GCS_PROJECT_ID` `APP_GCS_CLIENT_EMAIL` `APP_GCS_PRIVATE_KEY`| Private key should be base64 encoded.  
Azure Blob Storage| `APP_AZURE_STORAGE_ACCOUNT` `APP_AZURE_STORAGE_ACCESS_KEY`  
### 
[​](https://docs.literalai.com/self-hosting/deployment#configure-cors-for-your-storage-solution%3A)
Configure CORS for your storage solution:
  * [AWS S3](https://repost.aws/knowledge-center/s3-configure-cors)
  * [Google Cloud Storage](https://cloud.google.com/storage/docs/using-cors?hl=fr#command-line)
  * [Azure Blob Storage](https://learn.microsoft.com/en-us/rest/api/storageservices/cross-origin-resource-sharing--cors--support-for-the-azure-storage-services)


## 
[​](https://docs.literalai.com/self-hosting/deployment#4-cache-configuration-%5Boptional%5D)
4. Cache Configuration [Optional]
Setting up a cache is optional, but recommended to decrease latency.
Either `REDIS_URL` or `REDISHOST` and `REDISPORT` can be used.
Variable| Description| Example  
---|---|---  
`REDIS_URL`| Full Redis URL.| `redis://127.0.0.1:6379/0`  
`REDISHOST`| Redis host (useful with Google Cloud Run).| `127.0.0.1`  
`REDISPORT`| Redis port.| `6379`  
The cache allows for asynchronous task handling such as:
  * step ingestion
  * AI evals monitoring
  * experiment runs


Control over the queues and workers handling the asynchronous tasks requires an authentication token defined via the `ADMIN_AUTH_TOKEN` environment variable. Set it to a non-empty value to be able to investigate your cache-based queueing system.
## 
[​](https://docs.literalai.com/self-hosting/deployment#5-smtp-configuration-%5Boptional%5D)
5. SMTP Configuration [Optional]
Setting up SMTP is optional. It enables Literal AI to send emails for password resets and project invitations.
Variable| Description| Example  
---|---|---  
`EMAIL_SERVER_HOST`| Mail server host.| `"servicehost"`  
`EMAIL_SERVER_PORT`| Mail server port.| `"4000"`  
`EMAIL_SERVICE`| Email service provider.| `"gmail"`  
`EMAIL_USER`| Email username.| `"username"`  
`EMAIL_PASS`| Email password.| `"password"`  
`EMAIL_FROM`| Sender email address.| `"noreply@service.com"`  
## 
[​](https://docs.literalai.com/self-hosting/deployment#configure-the-port-and-start-the-container)
Configure the Port and Start the Container
The server within the Docker image runs on port `3000`. Ensure this port is exposed.
## 
[​](https://docs.literalai.com/self-hosting/deployment#start-the-container)
Start the Container
You should now be able to start the container and access the Literal AI Platform.

---

# Get Started

## Source
https://docs.literalai.com/self-hosting/get-started

To self host the Literal AI platform, you will need to deploy the Docker image on your infrastructure. The Docker image is available on a private registry. You will need to [contact us](https://forms.gle/uyKyZmmpVET95Wbx8) to get access to the registry.
If you are looking to try out the platform, you can use the [cloud hosted version](https://cloud.getliteral.ai/) for free!
# 
[​](https://docs.literalai.com/self-hosting/get-started#deploying-the-image)
Deploying the Image
You have several options for hosting the platform:
## [AzureDeploy the Literal AI platform on Azure in a few minutes with the Azure Developer CLI.](https://github.com/Azure-Samples/literalai-azure)## [AWSDeploy the Literal AI platform on AWS in a few minutes with the CDK.](https://github.com/Chainlit/literal-on-aws)## [Google CloudComing soon.](https://docs.literalai.com/self-hosting/get-started)## [Manual DeploymentDeploy the Literal AI platform anywhere. You will have to provision the infrastructure and deploy the Docker image.](https://docs.literalai.com/self-hosting/deployment)
# 
[​](https://docs.literalai.com/self-hosting/get-started#update-the-sdks)
Update the SDKs
By default, the Literal AI SDKs point to the cloud hosted version of the platform. To point the SDKs to your self-hosted platform you will have to update the `url` parameter in the SDK instantiation:
Python
TypeScript
Copy
```
from literalai.client import LiteralClient
MY_LITERAL_API_URL = "http://localhost:3000"
literalai_client = LiteralClient(api_key="YOUR_API_KEY", url=MY_LITERAL_API_URL)

```

# 
[​](https://docs.literalai.com/self-hosting/get-started#update-chainlit)
Update Chainlit
If you are using Chainlit, you will need to update the `LITERAL_API_URL` environment variable to point to your self-hosted platform.

---

# Api

## Source
https://docs.literalai.com/typescript-client/api-reference/api

Represents the API client for interacting with the Literal service. This class handles API requests, authentication, and provides methods for various operations supported by the Literal API.
To use this API, you need to initialize a LiteralClient and access the API through it:
Copy
```
const literalAiClient = new LiteralClient({apiKey: process.env["LITERAL_API_KEY"]});
const api = literalAiClient.api;

```

Then you can use the `api` object to make calls to the Literal service.
## 
[​](https://docs.literalai.com/typescript-client/api-reference/api#methods)
Methods
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getprojectid)
getProjectId()
Copy
```
getProjectId(): Promise<string>

```

Get the project id associated with the API key.
**Returns**
`Promise`<`string`>
the project id
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#sendsteps)
sendSteps()
Copy
```
sendSteps(steps): Promise<any>

```

Sends a collection of steps to the GraphQL endpoint.
This method constructs a GraphQL query using the provided steps, then executes the query.
**Parameters**
Parameter| Type| Description  
---|---|---  
`steps`| `Step`[]| An array of Step objects to be sent.  
**Returns**
type| description  
---|---  
`Promise`<`any`>| The response from the GraphQL call  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getsteps)
getSteps()
Copy
```
getSteps(variables): Promise<PaginatedResponse<Step>>

```

Retrieves a paginated list of steps (runs) based on the provided criteria.
**Parameters**
Parameter| Type| Description  
---|---|---  
`variables`| `object`| The parameters to filter and paginate the steps.  
`variables.first`?| `Maybe`<`number`>| The number of steps to retrieve after the cursor. (Optional)  
`variables.after`?| `Maybe`<`string`>| The cursor to start retrieving steps after. (Optional)  
`variables.before`?| `Maybe`<`string`>| The cursor to start retrieving steps before. (Optional)  
`variables.filters`?| `StepsFilter`[]| The filters to apply on the steps retrieval. (Optional)  
`variables.orderBy`?| `StepsOrderBy`| The order in which to retrieve the steps. (Optional)  
**Returns**
type| description  
---|---  
`Promise`<`PaginatedResponse`<`Step`>>| A promise that resolves to a paginated response of steps  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getstep)
getStep()
Copy
```
getStep(id): Promise<Maybe<Step>>

```

Retrieves a step by its ID.
This method constructs a GraphQL query to fetch a step by its unique identifier. It then executes the query and returns the step if found.
**Parameters**
Parameter| Type| Description  
---|---|---  
`id`| `string`| The unique identifier of the step to retrieve.  
**Returns**
type| description  
---|---  
`Promise`<`Maybe`<`Step`>>| A `Promise` that resolves to the step if found, or `null` if not found  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#deletestep)
deleteStep()
Copy
```
deleteStep(id): Promise<string>

```

Deletes a Step from the platform by its unique identifier.
This method constructs a GraphQL mutation to delete a step by its unique identifier. It then executes the mutation and returns the ID of the deleted step.
**Parameters**
Parameter| Type| Description  
---|---|---  
`id`| `string`| The unique identifier of the step to delete.  
**Returns**
type| description  
---|---  
`Promise`<`string`>| A `Promise` that resolves to the ID of the deleted step  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#uploadfile)
uploadFile()
**uploadFile(params)**
Copy
```
uploadFile(params): Promise<object>

```

Uploads a file to a specified thread. This method supports uploading either through direct content or via a file path. It first signs the upload through a pre-configured endpoint and then proceeds to upload the file using the signed URL.
**Parameters**
Parameter| Type| Description  
---|---|---  
`params`| `UploadFileParamsWithContent`| The parameters for uploading a file, including:  
**Returns**
type| description  
---|---  
`Promise`<`object`>| An object containing the `objectKey` of the uploaded file and the signed `url`, or `null` values if the upload fails  
**Throws**
Throws an error if neither `content` nor `path` is provided, or if the server response is invalid.
**uploadFile(params)**
Copy
```
uploadFile(params): Promise<object>

```

**Parameters**
Parameter| Type  
---|---  
`params`| `UploadFileParamsWithPath`  
**Returns**
`Promise`<`object`>
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#createattachment)
createAttachment()
**createAttachment(params)**
Copy
```
createAttachment(params): Promise<Attachment>

```

Uploads a file to a specified thread and creates an attachment object. If called inside a context, the attachment will be added to the current step and thread.
**Parameters**
Parameter| Type| Description  
---|---|---  
`params`| `UploadFileBaseParams` & `object` & `CreateAttachmentParams`| The parameters for uploading a file, including:  
**Returns**
type| description  
---|---  
`Promise`<`Attachment`>| An object containing the `objectKey` of the uploaded file and the signed `url`, or `null` values if the upload fails  
**Throws**
Throws an error if neither `content` nor `path` is provided, or if the server response is invalid.
**createAttachment(params)**
Copy
```
createAttachment(params): Promise<Attachment>

```

**Parameters**
Parameter| Type  
---|---  
`params`| `UploadFileBaseParams` & `object` & `CreateAttachmentParams`  
**Returns**
`Promise`<`Attachment`>
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getgenerations)
getGenerations()
Copy
```
getGenerations(variables): Promise<PaginatedResponse<PersistedGeneration>>

```

Retrieves a paginated list of Generations based on the provided filters and sorting order.
**Parameters**
Parameter| Type| Description  
---|---|---  
`variables`| `object`| The variables to filter and sort the Generations. It includes:- `first`: The number of items to return.- `after`: The cursor to fetch items after.- `before`: The cursor to fetch items before.- `filters`: The filters applied to the Generations.- `orderBy`: The order in which the Generations are sorted.  
`variables.first`?| `Maybe`<`number`>| -  
`variables.after`?| `Maybe`<`string`>| -  
`variables.before`?| `Maybe`<`string`>| -  
`variables.filters`?| `GenerationsFilter`[]| -  
`variables.orderBy`?| `GenerationsOrderBy`| -  
**Returns**
type| description  
---|---  
`Promise`<`PaginatedResponse`<`PersistedGeneration`>>| A `Promise` that resolves to a `PaginatedResponse<Generation>` object containing the filtered and sorted Generations  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#creategeneration)
createGeneration()
Copy
```
createGeneration(generation): Promise<PersistedGeneration>

```

Creates a new generation entity and sends it to the platform.
**Parameters**
Parameter| Type| Description  
---|---|---  
`generation`| `Generation`| The `Generation` object to be created and sent to the platform.  
**Returns**
type| description  
---|---  
`Promise`<`PersistedGeneration`>| A Promise resolving to the newly created `Generation` object  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#upsertthread)
upsertThread()
**upsertThread(options)**
Copy
```
upsertThread(options): Promise<CleanThreadFields>

```

Upserts a Thread with new information.
**Parameters**
Parameter| Type| Description  
---|---|---  
`options`| `object`| The parameters to upsert a thread.  
`options.threadId`| `string`| The unique identifier of the thread. (Required)  
`options.name`?| `Maybe`<`string`>| The name of the thread. (Optional)  
`options.metadata`?| `Maybe`<`Record`<`string`, `any`>>| Additional metadata for the thread as a key-value pair object. (Optional)  
`options.participantId`?| `Maybe`<`string`>| The unique identifier of the participant. (Optional)  
`options.tags`?| `Maybe`<`string`[]>| An array of tags associated with the thread. (Optional)  
**Returns**
type| description  
---|---  
`Promise`<`CleanThreadFields`>| The upserted thread object  
**upsertThread(threadId, name, metadata, participantId, tags)**
Copy
```
upsertThread(
  threadId, 
  name?, 
  metadata?, 
  participantId?, 
tags?): Promise<CleanThreadFields>

```

Upserts a Thread with new information.
**Parameters**
Parameter| Type| Description  
---|---|---  
`threadId`| `string`| The unique identifier of the thread. (Required)  
`name`?| `Maybe`<`string`>| The name of the thread. (Optional)  
`metadata`?| `Maybe`<`Record`<`string`, `any`>>| Additional metadata for the thread as a key-value pair object. (Optional)  
`participantId`?| `Maybe`<`string`>| The unique identifier of the participant. (Optional)  
`tags`?| `Maybe`<`string`[]>| An array of tags associated with the thread. (Optional)  
**Returns**
type| description  
---|---  
`Promise`<`CleanThreadFields`>| The upserted thread object  
**Deprecated**
Use one single object attribute instead of multiple parameters.
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getthreads)
getThreads()
Copy
```
getThreads(variables): Promise<PaginatedResponse<Thread>>

```

Retrieves a paginated list of threads (conversations) based on the provided criteria.
**Parameters**
Parameter| Type| Description  
---|---|---  
`variables`| `object`| The parameters to filter and paginate the threads.  
`variables.first`?| `Maybe`<`number`>| The number of threads to retrieve after the cursor. (Optional)  
`variables.after`?| `Maybe`<`string`>| The cursor to start retrieving threads after. (Optional)  
`variables.before`?| `Maybe`<`string`>| The cursor to start retrieving threads before. (Optional)  
`variables.filters`?| `ThreadsFilter`[]| The filters to apply on the threads retrieval. (Optional)  
`variables.orderBy`?| `ThreadsOrderBy`| The order in which to retrieve the threads. (Optional)  
`variables.stepTypesToKeep`?| `StepType`[]| -  
**Returns**
type| description  
---|---  
`Promise`<`PaginatedResponse`<`Thread`>>| A promise that resolves to a paginated response of threads  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getthread)
getThread()
Copy
```
getThread(id): Promise<Maybe<Thread>>

```

Retrieves information from a single Thread.
**Parameters**
Parameter| Type| Description  
---|---|---  
`id`| `string`| The unique identifier of the thread. This parameter is required.  
**Returns**
type| description  
---|---  
`Promise`<`Maybe`<`Thread`>>| The detailed information of the specified thread  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#deletethread)
deleteThread()
Copy
```
deleteThread(id): Promise<string>

```

Deletes a single Thread by its unique identifier.
**Parameters**
Parameter| Type| Description  
---|---|---  
`id`| `string`| The unique identifier of the thread to be deleted. This parameter is required.  
**Returns**
type| description  
---|---  
`Promise`<`string`>| The ID of the deleted thread  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getusers)
getUsers()
Copy
```
getUsers(variables): Promise<PaginatedResponse<OmitUtils<User>>>

```

Retrieves a list of users with optional filters.
**Parameters**
Parameter| Type| Description  
---|---|---  
`variables`| `object`| The parameters used to filter and paginate the user list.  
`variables.first`?| `Maybe`<`number`>| Optional. The number of items to return.  
`variables.after`?| `Maybe`<`string`>| Optional. The cursor after which to start fetching data.  
`variables.before`?| `Maybe`<`string`>| Optional. The cursor before which to start fetching data.  
`variables.filters`?| `ParticipantsFilter`[]| Optional. Array of filters to apply to the user query.  
**Returns**
type| description  
---|---  
`Promise`<`PaginatedResponse`<`OmitUtils`<`User`>>>| A `PaginatedResponse` containing a list of users without utility types  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#createuser)
createUser()
Copy
```
createUser(identifier, metadata?): Promise<User>

```

Creates a new user and sends it to the platform.
**Parameters**
Parameter| Type| Description  
---|---|---  
`identifier`| `string`| The unique identifier for the user. This parameter is required.  
`metadata`?| `Maybe`<`Record`<`string`, `any`>>| Optional metadata for the user. This parameter is optional.  
**Returns**
type| description  
---|---  
`Promise`<`User`>| A promise that resolves with the newly created User object  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#updateuser)
updateUser()
Copy
```
updateUser(
  id, 
  identifier?, 
metadata?): Promise<User>

```

Updates an existing user’s details in the platform.
**Parameters**
Parameter| Type| Description  
---|---|---  
`id`| `string`| The unique identifier of the user to update. This parameter is required.  
`identifier`?| `string`| A new identifier for the user. This parameter is optional.  
`metadata`?| `Maybe`<`Record`<`string`, `any`>>| Additional metadata for the user. This parameter is optional.  
**Returns**
type| description  
---|---  
`Promise`<`User`>| A promise that resolves with the updated User object  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getorcreateuser)
getOrCreateUser()
Copy
```
getOrCreateUser(identifier, metadata?): Promise<string>

```

Retrieves an existing user by their identifier or creates a new one if they do not exist.
**Parameters**
Parameter| Type| Description  
---|---|---  
`identifier`| `string`| The unique identifier for the user. This parameter is required.  
`metadata`?| `Maybe`<`Record`<`string`, `any`>>| Additional metadata for the user. This parameter is optional.  
**Returns**
type| description  
---|---  
`Promise`<`string`>| The ID of the existing or newly created user  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getuser)
getUser()
Copy
```
getUser(identifier): Promise<Maybe<User>>

```

Retrieves a user by their unique identifier.
**Parameters**
Parameter| Type| Description  
---|---|---  
`identifier`| `string`| The unique identifier for the user. This parameter is required.  
**Returns**
type| description  
---|---  
`Promise`<`Maybe`<`User`>>| A `Promise` that resolves to a `User` object if found, otherwise `undefined`  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#deleteuser)
deleteUser()
Copy
```
deleteUser(id): Promise<string>

```

Deletes a single user by their unique identifier.
**Parameters**
Parameter| Type| Description  
---|---|---  
`id`| `string`| The unique identifier of the user to be deleted. This parameter is required.  
**Returns**
type| description  
---|---  
`Promise`<`string`>| A `Promise` that resolves to the ID of the deleted user  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getscores)
getScores()
Copy
```
getScores(variables): Promise<PaginatedResponse<OmitUtils<Score>>>

```

Get all scores connected to the platform.
**Parameters**
Parameter| Type| Description  
---|---|---  
`variables`| `object`| The parameters for querying scores.  
`variables.first`?| `Maybe`<`number`>| Optional. The number of scores to retrieve.  
`variables.after`?| `Maybe`<`string`>| Optional. The cursor after which to start fetching scores.  
`variables.before`?| `Maybe`<`string`>| Optional. The cursor before which to start fetching scores.  
`variables.filters`?| `ScoresFilter`[]| Optional. Filters to apply to the score query.  
`variables.orderBy`?| `ScoresOrderBy`| Optional. The order in which to sort the scores.  
**Returns**
type| description  
---|---  
`Promise`<`PaginatedResponse`<`OmitUtils`<`Score`>>>| A `Promise` that resolves to a paginated response of scores, excluding certain utility fields  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#createscores)
createScores()
Copy
```
createScores(scores): Promise<Score[]>

```

Creates multiple scores in the database using the provided array of scores. Each score in the array is transformed into a GraphQL mutation call.
**Parameters**
Parameter| Type| Description  
---|---|---  
`scores`| `Score`[]| An array of `Score` objects to be created.  
**Returns**
type| description  
---|---  
`Promise`<`Score`[]>| A promise that resolves to an array of `Score` instances populated with the created scores’ data  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#createscore)
createScore()
Copy
```
createScore(variables): Promise<Score>

```

Creates a new score in the database using the provided parameters.
**Parameters**
Parameter| Type| Description  
---|---|---  
`variables`| `OmitUtils`<`Score`>| The score details to be used in the creation process. This includes:  
**Returns**
type| description  
---|---  
`Promise`<`Score`>| A new `Score` instance populated with the created score’s data  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#updatescore)
updateScore()
Copy
```
updateScore(id, updateParams): Promise<Score>

```

Updates an existing score in the database.
**Parameters**
Parameter| Type| Description  
---|---|---  
`id`| `string`| The unique identifier of the score to update. (required)  
`updateParams`| `object`| The parameters to update in the score. (required)  
`updateParams.comment`?| `Maybe`<`string`>| A new or updated comment for the score. (optional)  
`updateParams.value`| `number`| The new value to set for the score. (required)  
**Returns**
type| description  
---|---  
`Promise`<`Score`>| A `Score` instance representing the updated score  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#deletescore)
deleteScore()
Copy
```
deleteScore(id): Promise<any>

```

Deletes a single score from the database.
**Parameters**
Parameter| Type| Description  
---|---|---  
`id`| `string`| The unique identifier of the score to delete. (required)  
**Returns**
type| description  
---|---  
`Promise`<`any`>| The ID of the deleted score  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getdatasets)
getDatasets()
Copy
```
getDatasets(): Promise<object[]>

```

List all datasets in the platform.
**Returns**
type| description  
---|---  
`Promise`<`object`[]>| The names and ids of all datasets  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#createdataset)
createDataset()
Copy
```
createDataset(dataset): Promise<Dataset>

```

Creates a new dataset in the database.
**Parameters**
Parameter| Type| Description  
---|---|---  
`dataset`| `object`| The dataset details to be created.  
`dataset.name`| `string`| The name of the dataset. (required)  
`dataset.description`?| `Maybe`<`string`>| The description of the dataset. (optional)  
`dataset.metadata`?| `Maybe`<`Record`<`string`, `any`>>| Additional metadata for the dataset as a key-value pair object. (optional)  
`dataset.type`?| `DatasetType`| The type of the dataset, defined by the DatasetType enum. (optional)  
**Returns**
type| description  
---|---  
`Promise`<`Dataset`>| A new Dataset instance populated with the created dataset’s data  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getdataset)
getDataset()
Copy
```
getDataset(variables): Promise<null | Dataset>

```

Retrieves a dataset based on provided ID or name.
**Parameters**
Parameter| Type| Description  
---|---|---  
`variables`| `object`| An object containing optional `id` and `name` properties to specify which dataset to retrieve.  
`variables.id`?| `string`| -  
`variables.name`?| `string`| -  
**Returns**
type| description  
---|---  
`Promise`<`null` | `Dataset`>| A `Dataset` instance populated with the retrieved dataset’s data, or `null` if no data is found  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#updatedataset)
updateDataset()
Copy
```
updateDataset(id, dataset): Promise<Dataset>

```

Updates a dataset with new information.
**Parameters**
Parameter| Type| Description  
---|---|---  
`id`| `string`| The unique identifier of the dataset to update. This parameter is required.  
`dataset`| `object`| An object containing the new dataset information.  
`dataset.name`?| `Maybe`<`string`>| The new name of the dataset. (optional)  
`dataset.description`?| `Maybe`<`string`>| The new description of the dataset. (optional)  
`dataset.metadata`?| `Maybe`<`Record`<`string`, `any`>>| Additional metadata for the dataset as a key-value pair object. (optional)  
**Returns**
type| description  
---|---  
`Promise`<`Dataset`>| A new `Dataset` instance populated with the updated dataset’s data  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#deletedataset)
deleteDataset()
Copy
```
deleteDataset(id): Promise<Dataset>

```

Deletes a single dataset by its unique identifier.
**Parameters**
Parameter| Type| Description  
---|---|---  
`id`| `string`| The unique identifier of the dataset to delete. This parameter is required.  
**Returns**
type| description  
---|---  
`Promise`<`Dataset`>| A new `Dataset` instance populated with the deleted dataset’s data  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#createdatasetitem)
createDatasetItem()
Copy
```
createDatasetItem(datasetId, datasetItem): Promise<DatasetItem>

```

Creates a new item in a dataset.
**Parameters**
Parameter| Type| Description  
---|---|---  
`datasetId`| `string`| The unique identifier of the dataset. This parameter is required.  
`datasetItem`| `object`| The data for the new dataset item. This parameter is required.  
`datasetItem.input`| `Record`<`string`, `any`>| The input data for the dataset item. This field is required.  
`datasetItem.expectedOutput`?| `Maybe`<`Record`<`string`, `any`>>| The expected output data for the dataset item. This field is optional.  
`datasetItem.metadata`?| `Maybe`<`Record`<`string`, `any`>>| Additional metadata for the dataset item. This field is optional.  
**Returns**
type| description  
---|---  
`Promise`<`DatasetItem`>| A new `DatasetItem` instance populated with the created dataset item’s data  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getdatasetitem)
getDatasetItem()
Copy
```
getDatasetItem(id): Promise<DatasetItem>

```

Retrieves a single item from a dataset by its unique identifier.
**Parameters**
Parameter| Type| Description  
---|---|---  
`id`| `string`| The unique identifier of the dataset item. This parameter is required.  
**Returns**
type| description  
---|---  
`Promise`<`DatasetItem`>| A `DatasetItem` instance populated with the retrieved dataset item’s data  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#deletedatasetitem)
deleteDatasetItem()
Copy
```
deleteDatasetItem(id): Promise<DatasetItem>

```

Deletes a single item from a dataset by its unique identifier.
**Parameters**
Parameter| Type| Description  
---|---|---  
`id`| `string`| The unique identifier of the dataset item to be deleted. This parameter is required.  
**Returns**
type| description  
---|---  
`Promise`<`DatasetItem`>| A `DatasetItem` instance populated with the data of the deleted dataset item  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#addsteptodataset)
addStepToDataset()
Copy
```
addStepToDataset(
  datasetId, 
  stepId, 
metadata?): Promise<DatasetItem>

```

Adds a single step item to a dataset.
**Parameters**
Parameter| Type| Description  
---|---|---  
`datasetId`| `string`| The unique identifier of the dataset. This parameter is required.  
`stepId`| `string`| The unique identifier of the step to be added. This parameter is required.  
`metadata`?| `Maybe`<`Record`<`string`, `unknown`>>| Additional metadata for the step as a JSON object. This parameter is optional.  
**Returns**
type| description  
---|---  
`Promise`<`DatasetItem`>| A `DatasetItem` instance populated with the data of the newly added step  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#addgenerationtodataset)
addGenerationToDataset()
Copy
```
addGenerationToDataset(
  datasetId, 
  generationId, 
metadata?): Promise<DatasetItem>

```

Adds a generation item to a dataset.
**Parameters**
Parameter| Type| Description  
---|---|---  
`datasetId`| `string`| The unique identifier of the dataset. This parameter is required.  
`generationId`| `string`| The unique identifier of the generation to be added. This parameter is required.  
`metadata`?| `Maybe`<`Record`<`string`, `unknown`>>| Additional metadata for the generation as a JSON object. This parameter is optional.  
**Returns**
type| description  
---|---  
`Promise`<`DatasetItem`>| A `DatasetItem` instance populated with the data of the newly added generation  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#addgenerationstodataset)
addGenerationsToDataset()
Copy
```
addGenerationsToDataset(datasetId, generationIds): Promise<DatasetItem[]>

```

Adds multiple generation items to a dataset.
**Parameters**
Parameter| Type| Description  
---|---|---  
`datasetId`| `string`| The unique identifier of the dataset. This parameter is required.  
`generationIds`| `string`[]| An array of unique identifiers for the generations to be added. This parameter is required.  
**Returns**
`Promise`<`DatasetItem`[]>
An array of `DatasetItem` instances populated with the data of the newly added generations
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#createexperiment)
createExperiment()
Copy
```
createExperiment(datasetExperiment): Promise<DatasetExperiment>

```

Creates a new dataset experiment.
**Parameters**
Parameter| Type| Description  
---|---|---  
`datasetExperiment`| `object`  
`datasetExperiment.name`| `string`| The name of the dataset experiment.  
`datasetExperiment.datasetId`?| `string`| The dataset ID to associate with the experiment.  
`datasetExperiment.promptId`?| `string`| The prompt ID to associate with the experiment.  
`datasetExperiment.params`?| `Record`<`string`, `any`> | `Record`<`string`, `any`>[]| The parameters for the experiment as a key-value pair object or an array of the same.  
**Returns**
type| description  
---|---  
`Promise`<`DatasetExperiment`>| The newly created dataset experiment object  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#createexperimentitem)
createExperimentItem()
Copy
```
createExperimentItem(parameters): Promise<DatasetExperimentItem>

```

Creates a new dataset experiment item.
**Parameters**
Parameter| Type| Description  
---|---|---  
`parameters`| `DatasetExperimentItem`  
**Returns**
type| description  
---|---  
`Promise`<`DatasetExperimentItem`>| The dataset experiment object  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#createpromptlineage)
createPromptLineage()
Copy
```
createPromptLineage(name, description?): Promise<any>

```

Create a new prompt lineage.
**Parameters**
Parameter| Type| Description  
---|---|---  
`name`| `string`| The name of the prompt lineage. This parameter is required.  
`description`?| `string`| A description for the prompt lineage. This parameter is optional.  
**Returns**
type| description  
---|---  
`Promise`<`any`>| The newly created prompt lineage object, or null if creation failed  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#createprompt)
createPrompt()
Copy
```
createPrompt(
  name, 
  templateMessages, 
settings?): Promise<Prompt>

```

**Parameters**
Parameter| Type  
---|---  
`name`| `string`  
`templateMessages`| `IGenerationMessage`[]  
`settings`?| `Maybe`<`Record`<`string`, `any`>>  
**Returns**
`Promise`<`Prompt`>
**Deprecated**
Please use getOrCreatePrompt instead.
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getorcreateprompt)
getOrCreatePrompt()
Copy
```
getOrCreatePrompt(
  name, 
  templateMessages, 
  settings?, 
tools?): Promise<Prompt>

```

A Prompt is fully defined by its name, template_messages, settings and tools. If a prompt already exists for the given arguments, it is returned. Otherwise, a new prompt is created.
**Parameters**
Parameter| Type| Description  
---|---|---  
`name`| `string`| The name of the prompt to retrieve or create.  
`templateMessages`| `IGenerationMessage`[]| A list of template messages for the prompt.  
`settings`?| `Maybe`<`Record`<`string`, `any`>>| Optional settings for the prompt.  
`tools`?| `Maybe`<`Record`<`string`, `any`>>| Optional tools for the prompt.  
**Returns**
type| description  
---|---  
`Promise`<`Prompt`>| The prompt that was retrieved or created  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getpromptbyid)
getPromptById()
Copy
```
getPromptById(id): Promise<null | Prompt>

```

Retrieves a prompt by its id.
**Parameters**
Parameter| Type| Description  
---|---|---  
`id`| `string`| ID of the prompt to retrieve.  
**Returns**
type| description  
---|---  
`Promise`<`null` | `Prompt`>| The prompt with given ID  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getprompt)
getPrompt()
Copy
```
getPrompt(name, version?): Promise<null | Prompt>

```

Retrieves a prompt by its name and optionally by its version.
**Parameters**
Parameter| Type| Description  
---|---|---  
`name`| `string`| The name of the prompt to retrieve.  
`version`?| `number`| The version number of the prompt (optional).  
**Returns**
type| description  
---|---  
`Promise`<`null` | `Prompt`>| An instance of `Prompt` containing the prompt data, or `null` if not found  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#getpromptabtesting)
getPromptAbTesting()
Copy
```
getPromptAbTesting(name): Promise<null | IPromptRollout[]>

```

Retrieves a prompt A/B testing rollout by its name.
**Parameters**
Parameter| Type| Description  
---|---|---  
`name`| `string`| The name of the prompt to retrieve.  
**Returns**
type| description  
---|---  
`Promise`<`null` | `IPromptRollout`[]>| A list of prompt rollout versions  
### 
[​](https://docs.literalai.com/typescript-client/api-reference/api#updatepromptabtesting)
updatePromptAbTesting()
Copy
```
updatePromptAbTesting(name, rollouts): Promise<any>

```

Update a prompt A/B testing rollout by its name.
**Parameters**
Parameter| Type| Description  
---|---|---  
`name`| `string`| The name of the prompt to retrieve.  
`rollouts`| `IPromptRollout`[]| A list of prompt rollout versions.  
**Returns**
type| description  
---|---  
`Promise`<`any`>| A list of prompt rollout versions

---

# Dataset

## Source
https://docs.literalai.com/typescript-client/api-reference/dataset

## 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#dataset)
Dataset
Copy
```
type Dataset = {
 id: string;
 createdAt: string;
 name?: Maybe<string>;
 description?: Maybe<string>;
 metadata: Record<string, any>;
 items?: Maybe<DatasetItem[]>;
 type?: Maybe<DatasetType>
}

```

[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-id)
id
string
The unique identifier of the dataset. This is an immutable field generated when the dataset is created.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-created-at)
createdAt
string
The date and time when the dataset was created, expressed in ISO 8601 format.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-metadata)
metadata
Record<string, any>
A dictionary containing additional data associated with the dataset. This can include user-defined key-value pairs that provide more context or details about the dataset.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-name)
name
Maybe<string>
An optional name for the dataset. This can be used to give a descriptive, human-readable title to the dataset for easier identification.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-description)
description
Maybe<string>
An optional description field that can provide more detailed information about the dataset, such as its purpose, contents, or specific characteristics.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-items)
items
Maybe<DatasetItem[]>
An optional list of items contained within the dataset. Each item in the list is a dictionary representing a specific data entry in the dataset.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-type)
type
Maybe<DatasetType>
An optional type of dataset. Defaults to “key_value”, other option is “generation”.
### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#create-a-dataset)
Create a dataset
Copy
```
const dataset = await literalAiClient.api.createDataset({
  name: "Foo", 
  description: "A dataset to store samples.", 
  metadata: { isDemo: true },
});

```

#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#params)
Params
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset)
dataset
DatasetInput
A partial representation of a dataset.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-name)
dataset.name
Maybe<string>
An optional name for the dataset. This can be used to give a descriptive, human-readable title to the dataset for easier identification.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-description)
dataset.description
Maybe<string>
An optional description field that can provide more detailed information about the dataset, such as its purpose, contents, or specific characteristics.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-metadata)
dataset.metadata
Maybe<Record<string, any>>
A dictionary containing additional data associated with the dataset. This can include user-defined key-value pairs that provide more context or details about the dataset.
#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#response)
Response
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-1)
dataset
Dataset
Return a Dataset
### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#update-a-dataset)
Update a dataset
Copy
```
const dataset = await literalAiClient.api.updateDataset("dataset_id", {
 name: "Baz",
});

```

#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#params-2)
Params
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-id-1)
id
string
The unique identifier of the dataset. This is an immutable field generated when the dataset is created.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-2)
dataset
DatasetInput
A partial representation of a dataset.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-name-1)
dataset.name
Maybe<string>
An optional name for the dataset. This can be used to give a descriptive, human-readable title to the dataset for easier identification.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-description-1)
dataset.description
Maybe<string>
An optional description field that can provide more detailed information about the dataset, such as its purpose, contents, or specific characteristics.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-metadata-1)
dataset.metadata
Maybe<Record<string, any>>
A dictionary containing additional data associated with the dataset. This can include user-defined key-value pairs that provide more context or details about the dataset.
#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#response-2)
Response
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-3)
dataset
Dataset
Return a Dataset
### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#get-a-dataset)
Get a dataset
Copy
```
const dataset = await literalAiClient.api.getDataset("dataset_id")

```

#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#params-3)
Params
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-id-2)
id
string
The unique identifier of the dataset. This is an immutable field generated when the dataset is created.
#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#response-3)
Response
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-4)
dataset
Dataset
Return a Dataset
### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#delete-a-dataset)
Delete a dataset
Copy
```
await literalAiClient.api.deleteDataset("dataset_id")

```

#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#params-4)
Params
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-id-3)
id
string
The unique identifier of the dataset. This is an immutable field generated when the dataset is created.
#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#response-4)
Response
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-5)
dataset
Dataset
Return a Dataset
## 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#datasetitem)
DatasetItem
Copy
```
type DatasetItem = {
 id: string;
 createdAt: string;
 datasetId: string;
 metadata: Record<string, any>;
 input: Record<string, any>;
 expectedOutput?: Maybe<Record<string, any>>;
 intermediarySteps: Record<string, any>[];
 stepId?: Maybe<string>;
}

```

[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-id-4)
id
string
The unique identifier of the dataset item. Each item within a dataset has its own unique ID.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-created-at-1)
createdAt
string
The date and time when the dataset item was created, expressed in ISO 8601 format.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-id)
datasetId
string
The identifier of the dataset to which this item belongs. Links the item to its parent dataset.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-metadata-1)
metadata
Record<string, any>
A dictionary containing additional data associated with this dataset item. This can include specific details relevant to the item.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-input)
input
Record<string, any>
The input data for this dataset item. Typically contains the parameters or information that forms the LLM request.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-expected-output)
expectedOutput
Maybe<Record<string, any>>
The optional output data for this dataset item. Represents the result or outcome associated with the item’s input.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-intermediary-steps)
intermediarySteps
Record<string, any>[]
An optional list of intermediary steps involved in the output. This can include any processing steps or intermediate results. They are automatically generated when using “steps” to build a dataset.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-step-id)
stepId
Record<string, any>[]
The optional reference ID to a specific step.
### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#create-a-dataset-item)
Create a dataset item
Copy
```
const datasetItem = await literalAiClient.api.createDatasetItem("dataset_id", {
 input: { content: "What is Literal AI?" },
 expectedOutput: { content: "Literal AI is an observability solution." },
});
# Or directly through a Dataset
const datasetItem = await dataset.createDatasetItem("dataset_id", {
 input: { content: "What is Literal AI?" },
 expectedOutput: { content: "Literal AI is an observability solution." },
});

```

#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#params-5)
Params
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-id-1)
datasetId
string
The identifier of the dataset to which this item belongs.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-item)
datasetItem
DatasetItemInput
A partial representation of a dataset item.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-item-input)
datasetItem.input
Record<string, any>
The input data for this dataset item. Typically contains the parameters or information that forms the LLM request.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-item-expected-output)
datasetItem.expectedOutput
Maybe<Record<string, any>>
The optional output data for this dataset item. Represents the result or outcome associated with the item’s input.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-item-metadata)
datasetItem.metadata
Maybe<Record<string, any>>
A dictionary containing additional data associated with this dataset item. This can include specific details relevant to the item.
#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#response-5)
Response
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-item-1)
datasetItem
DatasetItem
Return a DatasetItem
### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#add-a-step-to-a-dataset)
Add a step to a dataset
Copy
```
const datasetItem = await literalAiClient.api.addStepToDataset("dataset_id", "step_id");
# Or directly through a Dataset
const datasetItem = await dataset.addStep("step_id");

```

#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#params-6)
Params
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-id-2)
datasetId
string
The identifier of the dataset to which this item belongs.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-step-id-1)
stepId
string
The identifier of the step created beforehand.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-metadata-2)
metadata
Maybe<Record<string, any>>
A dictionary containing additional data associated with this dataset item. This can include specific details relevant to the item.
#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#response-6)
Response
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-item-2)
datasetItem
DatasetItem
Return a DatasetItem
### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#add-a-generation-to-a-dataset)
Add a generation to a dataset
Copy
```
const datasetItem = await literalAiClient.api.addGeneationToDataset("dataset_id", "generation_id");
# Or directly through a Dataset
const datasetItem = await dataset.addGeneration("generation_id");

```

#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#params-7)
Params
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-id-3)
datasetId
string
The identifier of the dataset to which this item belongs.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-generation-id)
generationId
string
The identifier of the generation created beforehand.
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-metadata-3)
metadata
Maybe<Record<string, any>>
A dictionary containing additional data associated with this dataset item. This can include specific details relevant to the item.
#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#response-7)
Response
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-item-3)
datasetItem
DatasetItem
Return a DatasetItem
### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#get-a-dataset-item)
Get a dataset item
Copy
```
const datasetItem = await literalAiClient.api.getDatasetItem("dataset_item_id");

```

#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#params-8)
Params
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-id-5)
id
string
The unique identifier of the dataset item. Each item within a dataset has its own unique ID.
#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#response-8)
Response
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-item-4)
datasetItem
DatasetItem
Return a DatasetItem
### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#delete-a-dataset-item)
Delete a dataset item
Copy
```
await literalAiClient.api.deleteDatasetItem("dataset_item_id");
# Or directly through a Dataset
await dataset.deleteDatasetItem("dataset_item_id");

```

#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#params-9)
Params
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-id-6)
id
string
The unique identifier of the dataset item. Each item within a dataset has its own unique ID.
#### 
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#response-9)
Response
[​](https://docs.literalai.com/typescript-client/api-reference/dataset#param-dataset-item-5)
datasetItem
DatasetItem
Return a DatasetItem

---

# Changelog

## Source
https://docs.literalai.com/typescript-client/development/changelog

# 
[​](https://docs.literalai.com/typescript-client/development/changelog#0-1-0-november-12th%2C-2024)
`0.1.0` (November 12th, 2024)
### 
[​](https://docs.literalai.com/typescript-client/development/changelog#improvements)
Improvements
  * Added `createPromptVariant` API to create `Prompt` variants
  * Modified `createExperiment` to take as input a `promptVariantId` instead of a `promptId`


Starting with `0.1.0`, you need to have Literal AI version 0.1.0-beta or above to create experiments.
# 
[​](https://docs.literalai.com/typescript-client/development/changelog#0-0-520-october-28th%2C-2024)
`0.0.520` (October 28th, 2024)
### 
[​](https://docs.literalai.com/typescript-client/development/changelog#improvements-2)
Improvements
  * Added fields `label`, `status` and `tags` to `getPromptById` API.


Starting with `0.0.520`, you need to have Literal AI version 0.0.629-beta or above.
# 
[​](https://docs.literalai.com/typescript-client/development/changelog#0-0-519-october-21st%2C-2024)
`0.0.519` (October 21st, 2024)
### 
[​](https://docs.literalai.com/typescript-client/development/changelog#improvements-3)
Improvements
  * Added `release` parameter to Literal AI client constructors


# 
[​](https://docs.literalai.com/typescript-client/development/changelog#0-0-518-september-18th%2C-2024)
`0.0.518` (September 18th, 2024)
### 
[​](https://docs.literalai.com/typescript-client/development/changelog#bug-fixes)
Bug fixes
  * Use getters to lazy load modules


# 
[​](https://docs.literalai.com/typescript-client/development/changelog#0-0-517-september-16th%2C-2024)
`0.0.517` (September 16th, 2024)
### 
[​](https://docs.literalai.com/typescript-client/development/changelog#improvements-4)
Improvements
  * Added link to prompt as URL in the `get_prompt` API
  * Introduced decorators with `metadata`, `tags` and `stepid` fields
  * Adjusted integrations to account for `metadata` and `tags` logging


### 
[​](https://docs.literalai.com/typescript-client/development/changelog#bug-fixes-2)
Bug fixes
  * Made third-party dependencies optional when importing Literal AI client.


# 
[​](https://docs.literalai.com/typescript-client/development/changelog#0-0-515)
`0.0.515`
### 
[​](https://docs.literalai.com/typescript-client/development/changelog#improvements-5)
Improvements
This version improves compatibility with the LangChain ecosystem, notably with complex LangGraph flows.
# 
[​](https://docs.literalai.com/typescript-client/development/changelog#0-0-514)
`0.0.514`
### 
[​](https://docs.literalai.com/typescript-client/development/changelog#improvements-6)
Improvements
Simplify the Vercel AI SDK integration by leveraging the internal context. `threadId` and `parentId` are now automatically handled without the need to pass them as a `literalAiParent` argument.
# 
[​](https://docs.literalai.com/typescript-client/development/changelog#0-0-513)
`0.0.513`
### 
[​](https://docs.literalai.com/typescript-client/development/changelog#improvements-7)
Improvements
  * Added option to send a root run ID upon creation of a `Step`
  * The API `update_prompt_ab_testing` replaces the old `promote_prompt` and extends on it to allow for A/B testing


Starting with `0.0.513`, you need to have Literal AI version `0.0.617-beta` or above.
# 
[​](https://docs.literalai.com/typescript-client/development/changelog#0-0-512)
`0.0.512`
### 
[​](https://docs.literalai.com/typescript-client/development/changelog#breaking-changes)
Breaking Changes
Instantiation of the TypeScript client was altered to take in an object as argument:
>= 0.0.512
< 0.0.512
Copy
```
client = new LiteralClient({
 apiKey: process.env.LITERAL_API_KEY,
 apiUrl: process.env.LITERAL_API_URL
});

```

### 
[​](https://docs.literalai.com/typescript-client/development/changelog#new-features)
New Features
This release introduces a new, wrapper-aware method to create attachments : `client.api.createAttachment`. This method is a wrapper around the `uploadFile` method and the `new Attachment` constructor. It allows you to create an attachment in a single call, and attach it to the thread/step in the current context.
The old method using `uploadFile` and `new Attachment` is considered obsolete, although it remains in the SDK for backward compatibility.
### 
[​](https://docs.literalai.com/typescript-client/development/changelog#improvements-8)
Improvements
  * `threadId` is now optional when creating an attachment or uploading a file
  * `createAttachment` and `uploadFile` now accept file data in the following formats : ReadableStream, ReadStream, Buffer, File, Blob & ArrayBuffer
  * `Generation` creation now supports passing `tags` and `metadata`
  * `Experiment` become optionally linked to a `Dataset`


Starting with `0.0.512`, you need to have Literal AI version 0.0.615-beta or above.
# 
[​](https://docs.literalai.com/typescript-client/development/changelog#0-0-510)
`0.0.510`
### 
[​](https://docs.literalai.com/typescript-client/development/changelog#breaking-changes-2)
Breaking Changes
Revamp of the OpenAI instrumentation : instead of manually instrumenting each call result, you can now instrument OpenAI globally. Each subsequently call will be logged with no additional code.
This new implementation is context-aware and will log the OpenAI calls in the right steps and threads if you use the new wrapper syntax.
If you were using the `instrumentation.openai` method in your code and upgrade the Literal AI SDK to `0.0.510`, you will get the following error at compile time : `error TS2554: Expected 0-1 arguments, but got 2.`.
To fix this, you should move the `instrumentation.openai` call to the top of your code and remove the arguments apart from the optional `{ tags: []}` argument.
You can find code examples in the [OpenAI Instrumentation](https://docs.literalai.com/integrations/openai) guide.
# 
[​](https://docs.literalai.com/typescript-client/development/changelog#0-0-509)
`0.0.509`
### 
[​](https://docs.literalai.com/typescript-client/development/changelog#new-features-2)
New Features
  * New syntax with wrappers for `step` and `thread` methods.


# 
[​](https://docs.literalai.com/typescript-client/development/changelog#0-0-504)
`0.0.504`
### 
[​](https://docs.literalai.com/typescript-client/development/changelog#deprecations)
Deprecations
  * `format` is deprecated. `formatMessages` should be used now.


### 
[​](https://docs.literalai.com/typescript-client/development/changelog#new-features-3)
New Features
  * Add support for tags with OpenAI Instrumentation
  * Allow thread upsert with one single object argument


# 
[​](https://docs.literalai.com/typescript-client/development/changelog#0-0-503)
`0.0.503`
### 
[​](https://docs.literalai.com/typescript-client/development/changelog#deprecations-2)
Deprecations
  * `createPrompt()` is deprecated. `getOrCreatePrompt()` should now be used.


### 
[​](https://docs.literalai.com/typescript-client/development/changelog#new-features-4)
New Features
  * `getOrCreatePrompt()`. For creating a new Prompt, use `getOrCreatePrompt()`.


A Prompt is fully defined by its name, template_messages, settings and tools. If a prompt already exists for the given arguments, it is returned. Otherwise, a new prompt is created.
Copy
```
/* 
* @param name The name of the prompt to retrieve or create.
* @param templateMessages A list of template messages for the prompt.
* @param settings Optional settings for the prompt.
* @returns The prompt that was retrieved or created.
*/

```

---

# Installation

## Source
https://docs.literalai.com/typescript-client/introduction

## [Open Source SDKCheck out the open-source Github Repo](https://github.com/Chainlit/literalai-typescript)
### 
[​](https://docs.literalai.com/typescript-client/introduction#introduction)
Introduction
In addition to the TypeScript SDK documentation, all [Guides](https://docs.literalai.com/guides) showcase code examples in TypeScript.
Check the Literal AI TypeScript Client on [NPM](https://www.npmjs.com/package/@literalai/client) for the documentation.
Copy
```
npm i @literalai/client@latest

```

## 
[​](https://docs.literalai.com/typescript-client/introduction#instantiation)
Instantiation
Copy
```
import { LiteralClient } from "@literalai/client";
const literalAiClient = new LiteralClient({apiKey: process.env["LITERAL_API_KEY"]});

```

#### 
[​](https://docs.literalai.com/typescript-client/introduction#disable-thread-and-step-ingestion-apis)
Disable Thread and Step Ingestion APIs
When instantiating the client, you have the option to disable the Thread and Step ingestion APIs that comes with the SDK, like `thread.step()`. This can be useful when you want to disable this without changing the code.
Copy
```
import { LiteralClient } from "@literalai/client";
const literalAiClient = new LiteralClient({
 apiKey: process.env["LITERAL_API_KEY"],
 disabled: true
 });

```

---

# With wrappers

## Source
https://docs.literalai.com/typescript-client/with-wrappers

The simplest way to add Literal AI to your codebase is to use the provided wrappers. Wrappers are based on the [AsyncLocalStorage API](https://nodejs.org/api/async_context.html#class-asynclocalstorage) and are designed to play nice with async code.
Wrappers will automatically handle the thread and step IDs for you, and will also automatically send entities to the Literal AI API upon exiting. When a step wrapper is exited, the following actions will be taken :
  * Start and end time are logged
  * The output from the wrapped function is logged as the output of the step
  * The updated step is sent to the Literal AI API


If your environment doesn’t support AsyncLocalStorage, or if you don’t like the callback style, you can always revert to the [wrapper-less syntax](https://docs.literalai.com/typescript-client/typescript-client/without-wrappers). Please note that this syntax requires you to manually manage thread and step IDs, and also to manually send entities to the Literal AI API.
## 
[​](https://docs.literalai.com/typescript-client/with-wrappers#basic-usage)
Basic usage
### 
[​](https://docs.literalai.com/typescript-client/with-wrappers#thread-wrapper)
Thread wrapper
Copy
```
import { LiteralClient } from '@literalai/client';
const client = new LiteralClient();
await client
 .thread({ name: 'Test Wrappers Thread' })
 .wrap(async () => {
  // You can access the current thread using the client
  const thread = client.getCurrentThread();
  // Because we are not currently in a step, the next line would throw if uncommented
  // const step = client.getCurrentStep();
  // This step will be created with the thread as its parent
  await client.step({ name: 'Test Wrappers Step', type: "assistant_message" }).send();
 });

```

### 
[​](https://docs.literalai.com/typescript-client/with-wrappers#step-wrapper)
Step wrapper
Copy
```
import { LiteralClient } from '@literalai/client';
const client = new LiteralClient();
await client
 .step({ name: 'Test Wrappers Thread', type: 'run' })
 .wrap(async () => {
  // You can access the current step using the client
  const thread = client.getCurrentStep();
  // This step will be created with the wrapped step as its parent
  await client.step({ name: 'Test Wrappers Step', type: "assistant_message" }).send();
  const openai = new OpenAI();
  client.instrumentation.openai();
  // This generation will be created with the wrapped step as its parent
  const completion = await openai.chat.completions.create({
    model: 'gpt-4',
    messages: [{ role: 'user', content: 'Say hello !' }]
   });
   return completion.choices[0].message;
 });

```

### 
[​](https://docs.literalai.com/typescript-client/with-wrappers#decoration-wrapper)
Decoration wrapper
This wrapper is used to add metadata and tags to everything that will be logged inside it. It can also be used to specify in advance the ID of any generation created inside it.
This is especially useful when using one of our integrations, as you may not be able to specify metadata or tags directly in the API call, and you may not be able to retrieve the ID of the generation that was logged.
Copy
```
import { LiteralClient } from '@literalai/client';
import { v4 as uuidv4 } from 'uuid';
const client = new LiteralClient();
const openai = new OpenAI();
client.instrumentation.openai();
const stepId = uuidv4();
const metadata = { key: 'value' };
const tags = ['tag1', 'tag2'];
await client.decorate({ metadata, tags, stepId }).wrap(async () => {
 // This generation will be logged with the provided stepId, metadata and tags
 const completion = await openai.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Say hello !' }]
 });
 // Because step IDs are unique, this call will revert to the default behaviour of
 // assigning a random UUID to the generation when it is logged.
 const completion = await openai.chat.completions.create({
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Say hello !' }]
 });
})

```

## 
[​](https://docs.literalai.com/typescript-client/with-wrappers#advanced-usage)
Advanced usage
### 
[​](https://docs.literalai.com/typescript-client/with-wrappers#nesting-wrappers)
Nesting wrappers
One advantage of using wrappers is that the structure of your code will closely represent the structure of the conversation as it is logged on Literal AI. For example consider the following code :
Copy
```
import { LiteralClient } from '@literalai/client';
const client = new LiteralClient();
// Wrapped functions can be defined anywhere
// When they run, they will inherit the current thread and step from the context
const retrieve = async (_query: string) =>
 client.step({ name: 'Retrieve', type: 'retrieval' }).wrap(async () => {
  // Fetch data from the vector database ...
  return [
   { score: 0.8, text: 'France is a country in Europe' },
   { score: 0.7, text: 'Paris is the capital of France' }
  ]
 });
const completion = async (_query: string, _augmentations: string[]) =>
 client.step({ name: 'Completion', type: 'llm' }).wrap(async () => {
  // Fetch completions from the language model ...
  return { content: 'Paris is a city in Europe' };
 });
 // The output of wrapped functions will always bubble up the wrapper chain
 const result = await client
  .thread({ name: 'Test Wrappers Thread' })
  .wrap(async () => {
   return client.run({ name: 'Test Wrappers Run' }).wrap(async () => {
    const results = await retrieve(query);
    const augmentations = results.map((result) => result.text);
    const completionText = await completion(query, augmentations);
    return completionText.content;
   });
  });
 return result;

```

This will result in the following structure on Literal AI :
![Step with score](https://mintlify.s3.us-west-1.amazonaws.com/chainlit-5/images/syntax/wrappers-log.png)
Nested threads and steps
### 
[​](https://docs.literalai.com/typescript-client/with-wrappers#wrapping-existing-threads-or-steps)
Wrapping existing threads or steps
When you fetch a thread or step from the client, you can wrap it with the `wrap` method. This will allow you to add additional steps or threads to the existing one.
Copy
```
const thread = await client.api.getThread(threadId);
await thread.wrap(async () => {
 await client.step({ name: 'New step', type: 'assistant_message' }).send();
});

```

## 
[​](https://docs.literalai.com/typescript-client/with-wrappers#updating-steps-and-threads)
Updating steps and threads
### 
[​](https://docs.literalai.com/typescript-client/with-wrappers#using-the-context)
Using the context
You can also update the current step or thread using the context object. The context object is available in the wrapped function and it will be sent to Literal AI upon exiting the wrapped function.
There are two ways to access the context object:
  * It is provided as an argument to the wrapper function
  * It is available as a property of the client object : `client.getCurrentThread()` and `client.getCurrentStep()`


The `getCurrentThread` and `getCurrentStep` methods are type-safe and they will throw an error if called outside of a wrapped context.
Copy
```
// Using the argument
client.step({ name: 'Completion', type: 'llm' }).wrap(async (step) => {
  step.name = "Updated completion";
  return { content: 'Paris is a city in Europe' };
 });
// Using the client helper
client.step({ name: 'Retrieve', type: 'retrieval' }).wrap(async () => {
 client.getCurrentThread().name = 'Updated retrieval';
 return [
  { score: 0.8, text: 'France is a country in Europe' },
  { score: 0.7, text: 'Paris is the capital of France' }
 ]
});

```

### 
[​](https://docs.literalai.com/typescript-client/with-wrappers#using-the-wrapper%E2%80%99s-callback)
Using the wrapper’s callback
The wrappers take a second argument that allows you to update the current step or thread upon exiting the wrapped function. The second argument can either be a static object, or a function that takes the wrapper’s output as an argument and returns a valid step or thread object.
Copy
```
// Update the current step with a static object
client.step({ name: 'Completion', type: 'llm' }).wrap(async () => {
 return { content: 'Paris is a city in Europe' };
}, { name: "Updated completion" }); 
// Update the current step with a callback
client.step({ name: 'Retrieve', type: 'retrieval' }).wrap(async () => {
 return [
  { score: 0.8, text: 'France is a country in Europe' },
  { score: 0.7, text: 'Paris is the capital of France' }
 ]
}, (output) => {
 return {
  metadata: { topScore: output[0].score, topK: output.length}
 }
});

```

---

# Without wrappers

## Source
https://docs.literalai.com/typescript-client/without-wrappers

The Literal AI Typescript SDK provides a way to interact with the Literal AI API without using wrappers. This is useful when you want to have more control over the API calls, or when you want to use the API in a more functional way. It implies that you will have to manually manage thread and step IDs, and also to manually send API calls to Literal AI.
Manual code can further be simplified by using one of our [Integrations](https://docs.literalai.com/integrations).
Copy
```
import {
 IGenerationMessage,
 LiteralClient,
} from "@literalai/client";
import OpenAI from "openai";
const client = new LiteralClient();
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
async function main(file: ArrayBuffer) {
 const thread = await client.thread({ name: 'Test Thread' }).upsert();
 const run = await thread
  .run({
   threadId: thread.id,
   name: "Tool Run",
  })
  .send();
 const start = new Date();
 const { text: transcribedText } = await openai.audio.transcriptions.create({
  file: audioFile,
  model: "whisper-1",
  language: "en",
 });
 const end = new Date();
 await run
 .step({
  type: "llm",
  name: "whisper-1",
  input,
  output,
  startTime: start.toISOString(),
  endTime: end.toISOString(),
 })
 .send();
 run.endTime = end.toISOString();
 await run.send();
}

```

---

